"use strict";(self.webpackChunkseata_website=self.webpackChunkseata_website||[]).push([[95894],{76042:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/seata-namingserver","metadata":{"permalink":"/blog/seata-namingserver","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/blog/seata-namingserver.md","source":"@site/blog/seata-namingserver.md","title":"seata-namingserver","description":"Placeholder. DO NOT DELETE.","date":"2025-02-28T16:05:41.000Z","formattedDate":"February 28, 2025","tags":[],"readingTime":0.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"nextItem":{"title":"Getting Started with Seata Network Communication Source Code","permalink":"/blog/seata-rpc-analysis"}},"content":"Placeholder. DO NOT DELETE."},{"id":"/seata-rpc-analysis","metadata":{"permalink":"/blog/seata-rpc-analysis","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-rpc-analysis.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-rpc-analysis.md","title":"Getting Started with Seata Network Communication Source Code","description":"A comprehensive introductory analysis of Seata\'s RPC source code","date":"2024-12-18T00:00:00.000Z","formattedDate":"December 18, 2024","tags":[],"readingTime":44.2,"hasTruncateMarker":false,"authors":[{"name":"Jin He"}],"frontMatter":{"title":"Getting Started with Seata Network Communication Source Code","keywords":["Seata","RPC","Source Code","Distributed Transactions"],"description":"A comprehensive introductory analysis of Seata\'s RPC source code","author":"Jin He","date":"2024-12-18T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"seata-namingserver","permalink":"/blog/seata-namingserver"},"nextItem":{"title":"Go Language Client Communication with Seata Server","permalink":"/blog/seata-grpc-client"}},"content":"In the previous articles, we have thoroughly discussed Seata\'s XA, AT, and TCC modes, all of which are different transaction models defined within the global framework of Seata.\\n\\nWe know that in Seata, there are three types of roles: TC (Transaction Coordinator), RM (Resource Manager), and TM (Transaction Manager). The Seata Server acts as a TC to coordinate the commit and rollback of branch transactions, while various resources act as RMs and TMs. So, how do these three communicate with each other?\\n\\nTherefore, this article will explore how Seata performs network communication at the underlying level.\\n\\n## Overall Class Hierarchy Structure\\n\\nLet\'s start by looking at the big picture, examining the overall RPC class hierarchy structure of Seata.\\n\\n![image-20241217222005964](/img/blog/class-level.png)\\n\\nFrom the class hierarchy structure, it can be seen that AbstractNettyRemoting is the top-level abstract class for the entire Seata network communication.\\n\\nIn this class, some basic common methods of RPC are mainly implemented, such as synchronous call sendSync, asynchronous call sendAsync, etc.\\n\\nIndeed, when it comes to network calls, they essentially boil down to synchronous calls and asynchronous calls; other aspects like requests and responses are just distinctions in message content.\\n\\nSo, in Seata, I personally think there should also be a top-level interface Remoting, similar to the following:\\n\\n```java\\nimport io.netty.channel.Channel;\\nimport java.util.concurrent.TimeoutException;\\n\\npublic interface Remoting<Req, Resp> {\\n\\n    /**\\n     * Synchronous call\\n     */ \\n    Resp sendSync(Channel channel, Req request, long timeout) throws TimeoutException;\\n\\n    /** \\n     * Asynchronous call\\n     */ \\n    void sendAsync(Channel channel, Req request);\\n}\\n```\\n\\nWhile AbstractNettyRemoting implements general network calling methods, there are still some differences among different roles. For example, for the server, its request call needs to know which client to send to, whereas for the TM and RM, they can simply send requests without specifying a particular TC service. They only need to find an appropriate server node via a load balancing algorithm in the implementation class.\\n\\nThus, RemotingServer and RemotingClient are differentiated, but they still rely on AbstractNettyRemoting for network calls at the bottom layer, so each has subclasses that implement AbstractNettyRemoting.\\n\\nOne might say that this design in Seata is quite commendable, serving as a general solution pattern for remote communications in this kind of Client-Server architecture.\\n\\n## How to Start the Server and Client\\n\\nAfter discussing the underlying class hierarchy of Seata, let\'s look from the perspectives of the Server and Client on how they start up and what needs to be done during startup.\\n\\n### How the Server Starts\\n\\nAs an independent Spring Boot project, how does the Seata Server automatically perform certain tasks when Spring Boot starts?\\n\\nSeata achieves this by implementing the `CommandLineRunner` interface. The principle behind this is not within the scope of this article.\\n\\nWe mainly focus on its `run` method:\\n\\n```java\\n// org.apache.seata.server.ServerRunner#run\\npublic void run(String... args) {\\n    try {\\n        long start = System.currentTimeMillis();\\n        seataServer.start(args);\\n        started = true;\\n        long cost = System.currentTimeMillis() - start;\\n        LOGGER.info(\\"\\\\r\\\\n you can visit seata console UI on http://127.0.0.1:{}. \\\\r\\\\n log path: {}.\\", this.port, this.logPath);\\n        LOGGER.info(\\"seata server started in {} millSeconds\\", cost);\\n    } catch (Throwable e) {\\n        started = Boolean.FALSE;\\n        LOGGER.error(\\"seata server start error: {} \\", e.getMessage(), e);\\n        System.exit(-1);\\n    }\\n}\\n```\\n\\nThe core logic lies within the `seataServer.start()` method:\\n\\n```java\\n// org.apache.seata.server.Server#start\\npublic void start(String[] args) {\\n    // Parameter parser used to parse startup parameters from the shell script\\n    ParameterParser parameterParser = new ParameterParser(args);\\n    // Initialize metrics\\n    MetricsManager.get().init();\\n    ThreadPoolExecutor workingThreads = new ThreadPoolExecutor(\\n            NettyServerConfig.getMinServerPoolSize(),\\n            NettyServerConfig.getMaxServerPoolSize(),\\n            NettyServerConfig.getKeepAliveTime(), TimeUnit.SECONDS,\\n            new LinkedBlockingQueue<>(NettyServerConfig.getMaxTaskQueueSize()),\\n            new NamedThreadFactory(\\"ServerHandlerThread\\", NettyServerConfig.getMaxServerPoolSize()),\\n            new ThreadPoolExecutor.CallerRunsPolicy());\\n    // 127.0.0.1 and 0.0.0.0 are not valid here.\\n    if (NetUtil.isValidIp(parameterParser.getHost(), false)) {\\n        XID.setIpAddress(parameterParser.getHost());\\n    } else {\\n        String preferredNetworks = ConfigurationFactory.getInstance().getConfig(REGISTRY_PREFERED_NETWORKS);\\n        if (StringUtils.isNotBlank(preferredNetworks)) {\\n            XID.setIpAddress(NetUtil.getLocalIp(preferredNetworks.split(REGEX_SPLIT_CHAR)));\\n        } else {\\n            XID.setIpAddress(NetUtil.getLocalIp());\\n        }\\n    }\\n\\n    /**\\n     * Main tasks performed:\\n     * 1. Set workingThreads as the messageExecutor handler for AbstractNettyRemoting\\n     * 2. Create ServerBootstrap, configure Boss and Worker, and set the port that the Seata Server listens on\\n     * 3. Set outbound and inbound handlers ServerHandler, which is a composite handler of ChannelDuplexHandler\\n     */\\n    NettyRemotingServer nettyRemotingServer = new NettyRemotingServer(workingThreads);\\n    XID.setPort(nettyRemotingServer.getListenPort());\\n    UUIDGenerator.init(parameterParser.getServerNode());\\n    ConfigurableListableBeanFactory beanFactory = ((GenericWebApplicationContext) ObjectHolder.INSTANCE.getObject(OBJECT_KEY_SPRING_APPLICATION_CONTEXT)).getBeanFactory();\\n    DefaultCoordinator coordinator = DefaultCoordinator.getInstance(nettyRemotingServer);\\n    if (coordinator instanceof ApplicationListener) {\\n        beanFactory.registerSingleton(NettyRemotingServer.class.getName(), nettyRemotingServer);\\n        beanFactory.registerSingleton(DefaultCoordinator.class.getName(), coordinator);\\n        ((GenericWebApplicationContext) ObjectHolder.INSTANCE.getObject(OBJECT_KEY_SPRING_APPLICATION_CONTEXT)).addApplicationListener((ApplicationListener<?>) coordinator);\\n    }\\n    // Log store mode: file, db, redis\\n    SessionHolder.init();\\n    LockerManagerFactory.init();\\n    // Initialize a series of scheduled thread pools for retrying transaction commit/rollback, etc.\\n    coordinator.init();\\n    // Set the transaction processing Handler to DefaultCoordinator\\n    nettyRemotingServer.setHandler(coordinator);\\n    serverInstance.serverInstanceInit();\\n    // Let ServerRunner handle destruction instead of ShutdownHook, see https://github.com/seata/seata/issues/4028\\n    ServerRunner.addDisposable(coordinator);\\n    // Server initialization\\n    nettyRemotingServer.init();\\n}\\n```\\n\\nThe final `nettyRemotingServer.init()` is crucial for starting the entire Seata Server, primarily performing the following tasks:\\n\\n1. Register a series of handlers\\n2. Initialize a scheduled thread pool for cleaning up expired MessageFuture objects\\n3. Start the ServerBootstrap and register the TC service with the registry center, such as Nacos\\n\\n#### Registering Processors\\n\\nWithin Seata, a `Pair` object is used to associate a processor with an executor (thread pool), as shown below:\\n\\n```java\\npackage org.apache.seata.core.rpc.processor;\\n\\npublic final class Pair<T1, T2> {\\n\\n    private final T1 first;\\n    private final T2 second;\\n\\n    public Pair(T1 first, T2 second) {\\n        this.first = first;\\n        this.second = second;\\n    }\\n\\n    public T1 getFirst() {\\n        return first;\\n    }\\n\\n    public T2 getSecond() {\\n        return second;\\n    }\\n}\\n```\\n\\nRegistering processors essentially involves associating message types, the processors that handle those messages, and the specific thread pools for execution, all stored in a hash table.\\n\\n```java\\n// AbstractNettyRemotingServer\\nprotected final Map<Integer/*MessageType*/, Pair<RemotingProcessor, ExecutorService>> processorTable = new HashMap<>(32);\\n```\\n\\n```java\\n// org.apache.seata.core.rpc.netty.NettyRemotingServer#registerProcessor\\nprivate void registerProcessor() {\\n    // 1. Register request message processors\\n    ServerOnRequestProcessor onRequestProcessor = new ServerOnRequestProcessor(this, getHandler());\\n    ShutdownHook.getInstance().addDisposable(onRequestProcessor);\\n    super.registerProcessor(MessageType.TYPE_BRANCH_REGISTER, onRequestProcessor, messageExecutor);\\n    super.registerProcessor(MessageType.TYPE_BRANCH_STATUS_REPORT, onRequestProcessor, messageExecutor);\\n    super.registerProcessor(MessageType.TYPE_GLOBAL_BEGIN, onRequestProcessor, messageExecutor);\\n    super.registerProcessor(MessageType.TYPE_GLOBAL_COMMIT, onRequestProcessor, messageExecutor);\\n    super.registerProcessor(MessageType.TYPE_GLOBAL_LOCK_QUERY, onRequestProcessor, messageExecutor);\\n    super.registerProcessor(MessageType.TYPE_GLOBAL_REPORT, onRequestProcessor, messageExecutor);\\n    super.registerProcessor(MessageType.TYPE_GLOBAL_ROLLBACK, onRequestProcessor, messageExecutor);\\n    super.registerProcessor(MessageType.TYPE_GLOBAL_STATUS, onRequestProcessor, messageExecutor);\\n    super.registerProcessor(MessageType.TYPE_SEATA_MERGE, onRequestProcessor, messageExecutor);\\n    // 2. Register response message processors\\n    ServerOnResponseProcessor onResponseProcessor = new ServerOnResponseProcessor(getHandler(), getFutures());\\n    super.registerProcessor(MessageType.TYPE_BRANCH_COMMIT_RESULT, onResponseProcessor, branchResultMessageExecutor);\\n    super.registerProcessor(MessageType.TYPE_BRANCH_ROLLBACK_RESULT, onResponseProcessor, branchResultMessageExecutor);\\n    // 3. Register RM message processors\\n    RegRmProcessor regRmProcessor = new RegRmProcessor(this);\\n    super.registerProcessor(MessageType.TYPE_REG_RM, regRmProcessor, messageExecutor);\\n    // 4. Register TM message processors\\n    RegTmProcessor regTmProcessor = new RegTmProcessor(this);\\n    super.registerProcessor(MessageType.TYPE_REG_CLT, regTmProcessor, null);\\n    // 5. Register heartbeat message processors\\n    ServerHeartbeatProcessor heartbeatMessageProcessor = new ServerHeartbeatProcessor(this);\\n    super.registerProcessor(MessageType.TYPE_HEARTBEAT_MSG, heartbeatMessageProcessor, null);\\n}\\n\\n// org.apache.seata.core.rpc.netty.AbstractNettyRemotingServer#registerProcessor\\npublic void registerProcessor(int messageType, RemotingProcessor processor, ExecutorService executor) {\\n    Pair<RemotingProcessor, ExecutorService> pair = new Pair<>(processor, executor);\\n    this.processorTable.put(messageType, pair);\\n}\\n```\\n\\nYou might notice that during the registration of some processors, the passed-in thread pool is `null`. In such cases, which thread will execute the corresponding message?\\n\\nWe will discuss this in a later section.\\n\\n#### Initializing the Scheduled Thread Pool\\n\\n```java\\n// org.apache.seata.core.rpc.netty.AbstractNettyRemoting#init\\npublic void init() {\\n    timerExecutor.scheduleAtFixedRate(() -> {\\n        for (Map.Entry<Integer, MessageFuture> entry : futures.entrySet()) {\\n            MessageFuture future = entry.getValue();\\n            if (future.isTimeout()) {\\n                futures.remove(entry.getKey());\\n                RpcMessage rpcMessage = future.getRequestMessage();\\n                future.setResultMessage(new TimeoutException(String.format(\\"msgId: %s, msgType: %s, msg: %s, request timeout\\",\\n                        rpcMessage.getId(), String.valueOf(rpcMessage.getMessageType()), rpcMessage.getBody().toString())));\\n                if (LOGGER.isDebugEnabled()) {\\n                    LOGGER.debug(\\"timeout clear future: {}\\", entry.getValue().getRequestMessage().getBody());\\n                }\\n            }\\n        }\\n        nowMills = System.currentTimeMillis();\\n    }, TIMEOUT_CHECK_INTERVAL, TIMEOUT_CHECK_INTERVAL, TimeUnit.MILLISECONDS);\\n}\\n```\\n\\nThere\'s not much to explain here\u2014it initializes a scheduled thread pool that periodically cleans up timed-out `MessageFuture` objects. The `MessageFuture` is key to Seata converting asynchronous calls into synchronous ones, which we will discuss in detail later.\\n\\n#### Starting the ServerBootstrap\\n\\nFinally, starting the `ServerBootstrap` is mostly related to Netty.\\n\\n```java\\n// org.apache.seata.core.rpc.netty.NettyServerBootstrap#start\\npublic void start() {\\n    int port = getListenPort();\\n    this.serverBootstrap.group(this.eventLoopGroupBoss, this.eventLoopGroupWorker)\\n            .channel(NettyServerConfig.SERVER_CHANNEL_CLAZZ)\\n            .option(ChannelOption.SO_BACKLOG, nettyServerConfig.getSoBackLogSize())\\n            .option(ChannelOption.SO_REUSEADDR, true)\\n            .childOption(ChannelOption.SO_KEEPALIVE, true)\\n            .childOption(ChannelOption.TCP_NODELAY, true)\\n            .childOption(ChannelOption.SO_SNDBUF, nettyServerConfig.getServerSocketSendBufSize())\\n            .childOption(ChannelOption.SO_RCVBUF, nettyServerConfig.getServerSocketResvBufSize())\\n            .childOption(ChannelOption.WRITE_BUFFER_WATER_MARK, new WriteBufferWaterMark(nettyServerConfig.getWriteBufferLowWaterMark(), nettyServerConfig.getWriteBufferHighWaterMark()))\\n            .localAddress(new InetSocketAddress(port))\\n            .childHandler(new ChannelInitializer<SocketChannel>() {\\n                @Override\\n                public void initChannel(SocketChannel ch) {\\n                    // Multi-version protocol decoder\\n                    MultiProtocolDecoder multiProtocolDecoder = new MultiProtocolDecoder(channelHandlers);\\n                    ch.pipeline()\\n                            .addLast(new IdleStateHandler(nettyServerConfig.getChannelMaxReadIdleSeconds(), 0, 0))\\n                            .addLast(multiProtocolDecoder);\\n                }\\n            });\\n    try {\\n        this.serverBootstrap.bind(port).sync();\\n        LOGGER.info(\\"Server started, service listen port: {}\\", getListenPort());\\n        InetSocketAddress address = new InetSocketAddress(XID.getIpAddress(), XID.getPort());\\n        for (RegistryService<?> registryService : MultiRegistryFactory.getInstances()) {\\n            // Register service\\n            registryService.register(address);\\n        }\\n        initialized.set(true);\\n    } catch (SocketException se) {\\n        throw new RuntimeException(\\"Server start failed, the listen port: \\" + getListenPort(), se);\\n    } catch (Exception exx) {\\n        throw new RuntimeException(\\"Server start failed\\", exx);\\n    }\\n}\\n```\\n\\nThe `childOption` settings during the startup of `ServerBootstrap` belong to the networking part and won\'t be explained in depth here.\\n\\nYou might have a question regarding why only a `MultiProtocolDecoder` is added to the pipeline, what about the business handler?\\n\\nIn fact, the `channelHandlers` passed into the constructor of `MultiProtocolDecoder` include the `ServerHandler`, which is set when creating the `NettyRemotingServer`.\\n\\nThis approach is related to Seata\'s multi-version protocol support.\\n\\nWhen the Seata Server decodes messages for the first time after starting, it removes the `MultiProtocolDecoder` from the pipeline and adds specific `Encoder` and `Decoder` based on the version to the pipeline. At this point, the `ServerHandler` is also added to the pipeline.\\n\\n### How the Client Starts\\n\\nFor the Client, since we typically use Seata within a Spring Boot application, our focus lies within the `SeataAutoConfiguration` class.\\n\\nIn this class, a `GlobalTransactionScanner` object is created. Notably, it implements `InitializingBean`, so we turn our attention to the `afterPropertiesSet` method.\\n\\nIndeed, within this method, the initialization of TM (Transaction Manager) and RM (Resource Manager) takes place.\\n\\n#### Initialization of TM\\n\\nFor TM, the initialization logic is as follows:\\n\\n```java\\npublic static void init(String applicationId, String transactionServiceGroup, String accessKey, String secretKey) {\\n    /**\\n     * Main tasks include:\\n     * 1. Creating a thread pool as the messageExecutor for AbstractNettyRemotingClient\\n     * 2. Setting the transaction role transactionRole to TM_ROLE\\n     * 3. Creating Bootstrap and setting outbound and inbound handlers ClientHandler\\n     * 4. Creating a client Channel manager NettyClientChannelManager\\n     */\\n    TmNettyRemotingClient tmNettyRemotingClient = TmNettyRemotingClient.getInstance(applicationId, transactionServiceGroup, accessKey, secretKey);\\n\\n    /**\\n     * Main tasks include:\\n     * 1. Registering a series of processors\\n     * 2. Creating a scheduled thread pool that periodically initiates connections to servers within the transaction group; if the connection is broken, it tries to reconnect\\n     * 3. If the client allows batch message sending, creating a mergeSendExecutorService thread pool and submitting MergedSendRunnable tasks\\n     * 4. Initializing a scheduled thread pool to clean up expired MessageFuture objects\\n     * 5. Starting the client Bootstrap\\n     * 6. Initializing connections initConnection\\n     */\\n    tmNettyRemotingClient.init();\\n}\\n```\\n\\nThe logic for starting the client Bootstrap is as follows:\\n\\n```java\\n@Override\\npublic void start() {\\n    if (this.defaultEventExecutorGroup == null) {\\n        this.defaultEventExecutorGroup = new DefaultEventExecutorGroup(nettyClientConfig.getClientWorkerThreads(),\\n                new NamedThreadFactory(getThreadPrefix(nettyClientConfig.getClientWorkerThreadPrefix()), nettyClientConfig.getClientWorkerThreads()));\\n    }\\n    this.bootstrap.group(this.eventLoopGroupWorker)\\n            .channel(nettyClientConfig.getClientChannelClazz())\\n            .option(ChannelOption.TCP_NODELAY, true)\\n            .option(ChannelOption.SO_KEEPALIVE, true)\\n            .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, nettyClientConfig.getConnectTimeoutMillis())\\n            .option(ChannelOption.SO_SNDBUF, nettyClientConfig.getClientSocketSndBufSize())\\n            .option(ChannelOption.SO_RCVBUF, nettyClientConfig.getClientSocketRcvBufSize());\\n    if (nettyClientConfig.enableNative()) {\\n        if (PlatformDependent.isOsx()) {\\n            if (LOGGER.isInfoEnabled()) {\\n                LOGGER.info(\\"client run on macOS\\");\\n            }\\n        } else {\\n            bootstrap.option(EpollChannelOption.EPOLL_MODE, EpollMode.EDGE_TRIGGERED)\\n                    .option(EpollChannelOption.TCP_QUICKACK, true);\\n        }\\n    }\\n    bootstrap.handler(new ChannelInitializer<SocketChannel>() {\\n        @Override\\n        public void initChannel(SocketChannel ch) {\\n            ch.pipeline().addLast(new IdleStateHandler(nettyClientConfig.getChannelMaxReadIdleSeconds(),\\n                            nettyClientConfig.getChannelMaxWriteIdleSeconds(),\\n                            nettyClientConfig.getChannelMaxAllIdleSeconds()))\\n                    .addLast(new ProtocolDecoderV1())\\n                    .addLast(new ProtocolEncoderV1());\\n            if (channelHandlers != null) {\\n                addChannelPipelineLast(ch, channelHandlers);\\n            }\\n        }\\n    });\\n    if (initialized.compareAndSet(false, true) && LOGGER.isInfoEnabled()) {\\n        LOGGER.info(\\"NettyClientBootstrap has started\\");\\n    }\\n}\\n```\\n\\nSince the protocol version for the client can be determined based on different versions of Seata, V1 version encoders and decoders are directly added here. The `channelHandlers` are actually the `ClientHandler`, which is also a composite handler in Netty.\\n\\n#### Initialization of RM\\n\\nThe initialization logic for RM is largely similar to that of TM and will not be elaborated on further here.\\n\\n## How Messages Are Sent and Handled\\n\\nAfter understanding the general startup processes of the Seata Server and Client, we can delve deeper into how Seata sends and handles messages.\\n\\nWe mentioned earlier that the core logic for sending requests and processing messages lies within `AbstractNettyRemoting`. Let\'s take a closer look at this class.\\n\\n### Synchronous and Asynchronous\\n\\nFirst, let\'s briefly discuss what synchronous and asynchronous mean.\\n\\nSynchronous (Synchronous) and Asynchronous (Asynchronous), in essence, describe different behavior patterns when a program handles multiple events or tasks.\\n\\nSynchronous means one process must wait for another to complete before it can proceed. In other words, in synchronous operations, the caller will block waiting for a response after issuing a request until it receives a response result or times out before continuing with subsequent code execution.\\n\\nIn contrast, asynchronous allows the caller to continue executing without waiting for a response after making a request, but when the request is completed, it notifies the caller of the response in some way (such as through callback functions or Future). The asynchronous model can improve concurrency and efficiency.\\n\\nFrom another perspective, synchronous calls require the calling thread to obtain the result, whereas asynchronous calls either have an asynchronous thread place the result somewhere (Future) or execute pre-prepared call success/failure callback methods (callback function).\\n\\nBelow is a simple example demonstrating three invocation styles: synchronous, asynchronous with Future, and asynchronous with Callback.\\n\\n```java\\nimport lombok.Data;\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\n\\nimport java.util.concurrent.CompletableFuture;\\nimport java.util.concurrent.ExecutionException;\\nimport java.util.concurrent.TimeUnit;\\n\\npublic class AsyncTest {\\n\\n    private static final Logger LOGGER = LoggerFactory.getLogger(AsyncTest.class);\\n\\n    public static void main(String[] args) throws InterruptedException, ExecutionException {\\n        Result syncResponse = testSync();\\n        LOGGER.info(\\"Synchronous response result: {}\\", syncResponse.getString());\\n        CompletableFuture<Result> result = testAsyncFuture();\\n        testAsyncCallback();\\n        LOGGER.info(\\"Main thread continues executing~~\\");\\n        TimeUnit.SECONDS.sleep(1); // Ensure all results are processed\\n        LOGGER.info(\\"Main thread retrieves result from async Future: {}\\", result.get().getString());\\n    }\\n\\n    public static void testAsyncCallback() {\\n        new AsyncTask().execute(new AsyncCallback() {\\n            @Override\\n            public void onComplete(Result result) {\\n                try {\\n                    TimeUnit.MILLISECONDS.sleep(50); // Simulate asynchronous delay\\n                } catch (InterruptedException e) {\\n                }\\n                LOGGER.info(\\"Async Callback gets result: {}\\", result.getString());\\n            }\\n        });\\n    }\\n\\n    public static CompletableFuture<Result> testAsyncFuture() {\\n        return CompletableFuture.supplyAsync(() -> {\\n            try {\\n                TimeUnit.MILLISECONDS.sleep(50); // Simulate asynchronous delay\\n            } catch (InterruptedException e) {\\n            }\\n            Result asyncResponse = getResult();\\n            LOGGER.info(\\"Async Future gets result: {}\\", asyncResponse.getString());\\n            return asyncResponse;\\n        });\\n    }\\n\\n    public static Result testSync() {\\n        return getResult();\\n    }\\n\\n    @Data\\n    static class Result {\\n        private String string;\\n    }\\n\\n    interface AsyncCallback {\\n        void onComplete(Result result);\\n    }\\n\\n    static class AsyncTask {\\n        void execute(AsyncCallback callback) {\\n            new Thread(() -> {\\n                Result asyncRes = getResult();\\n                callback.onComplete(asyncRes);\\n            }).start();\\n        }\\n    }\\n\\n    private static Result getResult() {\\n        Result result = new Result();\\n        result.setString(\\"result\\");\\n        return result;\\n    }\\n}\\n```\\n\\nOutput:\\n\\n```java\\n22:26:38.788 [main] INFO  org.hein.netty.AsyncTest - Synchronous response result: result\\n22:26:38.849 [main] INFO  org.hein.netty.AsyncTest - Main thread continues executing~~\\n22:26:38.911 [Thread-0] INFO  org.hein.netty.AsyncTest - Async Callback gets result: result\\n22:26:38.911 [ForkJoinPool.commonPool-worker-1] INFO  org.hein.netty.AsyncTest - Async Future gets result: result\\n22:26:39.857 [main] INFO  org.hein.netty.AsyncTest - Main thread retrieves result from async Future: result\\n```\\n\\nFrom the output, we can observe at least three points:\\n\\n+ One is that asynchronous Future and asynchronous Callback do not block the main thread from continuing its execution.\\n+ Two, the handling of results during asynchronous calls is not done by the main thread.\\n+ Finally, the difference between Future and Callback lies in that Future has the asynchronous thread store the result in a specific location (CompletableFuture#result), but retrieving the result still requires the main thread (or another thread) to call the get method. With Callback, it\'s essentially setting up the predefined way to handle the result, which is executed by the asynchronous thread.\\n\\nOf course, `CompletableFuture` can also be used for callbacks, for example, by calling the `whenComplete` method.\\n\\n### Asynchronous Invocation\\n\\nNetty, as a high-performance asynchronous IO framework, is designed to be asynchronous at its core. Therefore, implementing asynchronous calls based on Netty is relatively straightforward.\\n\\n```java\\nprotected void sendAsync(Channel channel, RpcMessage rpcMessage) {\\n    channelWritableCheck(channel, rpcMessage.getBody());\\n    if (LOGGER.isDebugEnabled()) {\\n        LOGGER.debug(\\"write message: {}, channel: {}, active? {}, writable? {}, isopen? {}\\", rpcMessage.getBody(), channel, channel.isActive(), channel.isWritable(), channel.isOpen());\\n    }\\n    doBeforeRpcHooks(ChannelUtil.getAddressFromChannel(channel), rpcMessage);\\n    channel.writeAndFlush(rpcMessage).addListener((ChannelFutureListener) future -> {\\n        if (!future.isSuccess()) {\\n            destroyChannel(future.channel());\\n        }\\n    });\\n}\\n```\\n\\nAn asynchronous call can be achieved by simply invoking the `writeAndFlush` method of the channel.\\n\\nIt\'s important to note that the `writeAndFlush` method will operate synchronously when called from an EventLoop thread.\\n\\n### Synchronous Invocation\\n\\nImplementing asynchronous calls in Netty is simple, but converting them into synchronous calls requires more effort since it involves transforming an asynchronous call into a synchronous one.\\n\\nEssentially, converting asynchronous to synchronous means that after the calling thread initiates a call, it should block until it receives a response, and then it continues execution.\\n\\nThe core of Seata\'s handling for this conversion lies within the `MessageFuture` class, as follows:\\n\\n```java\\npackage org.apache.seata.core.protocol;\\n\\nimport org.apache.seata.common.exception.ShouldNeverHappenException;\\n\\nimport java.util.concurrent.CompletableFuture;\\nimport java.util.concurrent.ExecutionException;\\nimport java.util.concurrent.TimeUnit;\\nimport java.util.concurrent.TimeoutException;\\n\\npublic class MessageFuture {\\n\\n    private RpcMessage requestMessage;\\n    private long timeout;\\n    private final long start = System.currentTimeMillis();\\n\\n    private final transient CompletableFuture<Object> origin = new CompletableFuture<>();\\n\\n    public boolean isTimeout() {\\n        return System.currentTimeMillis() - start > timeout;\\n    }\\n\\n    public Object get(long timeout, TimeUnit unit) throws TimeoutException, InterruptedException {\\n        Object result;\\n        try {\\n            result = origin.get(timeout, unit);\\n            if (result instanceof TimeoutException) {\\n                throw (TimeoutException) result;\\n            }\\n        } catch (ExecutionException e) {\\n            throw new ShouldNeverHappenException(\\"Should not get results in a multi-threaded environment\\", e);\\n        } catch (TimeoutException e) {\\n            throw new TimeoutException(String.format(\\"%s, cost: %d ms\\", e.getMessage(), System.currentTimeMillis() - start));\\n        }\\n        if (result instanceof RuntimeException) {\\n            throw (RuntimeException) result;\\n        } else if (result instanceof Throwable) {\\n            throw new RuntimeException((Throwable) result);\\n        }\\n        return result;\\n    }\\n\\n    public void setResultMessage(Object obj) {\\n        origin.complete(obj);\\n    }\\n\\n    public RpcMessage getRequestMessage() { return requestMessage; }\\n\\n    public void setRequestMessage(RpcMessage requestMessage) { this.requestMessage = requestMessage;}\\n\\n    public long getTimeout() { return timeout; }\\n\\n    public void setTimeout(long timeout) { this.timeout = timeout;}\\n}\\n```\\n\\nWith this class, the process of a synchronous call works as follows, using a client request and server response as an example:\\n\\n+ First, the client constructs the request into a `MessageFuture`, then stores the request ID along with this `MessageFuture` object in a hash table.\\n+ The client then calls `channel.writeAndFlush` to initiate an asynchronous call. Yes, it\'s still asynchronous at this point.\\n+ The key to converting asynchronous to synchronous lies in the fact that the thread needs to call the `get` method on the `MessageFuture` object, which blocks the thread, effectively calling the `get` method on `CompletableFuture` to enter a blocking state.\\n+ When the server finishes processing and sends a request from its perspective, the client sees this as a response.\\n+ When the client receives the response, the EventLoop thread sets the response result in the `MessageFuture`. Since the request and response IDs are the same, the corresponding `MessageFuture` object can be retrieved from the aforementioned hash table.\\n+ Once the response result is set, the previously blocked thread can resume execution, thereby achieving a synchronous effect.\\n\\nThus, Seata\'s solution essentially uses `CompletableFuture` objects as containers for storing results.\\n\\n```java\\nprotected Object sendSync(Channel channel, RpcMessage rpcMessage, long timeoutMillis) throws TimeoutException {\\n    if (timeoutMillis <= 0) {\\n        throw new FrameworkException(\\"timeout should more than 0ms\\");\\n    }\\n    if (channel == null) {\\n        LOGGER.warn(\\"sendSync nothing, caused by null channel.\\");\\n        return null;\\n    }\\n    MessageFuture messageFuture = new MessageFuture();\\n    messageFuture.setRequestMessage(rpcMessage);\\n    messageFuture.setTimeout(timeoutMillis);\\n    futures.put(rpcMessage.getId(), messageFuture); // The request and response IDs are the same\\n    // Check if the Channel is writable (Channels have write buffers, and if the buffer reaches a threshold water level, it becomes unwritable)\\n    channelWritableCheck(channel, rpcMessage.getBody());\\n    // Get the destination IP address\\n    String remoteAddr = ChannelUtil.getAddressFromChannel(channel);\\n    // Execute pre-send hooks\\n    doBeforeRpcHooks(remoteAddr, rpcMessage);\\n    // Send the result and set up a callback, non-blocking\\n    channel.writeAndFlush(rpcMessage).addListener((ChannelFutureListener) future -> {\\n        // If sending fails, remove the future and close the Channel\\n        if (!future.isSuccess()) {\\n            MessageFuture mf = futures.remove(rpcMessage.getId());\\n            if (mf != null) {\\n                mf.setResultMessage(future.cause());\\n            }\\n            destroyChannel(future.channel());\\n        }\\n    });\\n    try {\\n        // Since Netty sends asynchronously, we need to wait for the result here, converting async to sync\\n        Object result = messageFuture.get(timeoutMillis, TimeUnit.MILLISECONDS);\\n        // Execute post-send hooks\\n        doAfterRpcHooks(remoteAddr, rpcMessage, result);\\n        return result;\\n    } catch (Exception exx) {\\n        LOGGER.error(\\"wait response error:{},ip:{},request:{}\\", exx.getMessage(), channel.remoteAddress(), rpcMessage.getBody());\\n        // Timeout exception\\n        if (exx instanceof TimeoutException) {\\n            throw (TimeoutException) exx;\\n        } else {\\n            throw new RuntimeException(exx);\\n        }\\n    }\\n}\\n```\\n\\n### Message Handling\\n\\nWhen it comes to message handling in Netty, one should think of inbound and outbound handlers first.\\n\\nIn the Seata Server side, besides common encoding and decoding handlers, there is also the `ServerHandler`. Here\'s an example:\\n\\n```java\\n@ChannelHandler.Sharable\\nclass ServerHandler extends ChannelDuplexHandler {\\n\\n    @Override\\n    public void channelRead(final ChannelHandlerContext ctx, Object msg) throws Exception {\\n        // Preceded by a decoder handler, so the message here is RpcMessage\\n        if (msg instanceof RpcMessage) {\\n            processMessage(ctx, (RpcMessage) msg);\\n        } else {\\n            LOGGER.error(\\"rpcMessage type error\\");\\n        }\\n    }\\n\\n    // ...\\n}\\n```\\n\\nThe `channelRead` method has significant business meaning, as all messages sent to the Server will come to this method after being decoded.\\n\\nThe `processMessage` method within this context refers to the business processing method found in `AbstractNettyRemoting`, as follows:\\n\\n```java\\nprotected void processMessage(ChannelHandlerContext ctx, RpcMessage rpcMessage) throws Exception {\\n    if (LOGGER.isDebugEnabled()) {\\n        LOGGER.debug(\\"{} msgId: {}, body: {}\\", this, rpcMessage.getId(), rpcMessage.getBody());\\n    }\\n    Object body = rpcMessage.getBody();\\n    if (body instanceof MessageTypeAware) {\\n        MessageTypeAware messageTypeAware = (MessageTypeAware) body;\\n        // During Server startup, a lot of processors are registered with processorTable\\n        final Pair<RemotingProcessor, ExecutorService> pair = this.processorTable.get((int) messageTypeAware.getTypeCode());\\n        if (pair != null) {\\n            // Execute with the corresponding thread pool\\n            if (pair.getSecond() != null) {\\n                try {\\n                    pair.getSecond().execute(() -> {\\n                        try {\\n                            // Find the corresponding processor to execute\\n                            pair.getFirst().process(ctx, rpcMessage);\\n                        } catch (Throwable th) {\\n                            LOGGER.error(FrameworkErrorCode.NetDispatch.getErrCode(), th.getMessage(), th);\\n                        } finally {\\n                            MDC.clear();\\n                        }\\n                    });\\n                } catch (RejectedExecutionException e) {\\n                    // Thread pool is full, execute rejection policy\\n                    LOGGER.error(FrameworkErrorCode.ThreadPoolFull.getErrCode(), \\"thread pool is full, current max pool size is \\" + messageExecutor.getActiveCount());\\n                    if (allowDumpStack) {\\n                        // Export thread stack information\\n                        String name = ManagementFactory.getRuntimeMXBean().getName();\\n                        String pid = name.split(\\"@\\")[0];\\n                        long idx = System.currentTimeMillis();\\n                        try {\\n                            String jstackFile = idx + \\".log\\";\\n                            LOGGER.info(\\"jstack command will dump to {}\\", jstackFile);\\n                            Runtime.getRuntime().exec(String.format(\\"jstack %s > %s\\", pid, jstackFile));\\n                        } catch (IOException exx) {\\n                            LOGGER.error(exx.getMessage());\\n                        }\\n                        allowDumpStack = false;\\n                    }\\n                }\\n            } else {\\n                try {\\n                    // If no thread pool is configured for the processor, it is executed by the current thread, which is basically the EventLoop thread\\n                    pair.getFirst().process(ctx, rpcMessage);\\n                } catch (Throwable th) {\\n                    LOGGER.error(FrameworkErrorCode.NetDispatch.getErrCode(), th.getMessage(), th);\\n                }\\n            }\\n        } else {\\n            LOGGER.error(\\"This message type [{}] has no processor.\\", messageTypeAware.getTypeCode());\\n        }\\n    } else {\\n        LOGGER.error(\\"This rpcMessage body[{}] is not MessageTypeAware type.\\", body);\\n    }\\n}\\n```\\n\\nThe logic of this method is quite straightforward.\\n\\nDuring the startup process of Seata Server, a multitude of processors are registered into the `processorTable`, so here we can obtain the corresponding processor and thread pool based on the message type code.\\n\\nIf there is a thread pool, the processor\'s method is executed within that thread pool; otherwise, it is handed over to the EventLoop thread for execution.\\n\\nOf course, the same approach applies to the Client. \\n\\n### Batch Sending\\n\\nIn network programming, there are times when batch sending is also required. Let\'s see how Seata implements this, focusing on the client sending to the server.\\n\\nRecall that during the Client startup process, we mentioned a thread pool `mergeSendExecutorService`. If batch sending is allowed, then upon Client startup, a `MergedSendRunnable` task is submitted. First, let\'s look at what this task does:\\n\\n```java\\nprivate class MergedSendRunnable implements Runnable {\\n\\n    @Override\\n    public void run() {\\n        // Infinite loop\\n        while (true) {\\n            synchronized (mergeLock) {\\n                try {\\n                    // Ensure the thread idles for no more than 1ms\\n                    mergeLock.wait(MAX_MERGE_SEND_MILLS); // 1\\n                } catch (InterruptedException ignore) {\\n                    // ignore\\n                }\\n            }\\n            // Flag indicating sending in progress\\n            isSending = true;\\n            // basketMap: key is address, value is the queue of messages (blocking queue) to be sent to that address\\n            basketMap.forEach((address, basket) -> {\\n                if (basket.isEmpty()) {\\n                    return;\\n                }\\n                MergedWarpMessage mergeMessage = new MergedWarpMessage();\\n                while (!basket.isEmpty()) {\\n                    // Merge all RpcMessages from the same blocking queue\\n                    RpcMessage msg = basket.poll();\\n                    mergeMessage.msgs.add((AbstractMessage) msg.getBody());\\n                    mergeMessage.msgIds.add(msg.getId());\\n                }\\n                if (mergeMessage.msgIds.size() > 1) {\\n                    printMergeMessageLog(mergeMessage);\\n                }\\n                Channel sendChannel = null;\\n                try {\\n                    // Batch message sending is a synchronous request but doesn\'t require a return value.\\n                    // Because messageFuture is created before putting the message into basketMap.\\n                    // The return value will be set in ClientOnResponseProcessor.\\n                    sendChannel = clientChannelManager.acquireChannel(address);\\n                    // Internally wraps mergeMessage as a regular RpcMessage and sends it\\n                    AbstractNettyRemotingClient.this.sendAsyncRequest(sendChannel, mergeMessage);\\n                } catch (FrameworkException e) {\\n                    if (e.getErrorCode() == FrameworkErrorCode.ChannelIsNotWritable && sendChannel != null) {\\n                        destroyChannel(address, sendChannel);\\n                    }\\n                    // Fast fail\\n                    for (Integer msgId : mergeMessage.msgIds) {\\n                        MessageFuture messageFuture = futures.remove(msgId);\\n                        if (messageFuture != null) {\\n                            messageFuture.setResultMessage(new RuntimeException(String.format(\\"%s is unreachable\\", address), e));\\n                        }\\n                    }\\n                    LOGGER.error(\\"client merge call failed: {}\\", e.getMessage(), e);\\n                }\\n            });\\n            isSending = false;\\n        }\\n    }\\n}\\n```\\n\\nThe related batch sending code follows:\\n\\n```java\\npublic Object sendSyncRequest(Object msg) throws TimeoutException {\\n    String serverAddress = loadBalance(getTransactionServiceGroup(), msg);\\n    long timeoutMillis = this.getRpcRequestTimeout();\\n    RpcMessage rpcMessage = buildRequestMessage(msg, ProtocolConstants.MSGTYPE_RESQUEST_SYNC);\\n    // Send batch message\\n    // Put message into basketMap, @see MergedSendRunnable\\n    if (this.isEnableClientBatchSendRequest()) {\\n        // If client-side batch message sending is enabled\\n        // Sending batch messages is a sync request, which needs to create messageFuture and put it in futures.\\n        MessageFuture messageFuture = new MessageFuture();\\n        messageFuture.setRequestMessage(rpcMessage);\\n        messageFuture.setTimeout(timeoutMillis);\\n        futures.put(rpcMessage.getId(), messageFuture);\\n\\n        // Put message into basketMap\\n        // Get the sending queue corresponding to serverAddress\\n        BlockingQueue<RpcMessage> basket = CollectionUtils.computeIfAbsent(basketMap, serverAddress,\\n                key -> new LinkedBlockingQueue<>());\\n        // Add the message to the queue, waiting for mergeSendExecutorService to perform the actual sending\\n        if (!basket.offer(rpcMessage)) {\\n            LOGGER.error(\\"put message into basketMap offer failed, serverAddress: {}, rpcMessage: {}\\", serverAddress, rpcMessage);\\n            return null;\\n        }\\n        if (!isSending) {\\n            // Ensure that once there is data in the queue, the thread is awakened to continue batch sending\\n            synchronized (mergeLock) {\\n                mergeLock.notifyAll();\\n            }\\n        }\\n        try {\\n            // Thread blocks waiting for response\\n            return messageFuture.get(timeoutMillis, TimeUnit.MILLISECONDS);\\n        } catch (Exception exx) {\\n            LOGGER.error(\\"wait response error: {}, ip: {}, request: {}\\", exx.getMessage(), serverAddress, rpcMessage.getBody());\\n            if (exx instanceof TimeoutException) {\\n                throw (TimeoutException) exx;\\n            } else {\\n                throw new RuntimeException(exx);\\n            }\\n        }\\n    } else {\\n        // Normal sending, acquire channel and call the parent class\'s synchronous method\\n        Channel channel = clientChannelManager.acquireChannel(serverAddress);\\n        return super.sendSync(channel, rpcMessage, timeoutMillis);\\n    }\\n}\\n```\\n\\nAs can be seen, object lock synchronization-wait mechanisms are used here, resulting in the following effects:\\n\\n1. Messages are sent by traversing the `basketMap` every 1ms at most.\\n2. During the blocking period of threads inside `mergeSendExecutorService` (mainLock.wait), if a message that needs to be sent arrives, the thread on `mainLock` is awakened to continue sending.\\n\\nHow does the Server handle this? It mainly looks at the `TypeCode` of the `MergedWarpMessage`, which is actually `TYPE_SEATA_MERGE`. During Server startup, the processor registered for this Code is actually `ServerOnRequestProcessor`.\\n\\n> This shows you how to find out how a certain message is processed; teaching you how to fish is better than giving you fish!\\n\\nOn the `ServerOnRequestProcessor` side, there are actually two ways to handle `MergedWarpMessage` messages:\\n\\n1. After processing all individual requests within `MergedWarpMessage`, send a unified `MergeResultMessage`.\\n2. Handle the sending task with the `batchResponseExecutorService` thread pool, ensuring two points: one is to respond immediately when there is a message result, even if the thread is waiting, it will notify it, and secondly, it responds at least once every 1ms because the thread executing within `batchResponseExecutorService` waits for no more than 1ms.\\n\\nNote that these two methods respond with different message types; the first responds with `MergeResultMessage`, and the second with `BatchResultMessage`, each handled differently on the Client side.\\n\\nThe core processing method within `ServerOnRequestProcessor` is as follows:\\n\\n```java\\nprivate void onRequestMessage(ChannelHandlerContext ctx, RpcMessage rpcMessage) {\\n    Object message = rpcMessage.getBody();\\n    RpcContext rpcContext = ChannelManager.getContextFromIdentified(ctx.channel());\\n    // the batch send request message\\n    if (message instanceof MergedWarpMessage) {\\n        final List<AbstractMessage> msgs = ((MergedWarpMessage) message).msgs;\\n        final List<Integer> msgIds = ((MergedWarpMessage) message).msgIds;\\n        // Allow TC server to batch return results && client version >= 1.5.0\\n        if (NettyServerConfig.isEnableTcServerBatchSendResponse() && StringUtils.isNotBlank(rpcContext.getVersion())\\n                && Version.isAboveOrEqualVersion150(rpcContext.getVersion())) {\\n            // Handled by `batchResponseExecutorService` individually without waiting for all batch requests to complete\\n            for (int i = 0; i < msgs.size(); i++) {\\n                if (PARALLEL_REQUEST_HANDLE) {\\n                    int finalI = i;\\n                    CompletableFuture.runAsync(\\n                            () -> handleRequestsByMergedWarpMessageBy150(msgs.get(finalI), msgIds.get(finalI), rpcMessage, ctx, rpcContext));\\n                } else {\\n                    handleRequestsByMergedWarpMessageBy150(msgs.get(i), msgIds.get(i), rpcMessage, ctx, rpcContext);\\n                }\\n            }\\n        } else {\\n            // Responses are sent only after each request has been processed\\n            List<AbstractResultMessage> results = new ArrayList<>();\\n            List<CompletableFuture<AbstractResultMessage>> futures = new ArrayList<>();\\n            for (int i = 0; i < msgs.size(); i++) {\\n                if (PARALLEL_REQUEST_HANDLE) {\\n                    int finalI = i;\\n                    futures.add(CompletableFuture.supplyAsync(() -> handleRequestsByMergedWarpMessage(msgs.get(finalI), rpcContext)));\\n                } else {\\n                    results.add(i, handleRequestsByMergedWarpMessage(msgs.get(i), rpcContext));\\n                }\\n            }\\n            if (CollectionUtils.isNotEmpty(futures)) {\\n                try {\\n                    for (CompletableFuture<AbstractResultMessage> future : futures) {\\n                        results.add(future.get()); // Blocking wait for processing result\\n                    }\\n                } catch (InterruptedException | ExecutionException e) {\\n                    LOGGER.error(\\"handle request error: {}\\", e.getMessage(), e);\\n                }\\n            }\\n            MergeResultMessage resultMessage = new MergeResultMessage();\\n            resultMessage.setMsgs(results.toArray(new AbstractResultMessage[0]));\\n            remotingServer.sendAsyncResponse(rpcMessage, ctx.channel(), resultMessage);\\n        }\\n    } else {\\n        // Handle individual message response\\n    }\\n}\\n```\\n\\nThe difference between `handleRequestsByMergedWarpMessage` and `handleRequestsByMergedWarpMessageBy150` lies in the fact that the latter encapsulates the result into a `QueueItem` and adds it to a blocking queue for actual sending by threads in `batchResponseExecutorService`, while the former simply returns the processing result.\\n\\n```java\\nprivate AbstractResultMessage handleRequestsByMergedWarpMessage(AbstractMessage subMessage, RpcContext rpcContext) {\\n    AbstractResultMessage resultMessage = transactionMessageHandler.onRequest(subMessage, rpcContext);\\n    return resultMessage;\\n}\\n\\nprivate void handleRequestsByMergedWarpMessageBy150(AbstractMessage msg, int msgId, RpcMessage rpcMessage,\\n                                                    ChannelHandlerContext ctx, RpcContext rpcContext) {\\n    AbstractResultMessage resultMessage = transactionMessageHandler.onRequest(msg, rpcContext);\\n    // Get the sending queue corresponding to the channel\\n    BlockingQueue<QueueItem> msgQueue = CollectionUtils.computeIfAbsent(basketMap, ctx.channel(), key -> new LinkedBlockingQueue<>());\\n    // Add the result to the queue, waiting for `batchResponseExecutorService` thread pool to perform the actual sending\\n    if (!msgQueue.offer(new QueueItem(resultMessage, msgId, rpcMessage))) {\\n        LOGGER.error(\\"put message into basketMap offer failed, channel: {}, rpcMessage: {}, resultMessage: {}\\", ctx.channel(), rpcMessage, resultMessage);\\n    }\\n    if (!isResponding) {\\n        // Ensure that once there is data in the queue, the thread is awakened to perform batch sending\\n        synchronized (batchResponseLock) {\\n            batchResponseLock.notifyAll();\\n        }\\n    }\\n}\\n```\\n\\nNow, let\'s look at how the `batchResponseExecutorService` thread pool handles batch sending tasks:\\n\\n```java\\nprivate class BatchResponseRunnable implements Runnable {\\n    @Override\\n    public void run() {\\n        while (true) {\\n            synchronized (batchResponseLock) {\\n                try {\\n                    // Idle for no more than 1ms\\n                    batchResponseLock.wait(MAX_BATCH_RESPONSE_MILLS);\\n                } catch (InterruptedException e) {\\n                    LOGGER.error(\\"BatchResponseRunnable Interrupted error\\", e);\\n                }\\n            }\\n            isResponding = true;\\n            // Traverse `basketMap` for processing\\n            basketMap.forEach((channel, msgQueue) -> {\\n                if (msgQueue.isEmpty()) {\\n                    return;\\n                }\\n                // Group responses according to [serialization,compressor,rpcMessageId,headMap] dimensions.\\n                // Encapsulate queue messages into `BatchResultMessage` but not send all at once.\\n                // Send asynchronously per group based on [serialization,compressor,rpcMessageId,headMap].\\n                Map<ClientRequestRpcInfo, BatchResultMessage> batchResultMessageMap = new HashMap<>();\\n                while (!msgQueue.isEmpty()) {\\n                    QueueItem item = msgQueue.poll();\\n                    BatchResultMessage batchResultMessage = CollectionUtils.computeIfAbsent(batchResultMessageMap,\\n                            new ClientRequestRpcInfo(item.getRpcMessage()),\\n                            key -> new BatchResultMessage());\\n                    batchResultMessage.getResultMessages().add(item.getResultMessage());\\n                    batchResultMessage.getMsgIds().add(item.getMsgId());\\n                }\\n                batchResultMessageMap.forEach((clientRequestRpcInfo, batchResultMessage) ->\\n                        remotingServer.sendAsyncResponse(buildRpcMessage(clientRequestRpcInfo), channel, batchResultMessage));\\n            });\\n            isResponding = false;\\n        }\\n    }\\n}\\n```\\n\\nFinally, let\'s see how the Client side processes Server\'s batch response messages. According to the processor registered by the Client, the processor handling batch messages is `ClientOnResponseProcessor`, as follows:\\n\\n```java\\npublic void process(ChannelHandlerContext ctx, RpcMessage rpcMessage) throws Exception {\\n    // Process `MergeResultMessage`\\n    if (rpcMessage.getBody() instanceof MergeResultMessage) {\\n        MergeResultMessage results = (MergeResultMessage) rpcMessage.getBody();\\n        MergedWarpMessage mergeMessage = (MergedWarpMessage) mergeMsgMap.remove(rpcMessage.getId());\\n        for (int i = 0; i < mergeMessage.msgs.size(); i++) {\\n            int msgId = mergeMessage.msgIds.get(i);\\n            MessageFuture future = futures.remove(msgId);\\n            if (future == null) {\\n                LOGGER.error(\\"msg: {} is not found in futures, result message: {}\\", msgId, results.getMsgs()[i]);\\n            } else {\\n                future.setResultMessage(results.getMsgs()[i]);\\n            }\\n        }\\n    } else if (rpcMessage.getBody() instanceof BatchResultMessage) {\\n        // Process `BatchResultMessage`\\n        try {\\n            BatchResultMessage batchResultMessage = (BatchResultMessage) rpcMessage.getBody();\\n            for (int i = 0; i < batchResultMessage.getMsgIds().size(); i++) {\\n                int msgId = batchResultMessage.getMsgIds().get(i);\\n                MessageFuture future = futures.remove(msgId);\\n                if (future == null) {\\n                    LOGGER.error(\\"msg: {} is not found in futures, result message: {}\\", msgId, batchResultMessage.getResultMessages().get(i));\\n                } else {\\n                    future.setResultMessage(batchResultMessage.getResultMessages().get(i));\\n                }\\n            }\\n        } finally {\\n            // For compatibility with old versions, in batch sending of version 1.5.0,\\n            // batch messages will also be placed in the local cache of `mergeMsgMap`,\\n            // but version 1.5.0 no longer needs to obtain batch messages from `mergeMsgMap`.\\n            mergeMsgMap.clear();\\n        }\\n    } else {\\n        // Process non-batch sending messages\\n        MessageFuture messageFuture = futures.remove(rpcMessage.getId());\\n        if (messageFuture != null) {\\n            messageFuture.setResultMessage(rpcMessage.getBody());\\n        } else {\\n            if (rpcMessage.getBody() instanceof AbstractResultMessage) {\\n                if (transactionMessageHandler != null) {\\n                    transactionMessageHandler.onResponse((AbstractResultMessage) rpcMessage.getBody(), null);\\n                }\\n            }\\n        }\\n    }\\n}\\n```\\n\\nOf course, the logic here is quite simple: it involves putting the results into the corresponding `MessageFuture`, so the initially blocked thread that sent the request can obtain the result, thereby completing one cycle of batch sending and response handling.\\n\\nLet\'s do some extra thinking: Why does Seata have two methods for batch sending, and which is better?\\n\\nFor the `MergeResultMessage` approach, it must wait until all messages have been processed before sending them out, so its response speed is limited by the longest-processing message, even if other messages could be sent out much sooner.\\n\\nHowever, the `BatchResultMessage` approach differs in that it can achieve sending as soon as a message is processed, without waiting for other messages, thanks to parallel processing with `CompletableFuture`. This method definitely responds faster.\\n\\nThe latter approach was introduced in Seata version 1.5 onwards, which can be seen as a better way to handle batch sending.\\n\\nLastly, sharing an interaction flow diagram for global transaction commit requests by the author of the Seata RPC refactoring would be beneficial.\\n\\n![image-20241217222048505](/img/blog/seata-rpc.png)\\n\\n## How Seata Manages Channel\\n\\nThroughout the network communication process involving TC, TM, and RM, Channel is a critical communication component. To understand how Seata manages Channels, the easiest approach is to examine where the Server and Client obtain the Channel when sending messages.\\n\\nIn the `sendSyncRequest` method of the `AbstractNettyRemotingClient` class, we can see the following code:\\n\\n```java\\npublic Object sendSyncRequest(Object msg) throws TimeoutException {\\n    // ...\\n    // The Client acquires a Channel through NettyClientChannelManager\\n    Channel channel = clientChannelManager.acquireChannel(serverAddress);\\n    return super.sendSync(channel, rpcMessage, timeoutMillis);\\n}\\n```\\n\\nAnd in the `sendSyncRequest` method of the `AbstractNettyRemotingServer` class, we can see the following code:\\n\\n```java\\npublic Object sendSyncRequest(String resourceId, String clientId, Object msg, boolean tryOtherApp) throws TimeoutException {\\n    // The Server obtains a Channel through ChannelManager\\n    Channel channel = ChannelManager.getChannel(resourceId, clientId, tryOtherApp);\\n    if (channel == null) {\\n        throw new RuntimeException(\\"rm client is not connected. dbkey:\\" + resourceId + \\",clientId:\\" + clientId);\\n    }\\n    RpcMessage rpcMessage = buildRequestMessage(msg, ProtocolConstants.MSGTYPE_RESQUEST_SYNC);\\n    return super.sendSync(channel, rpcMessage, NettyServerConfig.getRpcRequestTimeout());\\n}\\n```\\n\\nTherefore, on the Client side, it mainly acquires Channels through `NettyClientChannelManager`, while the Server retrieves Channels from `ChannelManager` based on `resourceId` and `clientId`.\\n\\nSo, below we will primarily investigate these two classes along with some related logic.\\n\\n### Client Channel\\n\\nLet\'s first look at how Channels are managed on the Client side; the core class here is `NettyClientChannelManager`.\\n\\nFirst, let\'s take a simple look at the attributes of this class,\\n\\n```java\\n// serverAddress -> lock\\nprivate final ConcurrentMap<String, Object> channelLocks = new ConcurrentHashMap<>();\\n// serverAddress -> NettyPoolKey\\nprivate final ConcurrentMap<String, NettyPoolKey> poolKeyMap = new ConcurrentHashMap<>();\\n// serverAddress -> Channel\\nprivate final ConcurrentMap<String, Channel> channels = new ConcurrentHashMap<>();\\n// Object pool, NettyPoolKey -> Channel\\nprivate final GenericKeyedObjectPool<NettyPoolKey, Channel> nettyClientKeyPool;\\n// Functional interface, encapsulates the logic for obtaining a NettyPoolKey via serverAddress\\nprivate final Function<String, NettyPoolKey> poolKeyFunction;\\n```\\n\\n#### Core Classes of the Object Pool\\n\\nSeata uses `GenericKeyedObjectPool` as the object pool managing Channels.\\n\\n`GenericKeyedObjectPool` is an implementation from the Apache Commons Pool library, primarily used for managing a set of object pools, each distinguished by a unique Key. It can support pooling requirements for multiple types of objects.\\n\\nWhen using `GenericKeyedObjectPool`, it\'s typically necessary to configure a `KeyedPoolableObjectFactory`. This factory defines how to create, validate, activate, passivate, and destroy objects within the pool.\\n\\nWhen `GenericKeyedObjectPool` needs to create an object, it calls the `makeObject` method of the `KeyedPoolableObjectFactory` factory, and when it needs to destroy an object, it calls the `destroyObject` method to destroy it\u2026\u2026\\n\\n#### How to Pool Channel\\n\\nThe object being pooled is the Channel, and the corresponding Key is `NettyPoolKey`, as follows:\\n\\n```java\\npublic class NettyPoolKey {\\n\\n    private TransactionRole transactionRole;\\n    private String address;\\n    private AbstractMessage message;\\n\\n    // ...\\n}\\n```\\n\\nIn `NettyPoolKey`, three pieces of information are maintained: the transaction role (TM, RM, Server), the target TC Server address, and the RPC message sent by the Client when connecting to the Server.\\n\\nHow is this `NettyPoolKey` created? In Seata, the client actually has two roles, TM and RM, and the creation logic for each will be different. Therefore, Seata abstracts a method in `AbstractNettyRemotingClient` whose return value is a functional interface that encapsulates the logic for creating a `NettyPoolKey` based on `serverAddress`.\\n\\n```java\\n// org.apache.seata.core.rpc.netty.AbstractNettyRemotingClient#getPoolKeyFunction\\nprotected abstract Function<String, NettyPoolKey> getPoolKeyFunction();\\n```\\n\\nFor example, the implementation in TM is:\\n\\n```java\\nprotected Function<String, NettyPoolKey> getPoolKeyFunction() {\\n    return severAddress -> {\\n        RegisterTMRequest message = new RegisterTMRequest(applicationId, transactionServiceGroup, getExtraData());\\n        return new NettyPoolKey(NettyPoolKey.TransactionRole.TM_ROLE, severAddress, message);\\n    };\\n}\\n```\\n\\nAnd the implementation in RM is:\\n\\n```java\\nprotected Function<String, NettyPoolKey> getPoolKeyFunction() {\\n    return serverAddress -> {\\n        String resourceIds = getMergedResourceKeys();\\n        if (resourceIds != null && LOGGER.isInfoEnabled()) {\\n            LOGGER.info(\\"RM will register: {}\\", resourceIds);\\n        }\\n        RegisterRMRequest message = new RegisterRMRequest(applicationId, transactionServiceGroup);\\n        message.setResourceIds(resourceIds);\\n        return new NettyPoolKey(NettyPoolKey.TransactionRole.RM_ROLE, serverAddress, message);\\n    };\\n}\\n```\\n\\nFrom here, you can see that the message sent by TM after connecting to the Server is `RegisterTMRequest`, while for RM it is `RegisterRMRequest`.\\n\\nWhen is this functional interface called? We\'ll look at that later.\\n\\nWe also mentioned earlier that an object pool comes with a corresponding object creation factory `KeyedPoolableObjectFactory`. In Seata, `NettyPoolableFactory` extends `KeyedPoolableObjectFactory` to implement this.\\n\\n```java\\n/**\\n * Netty Channel creation factory, creates Channel through NettyPoolKey, methods in this class must be thread-safe\\n */\\npublic class NettyPoolableFactory implements KeyedPoolableObjectFactory<NettyPoolKey, Channel> {\\n\\n    // ...\\n    \\n    /**\\n     * This method is called when a new instance is needed\\n     */\\n    @Override\\n    public Channel makeObject(NettyPoolKey key) {\\n        InetSocketAddress address = NetUtil.toInetSocketAddress(key.getAddress());\\n        // Create Channel, essentially connect to Seata Server via bootstrap.connect and return Channel\\n        Channel tmpChannel = clientBootstrap.getNewChannel(address);\\n        long start = System.currentTimeMillis();\\n        Object response;\\n        Channel channelToServer = null;\\n        if (key.getMessage() == null) {\\n            throw new FrameworkException(\\"register msg is null, role:\\" + key.getTransactionRole().name());\\n        }\\n        try {\\n            // Send Message, for TM it\'s RegisterTMRequest, for RM it\'s RegisterRMRequest\\n            response = rpcRemotingClient.sendSyncRequest(tmpChannel, key.getMessage());\\n            // Determine if registration was successful based on response\\n            if (!isRegisterSuccess(response, key.getTransactionRole())) {\\n                rpcRemotingClient.onRegisterMsgFail(key.getAddress(), tmpChannel, response, key.getMessage());\\n            } else {\\n                // Registration successful\\n                channelToServer = tmpChannel;\\n                // Add serverAddress as key and Channel as value to NettyClientChannelManager.channels\\n                // If RM, possibly need to register resources with Server\\n                rpcRemotingClient.onRegisterMsgSuccess(key.getAddress(), tmpChannel, response, key.getMessage());\\n            }\\n        } catch (Exception exx) {\\n            if (tmpChannel != null) {\\n                tmpChannel.close();\\n            }\\n            throw new FrameworkException(\\"register \\" + key.getTransactionRole().name() + \\" error, errMsg:\\" + exx.getMessage());\\n        }\\n        return channelToServer;\\n    }\\n\\n    // ...\\n\\n    @Override\\n    public void destroyObject(NettyPoolKey key, Channel channel) throws Exception {\\n        if (channel != null) {\\n            channel.disconnect();\\n            channel.close();\\n        }\\n    }\\n\\n    /**\\n     * This method is called to validate object validity (optional) when borrowing an object\\n     */\\n    @Override\\n    public boolean validateObject(NettyPoolKey key, Channel obj) {\\n        if (obj != null && obj.isActive()) {\\n            return true;\\n        }\\n        return false;\\n    }\\n\\n    /**\\n     * This method is called to activate the object when borrowing an object\\n     */\\n    @Override\\n    public void activateObject(NettyPoolKey key, Channel obj) throws Exception {}\\n\\n    /**\\n     * This method is called to passivate the object when returning it\\n     */\\n    @Override\\n    public void passivateObject(NettyPoolKey key, Channel obj) throws Exception {}\\n}\\n```\\n\\n#### Acquiring Channel\\n\\nThroughout the Seata client, there are three ways to acquire a Channel: initialization, scheduled reconnection, and acquiring Channel when sending messages.\\n\\n```java\\n// Entry point one\\nprivate void initConnection() {\\n    boolean failFast =\\n            ConfigurationFactory.getInstance().getBoolean(ConfigurationKeys.ENABLE_TM_CLIENT_CHANNEL_CHECK_FAIL_FAST, DefaultValues.DEFAULT_CLIENT_CHANNEL_CHECK_FAIL_FAST);\\n    getClientChannelManager().initReconnect(transactionServiceGroup, failFast);\\n}\\n\\n// Entry point two\\npublic void init() {\\n    // Default delay 60s, periodic reconnect every 10s\\n    timerExecutor.scheduleAtFixedRate(() -> {\\n        try {\\n            clientChannelManager.reconnect(getTransactionServiceGroup());\\n        } catch (Exception ex) {\\n            LOGGER.warn(\\"reconnect server failed. {}\\", ex.getMessage());\\n        }\\n    }, SCHEDULE_DELAY_MILLS, SCHEDULE_INTERVAL_MILLS, TimeUnit.MILLISECONDS);\\n    // ...\\n}\\n\\n// Entry point three\\npublic Object sendSyncRequest(Object msg) throws TimeoutException {\\n    // ...\\n    // Client acquires Channel through NettyClientChannelManager\\n    Channel channel = clientChannelManager.acquireChannel(serverAddress);\\n    return super.sendSync(channel, rpcMessage, timeoutMillis);\\n}\\n```\\n\\nHowever, these three entry points will eventually call the `acquireChannel` method of `clientChannelManager` to obtain a Channel.\\n\\n```java\\n/**\\n * Get Channel based on serverAddress, if Channel does not exist or connection is dead then need to establish a new connection\\n */\\nChannel acquireChannel(String serverAddress) {\\n    // Get Channel from channels based on serverAddress\\n    Channel channelToServer = channels.get(serverAddress);\\n    if (channelToServer != null) {\\n        channelToServer = getExistAliveChannel(channelToServer, serverAddress);\\n        if (channelToServer != null) {\\n            return channelToServer;\\n        }\\n    }\\n    // If Channel does not exist in channels or this Channel is dead, then need to establish a connection for this address\\n    Object lockObj = CollectionUtils.computeIfAbsent(channelLocks, serverAddress, key -> new Object());\\n    synchronized (lockObj) {\\n        // Establish connection\\n        return doConnect(serverAddress);\\n    }\\n}\\n\\nprivate Channel doConnect(String serverAddress) {\\n    // Try to get once more\\n    Channel channelToServer = channels.get(serverAddress);\\n    if (channelToServer != null && channelToServer.isActive()) {\\n        return channelToServer;\\n    }\\n    Channel channelFromPool;\\n    try {\\n        // Call the functional interface here\\n        NettyPoolKey currentPoolKey = poolKeyFunction.apply(serverAddress);\\n        poolKeyMap.put(serverAddress, currentPoolKey);\\n        // Borrow object from the object pool, if object creation is needed, it will call the factory\'s makeObject method,\\n        // which internally connects to the Server and sends the message of currentPoolKey.message\\n        channelFromPool = nettyClientKeyPool.borrowObject(currentPoolKey);\\n        channels.put(serverAddress, channelFromPool);\\n    } catch (Exception exx) {\\n        LOGGER.error(\\"{} register RM failed.\\", FrameworkErrorCode.RegisterRM.getErrCode(), exx);\\n        throw new FrameworkException(\\"can not register RM,err:\\" + exx.getMessage());\\n    }\\n    return channelFromPool;\\n}\\n```\\n\\n### Server Channel\\n\\nOn the Server side, almost all core logic related to Channel management is within `ChannelManager`. So how does the Server get its Channels? Remember that on the Client side, after initiating a connection to the Server, it also sends a registration request for TM and RM.\\n\\nLet\'s first take a look at how the Server handles these `registerRequest`s.\\n\\n#### Handling Client Registration\\n\\nThe related handlers are `RegRmProcessor` and `RegTmProcessor`. In these two processors, the core logic involves calling the `ChannelManager`\'s `registerTMChannel` and `registerRMChannel` methods.\\n\\n```java\\npublic static void registerTMChannel(RegisterTMRequest request, Channel channel) throws IncompatibleVersionException {\\n    // Build RpcContext, which maintains the context of client connection information\\n    RpcContext rpcContext = buildChannelHolder(NettyPoolKey.TransactionRole.TM_ROLE, request.getVersion(),\\n            request.getApplicationId(),\\n            request.getTransactionServiceGroup(),\\n            null, channel);\\n    // Put Channel as key and rpcContext as value into IDENTIFIED_CHANNELS\\n    rpcContext.holdInIdentifiedChannels(IDENTIFIED_CHANNELS);\\n    // applicationId:clientIp\\n    String clientIdentified = rpcContext.getApplicationId() + Constants.CLIENT_ID_SPLIT_CHAR + ChannelUtil.getClientIpFromChannel(channel);\\n    // Store Channel information in TM_CHANNELS\\n    ConcurrentMap<Integer, RpcContext> clientIdentifiedMap = CollectionUtils.computeIfAbsent(TM_CHANNELS, clientIdentified, key -> new ConcurrentHashMap<>());\\n    rpcContext.holdInClientChannels(clientIdentifiedMap);\\n}\\n\\npublic static void registerRMChannel(RegisterRMRequest resourceManagerRequest, Channel channel) throws IncompatibleVersionException {\\n    Set<String> dbkeySet = dbKeytoSet(resourceManagerRequest.getResourceIds());\\n    RpcContext rpcContext;\\n    if (!IDENTIFIED_CHANNELS.containsKey(channel)) {\\n        // Build RpcContext and IDENTIFIED_CHANNELS\\n        rpcContext = buildChannelHolder(NettyPoolKey.TransactionRole.RM_ROLE, resourceManagerRequest.getVersion(),\\n                resourceManagerRequest.getApplicationId(), resourceManagerRequest.getTransactionServiceGroup(),\\n                resourceManagerRequest.getResourceIds(), channel);\\n        rpcContext.holdInIdentifiedChannels(IDENTIFIED_CHANNELS);\\n    } else {\\n        rpcContext = IDENTIFIED_CHANNELS.get(channel);\\n        rpcContext.addResources(dbkeySet);\\n    }\\n    if (dbkeySet == null || dbkeySet.isEmpty()) {\\n        return;\\n    }\\n    for (String resourceId : dbkeySet) {\\n        String clientIp;\\n        // Maintain RM_CHANNELS information\\n        ConcurrentMap<Integer, RpcContext> portMap = CollectionUtils.computeIfAbsent(RM_CHANNELS, resourceId, key -> new ConcurrentHashMap<>())\\n                .computeIfAbsent(resourceManagerRequest.getApplicationId(), key -> new ConcurrentHashMap<>())\\n                .computeIfAbsent(clientIp = ChannelUtil.getClientIpFromChannel(channel), key -> new ConcurrentHashMap<>());\\n        rpcContext.holdInResourceManagerChannels(resourceId, portMap);\\n        updateChannelsResource(resourceId, clientIp, resourceManagerRequest.getApplicationId());\\n    }\\n}\\n```\\n\\nThese two methods have relatively simple logic. They construct an `RpcContext` based on the registration request and Channel information, maintaining relevant Map collections within the Server such as `IDENTIFIED_CHANNELS`, `RM_CHANNELS`, and `TM_CHANNELS`.\\n\\nHowever, to be honest, these collections are nested quite deeply, and it is uncertain whether they can be optimized.\\n\\n```java\\n/**\\n * Channel -> RpcContext\\n */\\nprivate static final ConcurrentMap<Channel, RpcContext> IDENTIFIED_CHANNELS = new ConcurrentHashMap<>();\\n\\n/**\\n * resourceId -> applicationId -> ip -> port -> RpcContext\\n */\\n//                               resourceId          applicationId               ip\\nprivate static final ConcurrentMap<String, ConcurrentMap<String, ConcurrentMap<String,\\n        //             port    RpcContext\\n        ConcurrentMap<Integer, RpcContext>>>> RM_CHANNELS = new ConcurrentHashMap<>();\\n\\n/**\\n * applicationId:clientIp -> port -> RpcContext\\n */\\nprivate static final ConcurrentMap<String, ConcurrentMap<Integer, RpcContext>> TM_CHANNELS = new ConcurrentHashMap<>();\\n```\\n\\n#### Acquiring Channel\\n\\nOn the Server side, the logic for acquiring a Channel is really long; those interested can take a look by themselves. Essentially, it involves obtaining an effective Channel from the map.\\n\\n```java\\npublic static Channel getChannel(String resourceId, String clientId, boolean tryOtherApp) {\\n    Channel resultChannel = null;\\n    // Parse ClientId, composed of three parts: applicationId + clientIp + clientPort\\n    String[] clientIdInfo = parseClientId(clientId);\\n    if (clientIdInfo == null || clientIdInfo.length != 3) {\\n        throw new FrameworkException(\\"Invalid Client ID: \\" + clientId);\\n    }\\n    if (StringUtils.isBlank(resourceId)) {\\n        if (LOGGER.isInfoEnabled()) {\\n            LOGGER.info(\\"No channel is available, resourceId is null or empty\\");\\n        }\\n        return null;\\n    }\\n    // applicationId\\n    String targetApplicationId = clientIdInfo[0];\\n    // clientIp\\n    String targetIP = clientIdInfo[1];\\n    // clientPort\\n    int targetPort = Integer.parseInt(clientIdInfo[2]);\\n    // Below is continuously extracting the inner ConcurrentHashMaps\\n    ConcurrentMap<String, ConcurrentMap<String, ConcurrentMap<Integer, RpcContext>>> applicationIdMap = RM_CHANNELS.get(resourceId);\\n    if (targetApplicationId == null || applicationIdMap == null || applicationIdMap.isEmpty()) {\\n        if (LOGGER.isInfoEnabled()) {\\n            LOGGER.info(\\"No channel is available for resource[{}]\\", resourceId);\\n        }\\n        return null;\\n    }\\n    ConcurrentMap<String, ConcurrentMap<Integer, RpcContext>> ipMap = applicationIdMap.get(targetApplicationId);\\n    if (ipMap != null && !ipMap.isEmpty()) {\\n        // Firstly, try to find the original channel through which the branch was registered.\\n        // Port -> RpcContext\\n        ConcurrentMap<Integer, RpcContext> portMapOnTargetIP = ipMap.get(targetIP);\\n        /**\\n         * Get Channel on targetIp\\n         */\\n        if (portMapOnTargetIP != null && !portMapOnTargetIP.isEmpty()) {\\n            RpcContext exactRpcContext = portMapOnTargetIP.get(targetPort);\\n            if (exactRpcContext != null) {\\n                Channel channel = exactRpcContext.getChannel();\\n                if (channel.isActive()) {\\n                    // If Channel is valid, skip all following ifs and return this Channel\\n                    resultChannel = channel;\\n                    if (LOGGER.isDebugEnabled()) {\\n                        LOGGER.debug(\\"Just got exactly the one {} for {}\\", channel, clientId);\\n                    }\\n                } else {\\n                    if (portMapOnTargetIP.remove(targetPort, exactRpcContext)) {\\n                        if (LOGGER.isInfoEnabled()) {\\n                            LOGGER.info(\\"Removed inactive {}\\", channel);\\n                        }\\n                    }\\n                }\\n            }\\n            // The original channel was broken, try another one.\\n            if (resultChannel == null) {\\n                // Try other ports on the current node\\n                for (ConcurrentMap.Entry<Integer, RpcContext> portMapOnTargetIPEntry : portMapOnTargetIP.entrySet()) {\\n                    Channel channel = portMapOnTargetIPEntry.getValue().getChannel();\\n                    if (channel.isActive()) {\\n                        resultChannel = channel;\\n                        if (LOGGER.isInfoEnabled()) {\\n                            LOGGER.info(\\n                                    \\"Choose {} on the same IP[{}] as alternative of {}\\", channel, targetIP, clientId);\\n                        }\\n                        break;\\n                    } else {\\n                        if (portMapOnTargetIP.remove(portMapOnTargetIPEntry.getKey(),\\n                                portMapOnTargetIPEntry.getValue())) {\\n                            if (LOGGER.isInfoEnabled()) {\\n                                LOGGER.info(\\"Removed inactive {}\\", channel);\\n                            }\\n                        }\\n                    }\\n                }\\n            }\\n        }\\n        /**\\n         * Get Channel on targetApplicationId\\n         */\\n        // No channel on the app node, try another one.\\n        if (resultChannel == null) {\\n            for (ConcurrentMap.Entry<String, ConcurrentMap<Integer, RpcContext>> ipMapEntry : ipMap.entrySet()) {\\n                if (ipMapEntry.getKey().equals(targetIP)) {\\n                    continue;\\n                }\\n                ConcurrentMap<Integer, RpcContext> portMapOnOtherIP = ipMapEntry.getValue();\\n                if (portMapOnOtherIP == null || portMapOnOtherIP.isEmpty()) {\\n                    continue;\\n                }\\n                for (ConcurrentMap.Entry<Integer, RpcContext> portMapOnOtherIPEntry : portMapOnOtherIP.entrySet()) {\\n                    Channel channel = portMapOnOtherIPEntry.getValue().getChannel();\\n                    if (channel.isActive()) {\\n                        resultChannel = channel;\\n                        if (LOGGER.isInfoEnabled()) {\\n                            LOGGER.info(\\"Choose {} on the same application[{}] as alternative of {}\\", channel, targetApplicationId, clientId);\\n                        }\\n                        break;\\n                    } else {\\n                        if (portMapOnOtherIP.remove(portMapOnOtherIPEntry.getKey(), portMapOnOtherIPEntry.getValue())) {\\n                            if (LOGGER.isInfoEnabled()) {\\n                                LOGGER.info(\\"Removed inactive {}\\", channel);\\n                            }\\n                        }\\n                    }\\n                }\\n                if (resultChannel != null) {\\n                    break;\\n                }\\n            }\\n        }\\n    }\\n    if (resultChannel == null && tryOtherApp) {\\n        // Try other applicationId\\n        resultChannel = tryOtherApp(applicationIdMap, targetApplicationId);\\n        if (resultChannel == null) {\\n            if (LOGGER.isInfoEnabled()) {\\n                LOGGER.info(\\"No channel is available for resource[{}] as alternative of {}\\", resourceId, clientId);\\n            }\\n        } else {\\n            if (LOGGER.isInfoEnabled()) {\\n                LOGGER.info(\\"Choose {} on the same resource[{}] as alternative of {}\\", resultChannel, resourceId, clientId);\\n            }\\n        }\\n    }\\n    return resultChannel;\\n}\\n\\nprivate static Channel tryOtherApp(ConcurrentMap<String, ConcurrentMap<String, ConcurrentMap<Integer, RpcContext>>> applicationIdMap, String myApplicationId) {\\n    Channel chosenChannel = null;\\n    for (ConcurrentMap.Entry<String, ConcurrentMap<String, ConcurrentMap<Integer, RpcContext>>> applicationIdMapEntry : applicationIdMap.entrySet()) {\\n        if (!StringUtils.isNullOrEmpty(myApplicationId) && applicationIdMapEntry.getKey().equals(myApplicationId)) {\\n            continue;\\n        }\\n        ConcurrentMap<String, ConcurrentMap<Integer, RpcContext>> targetIPMap = applicationIdMapEntry.getValue();\\n        if (targetIPMap == null || targetIPMap.isEmpty()) {\\n            continue;\\n        }\\n        for (ConcurrentMap.Entry<String, ConcurrentMap<Integer, RpcContext>> targetIPMapEntry : targetIPMap.entrySet()) {\\n            ConcurrentMap<Integer, RpcContext> portMap = targetIPMapEntry.getValue();\\n            if (portMap == null || portMap.isEmpty()) {\\n                continue;\\n            }\\n            for (ConcurrentMap.Entry<Integer, RpcContext> portMapEntry : portMap.entrySet()) {\\n                Channel channel = portMapEntry.getValue().getChannel();\\n                if (channel.isActive()) {\\n                    chosenChannel = channel;\\n                    break;\\n                } else {\\n                    if (portMap.remove(portMapEntry.getKey(), portMapEntry.getValue())) {\\n                        if (LOGGER.isInfoEnabled()) {\\n                            LOGGER.info(\\"Removed inactive {}\\", channel);\\n                        }\\n                    }\\n                }\\n            }\\n            if (chosenChannel != null) {\\n                break;\\n            }\\n        }\\n        if (chosenChannel != null) {\\n            break;\\n        }\\n    }\\n    return chosenChannel;\\n}\\n```\\n\\n### Summary in a Sequence Diagram\\n\\nFinally, let\'s summarize the Channel management process with a sequence diagram.\\n\\n![image-20241217222155609](/img/blog/seata-channel.png)\\n\\n## How Seata Designs Its Protocol\\n\\nFor any network program, communication protocols are indispensable, and Seata is no exception. Here we will look at how the V1 version of the Seata protocol is implemented.\\n\\nThe main related classes are `ProtocolEncoderV1` and `ProtocolDecoderV1`.\\n\\nOf course, as we know from before, the processor added when the Seata Server starts is actually `MultiProtocolDecoder`. In this class\'s decode method, it works as follows:\\n\\n```java\\nprotected Object decode(ChannelHandlerContext ctx, ByteBuf in) throws Exception {\\n    ByteBuf frame;\\n    Object decoded;\\n    byte version;\\n    try {\\n        if (isV0(in)) {\\n            decoded = in;\\n            version = ProtocolConstants.VERSION_0;\\n        } else {\\n            decoded = super.decode(ctx, in);\\n            version = decideVersion(decoded);\\n        }\\n        if (decoded instanceof ByteBuf) {\\n            frame = (ByteBuf) decoded;\\n            // Identify multi-version protocols through MultiProtocolDecoder\\n            // Select the corresponding codec based on version\\n            ProtocolDecoder decoder = protocolDecoderMap.get(version);\\n            ProtocolEncoder encoder = protocolEncoderMap.get(version);\\n            try {\\n                if (decoder == null || encoder == null) {\\n                    throw new UnsupportedOperationException(\\"Unsupported version: \\" + version);\\n                }\\n                return decoder.decodeFrame(frame);\\n            } finally {\\n                if (version != ProtocolConstants.VERSION_0) {\\n                    frame.release();\\n                }\\n                // Add the selected codec to the pipeline and remove MultiProtocolDecoder\\n                ctx.pipeline().addLast((ChannelHandler) decoder);\\n                ctx.pipeline().addLast((ChannelHandler) encoder);\\n                if (channelHandlers != null) {\\n                    ctx.pipeline().addLast(channelHandlers);\\n                }\\n                ctx.pipeline().remove(this);\\n            }\\n        }\\n    } catch (Exception exx) {\\n        LOGGER.error(\\"Decode frame error, cause: {}\\", exx.getMessage());\\n        throw new DecodeException(exx);\\n    }\\n    return decoded;\\n}\\n```\\n\\nTherefore, here the corresponding codec for the version is chosen, then added to the pipeline, which will remove the `MultiProtocolDecoder`.\\n\\n### V1 Version Protocol\\n\\nSeata\'s protocol design is quite comprehensive and general, also being a mainstream solution to address issues like message fragmentation and partial messages, namely message length + message content.\\n\\nThe format of the protocol is as follows:\\n\\n![image-20241217222155609](/img/blog/seata-protocol.png)\\n\\nAs can be seen, it includes magic numbers, protocol version numbers, length fields, header lengths, message types, serialization algorithms, compression algorithms, request IDs, optional map extensions, and the message body.\\n\\n### How Encoding and Decoding Are Performed\\n\\nSeata decoders use Netty\'s built-in `LengthFieldBasedFrameDecoder`; those unfamiliar with it can take a look.\\n\\nHowever, encoding and decoding are not difficult, so I\'ll simply provide the code without much explanation.\\n\\n```java\\npackage org.apache.seata.core.rpc.netty.v1;\\n\\nimport io.netty.buffer.ByteBuf;\\nimport io.netty.channel.ChannelHandlerContext;\\nimport io.netty.handler.codec.MessageToByteEncoder;\\nimport org.apache.seata.core.rpc.netty.ProtocolEncoder;\\nimport org.apache.seata.core.serializer.Serializer;\\nimport org.apache.seata.core.compressor.Compressor;\\nimport org.apache.seata.core.compressor.CompressorFactory;\\nimport org.apache.seata.core.protocol.ProtocolConstants;\\nimport org.apache.seata.core.protocol.RpcMessage;\\nimport org.apache.seata.core.serializer.SerializerServiceLoader;\\nimport org.apache.seata.core.serializer.SerializerType;\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\n\\nimport java.util.Map;\\n\\n/**\\n * <pre>\\n * 0     1     2     3     4     5     6     7     8     9    10     11    12    13    14    15    16\\n * +-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\\n * |   magic   |proto|     full length       |    head   | Msg |Seria|Compr|      RequestId        |\\n * |   code    |versi|     (head+body)       |   length  |Type |lizer|ess  |                       |\\n * +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\\n * |                                   Head Map [Optional]                                         |\\n * +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\\n * |                                         body                                                  |\\n * +-----------------------------------------------------------------------------------------------+\\n * </pre>\\n * <p>\\n * <li>Full Length: include all data </li>\\n * <li>Head Length: include head data from magic code to head map. </li>\\n * <li>Body Length: Full Length - Head Length</li>\\n * </p>\\n */\\npublic class ProtocolEncoderV1 extends MessageToByteEncoder implements ProtocolEncoder {\\n\\n    private static final Logger LOGGER = LoggerFactory.getLogger(ProtocolEncoderV1.class);\\n\\n    public void encode(RpcMessage message, ByteBuf out) {\\n        try {\\n            ProtocolRpcMessageV1 rpcMessage = new ProtocolRpcMessageV1();\\n            rpcMessage.rpcMsgToProtocolMsg(message);\\n            int fullLength = ProtocolConstants.V1_HEAD_LENGTH;\\n            int headLength = ProtocolConstants.V1_HEAD_LENGTH;\\n            byte messageType = rpcMessage.getMessageType();\\n            out.writeBytes(ProtocolConstants.MAGIC_CODE_BYTES);\\n            out.writeByte(ProtocolConstants.VERSION_1);\\n            // full Length(4B) and head length(2B) will fix in the end.\\n            out.writerIndex(out.writerIndex() + 6); // Here we skip the full length and head length positions and fill in the last\\n            out.writeByte(messageType);\\n            out.writeByte(rpcMessage.getCodec());\\n            out.writeByte(rpcMessage.getCompressor());\\n            out.writeInt(rpcMessage.getId());\\n            // direct write head with zero-copy\\n            Map<String, String> headMap = rpcMessage.getHeadMap();\\n            if (headMap != null && !headMap.isEmpty()) {\\n                int headMapBytesLength = HeadMapSerializer.getInstance().encode(headMap, out);\\n                headLength += headMapBytesLength;\\n                fullLength += headMapBytesLength;\\n            }\\n            byte[] bodyBytes = null;\\n            // heartbeat don\'t have body\\n            if (messageType != ProtocolConstants.MSGTYPE_HEARTBEAT_REQUEST && messageType != ProtocolConstants.MSGTYPE_HEARTBEAT_RESPONSE) {\\n                Serializer serializer = SerializerServiceLoader.load(SerializerType.getByCode(rpcMessage.getCodec()), ProtocolConstants.VERSION_1);\\n                bodyBytes = serializer.serialize(rpcMessage.getBody());\\n                Compressor compressor = CompressorFactory.getCompressor(rpcMessage.getCompressor());\\n                bodyBytes = compressor.compress(bodyBytes);\\n                fullLength += bodyBytes.length;\\n            }\\n            if (bodyBytes != null) {\\n                out.writeBytes(bodyBytes);\\n            }\\n            // fix fullLength and headLength\\n            int writeIndex = out.writerIndex();\\n            // skip magic code(2B) + version(1B)\\n            out.writerIndex(writeIndex - fullLength + 3);\\n            out.writeInt(fullLength);\\n            out.writeShort(headLength);\\n            out.writerIndex(writeIndex);\\n        } catch (Throwable e) {\\n            LOGGER.error(\\"Encode request error!\\", e);\\n            throw e;\\n        }\\n    }\\n\\n    @Override\\n    protected void encode(ChannelHandlerContext ctx, Object msg, ByteBuf out) throws Exception {\\n        try {\\n            if (msg instanceof RpcMessage) {\\n                this.encode((RpcMessage) msg, out);\\n            } else {\\n                throw new UnsupportedOperationException(\\"Not support this class:\\" + msg.getClass());\\n            }\\n        } catch (Throwable e) {\\n            LOGGER.error(\\"Encode request error!\\", e);\\n        }\\n    }\\n}\\n```\\n\\n```java\\npackage org.apache.seata.core.rpc.netty.v1;\\n\\nimport java.util.List;\\nimport java.util.Map;\\n\\nimport io.netty.buffer.ByteBuf;\\nimport io.netty.channel.ChannelHandlerContext;\\nimport io.netty.handler.codec.LengthFieldBasedFrameDecoder;\\nimport org.apache.seata.core.compressor.Compressor;\\nimport org.apache.seata.core.compressor.CompressorFactory;\\nimport org.apache.seata.core.exception.DecodeException;\\nimport org.apache.seata.core.protocol.HeartbeatMessage;\\nimport org.apache.seata.core.protocol.ProtocolConstants;\\nimport org.apache.seata.core.protocol.RpcMessage;\\nimport org.apache.seata.core.rpc.netty.ProtocolDecoder;\\nimport org.apache.seata.core.serializer.Serializer;\\nimport org.apache.seata.core.serializer.SerializerServiceLoader;\\nimport org.apache.seata.core.serializer.SerializerType;\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\n\\n/**\\n * <pre>\\n * 0     1     2     3     4     5     6     7     8     9    10     11    12    13    14    15    16\\n * +-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\\n * |   magic   |proto|     full length       |    head   | Msg |Seria|Compr|      RequestId        |\\n * |   code    |versi|     (head+body)       |   length  |Type |lizer|ess  |                       |\\n * +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\\n * |                                   Head Map [Optional]                                         |\\n * +-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\\n * |                                         body                                                  |\\n * +-----------------------------------------------------------------------------------------------+\\n * </pre>\\n * <p>\\n * <li>Full Length: include all data </li>\\n * <li>Head Length: include head data from magic code to head map. </li>\\n * <li>Body Length: Full Length - Head Length</li>\\n * </p>\\n */\\npublic class ProtocolDecoderV1 extends LengthFieldBasedFrameDecoder implements ProtocolDecoder {\\n\\n    private static final Logger LOGGER = LoggerFactory.getLogger(ProtocolDecoderV1.class);\\n\\n    private final List<SerializerType> supportDeSerializerTypes;\\n\\n    public ProtocolDecoderV1() {\\n        /**\\n         *  int maxFrameLength,\\n         *  int lengthFieldOffset,  Magic 2B, version 1B so the length is offset by 3B\\n         *  int lengthFieldLength,  FullLength is int(4B). so values is 4\\n         *  int lengthAdjustment,   FullLength include all data and read 7 bytes before, so the left length is (FullLength-7). so values is -7\\n         *  int initialBytesToStrip we will check magic code and version self, so do not strip any bytes. so values is 0\\n         */\\n        super(ProtocolConstants.MAX_FRAME_LENGTH, 3, 4, -7, 0);\\n        supportDeSerializerTypes = SerializerServiceLoader.getSupportedSerializers();\\n        if (supportDeSerializerTypes.isEmpty()) {\\n            throw new IllegalArgumentException(\\"No serializer found\\");\\n        }\\n    }\\n\\n    @Override\\n    public RpcMessage decodeFrame(ByteBuf frame) {\\n        byte b0 = frame.readByte();\\n        byte b1 = frame.readByte();\\n        if (ProtocolConstants.MAGIC_CODE_BYTES[0] != b0 || ProtocolConstants.MAGIC_CODE_BYTES[1] != b1) {\\n            throw new IllegalArgumentException(\\"Unknown magic code: \\" + b0 + \\", \\" + b1);\\n        }\\n        byte version = frame.readByte();\\n        int fullLength = frame.readInt();\\n        short headLength = frame.readShort();\\n        byte messageType = frame.readByte();\\n        byte codecType = frame.readByte();\\n        byte compressorType = frame.readByte();\\n        int requestId = frame.readInt();\\n        ProtocolRpcMessageV1 rpcMessage = new ProtocolRpcMessageV1();\\n        rpcMessage.setCodec(codecType);\\n        rpcMessage.setId(requestId);\\n        rpcMessage.setCompressor(compressorType);\\n        rpcMessage.setMessageType(messageType);\\n        // direct read head with zero-copy\\n        int headMapLength = headLength - ProtocolConstants.V1_HEAD_LENGTH;\\n        if (headMapLength > 0) {\\n            Map<String, String> map = HeadMapSerializer.getInstance().decode(frame, headMapLength);\\n            rpcMessage.getHeadMap().putAll(map);\\n        }\\n        // read body\\n        if (messageType == ProtocolConstants.MSGTYPE_HEARTBEAT_REQUEST) {\\n            rpcMessage.setBody(HeartbeatMessage.PING);\\n        } else if (messageType == ProtocolConstants.MSGTYPE_HEARTBEAT_RESPONSE) {\\n            rpcMessage.setBody(HeartbeatMessage.PONG);\\n        } else {\\n            int bodyLength = fullLength - headLength;\\n            if (bodyLength > 0) {\\n                byte[] bs = new byte[bodyLength];\\n                frame.readBytes(bs);\\n                Compressor compressor = CompressorFactory.getCompressor(compressorType);\\n                bs = compressor.decompress(bs);\\n                SerializerType protocolType = SerializerType.getByCode(rpcMessage.getCodec());\\n                if (this.supportDeSerializerTypes.contains(protocolType)) {\\n                    Serializer serializer = SerializerServiceLoader.load(protocolType, ProtocolConstants.VERSION_1);\\n                    rpcMessage.setBody(serializer.deserialize(bs));\\n                } else {\\n                    throw new IllegalArgumentException(\\"SerializerType not match\\");\\n                }\\n            }\\n        }\\n        return rpcMessage.protocolMsgToRpcMsg();\\n    }\\n\\n    @Override\\n    protected Object decode(ChannelHandlerContext ctx, ByteBuf in) throws Exception {\\n        Object decoded;\\n        try {\\n            decoded = super.decode(ctx, in);\\n            if (decoded instanceof ByteBuf) {\\n                ByteBuf frame = (ByteBuf) decoded;\\n                try {\\n                    return decodeFrame(frame);\\n                } finally {\\n                    frame.release();\\n                }\\n            }\\n        } catch (Exception exx) {\\n            LOGGER.error(\\"Decode frame error, cause: {}\\", exx.getMessage());\\n            throw new DecodeException(exx);\\n        }\\n        return decoded;\\n    }\\n}\\n```\\n\\n## Summary\\n\\nFrom the current perspective, the implementation of network communication in Seata is relatively easy to understand. However, this article\'s analysis is only superficial and does not delve into deeper, more critical aspects such as code robustness, exception handling, graceful shutdown, etc. Further analysis will be provided if there are new insights in the future.\\n\\n[Original Article Link](https://blog.hein-hp.click/article/5p94ivva/)"},{"id":"/seata-grpc-client","metadata":{"permalink":"/blog/seata-grpc-client","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-grpc-client.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-grpc-client.md","title":"Go Language Client Communication with Seata Server","description":"This article takes Go language as an example to demonstrate Seata\'s multi-language client communication capabilities.","date":"2024-11-30T00:00:00.000Z","formattedDate":"November 30, 2024","tags":[],"readingTime":3.775,"hasTruncateMarker":false,"authors":[{"name":"Wang Mingjun, Seata Open Source Summer Student Participant"}],"frontMatter":{"title":"Go Language Client Communication with Seata Server","author":"Wang Mingjun, Seata Open Source Summer Student Participant","description":"This article takes Go language as an example to demonstrate Seata\'s multi-language client communication capabilities.","date":"2024/11/30","keywords":["seata","distributed transaction","cloud-native","grpc","multi-language communication"]},"unlisted":false,"prevItem":{"title":"Getting Started with Seata Network Communication Source Code","permalink":"/blog/seata-rpc-analysis"},"nextItem":{"title":"How to Send Messages with RocketMQ in Seata","permalink":"/blog/how-to-send-message-with-rocketmq-in-seata"}},"content":"With the merge of PR [https://github.com/apache/incubator-seata/pull/6754](https://github.com/apache/incubator-seata/pull/6754), Seata Server is now capable of recognizing and processing Grpc requests. This means that any language client, by simply including the proto files, can communicate with the Seata Server deployed on the JVM, thereby achieving the full process of distributed transactions.\\n\\nBelow is a demonstration of this process using Go language as an example.\\n\\n# Environment Preparation\\nGoland 2024.2\\n\\nIdea 2024.3\\n\\nJDK 1.8\\n\\nGo 1.23.3\\n\\nSeata 2.3.0-SNAPSHOT\\n\\nlibprotoc 3.21.0\\n\\n# Operation Process\\n## Deploy and Start Seata Server\\nRun org.apache.seata.server.ServerApplication#main as shown below:\\n\\n![2024121301.png](../../../static/img/blog/2024121301.png)\\n\\n## Proto File Import\\nImport the necessary proto files for the transaction process in the Go project, including various transaction request and response proto files and the proto files for initiating RPC. As shown below:\\n\\n![2024121302.png](../../../static/img/blog/2024121302.png)\\n\\n## Grpc File Generation\\nIn the directory where the proto files were imported in the previous step, execute the command:\\n\\n```shell\\nprotoc --go_out=. --go-grpc_out=. .\\\\*.proto\\n```\\n\\nAfter execution, the grpc code will be generated as shown below:\\n\\n![2024121303.png](../../../static/img/blog/2024121303.png)\\n\\n## Grpc Invocation\\nComplete a distributed transaction process in main.go and print the response from Seata Server. The code is as follows:\\n\\n```go\\nfunc main() {\\n    conn, err := grpc.Dial(\\":8091\\", grpc.WithInsecure())\\n    if err != nil {\\n        log.Fatalf(\\"did not connect: %v\\", err)\\n    }\\n    defer conn.Close()\\n    client := pb.NewSeataServiceClient(conn)\\n    stream, err := client.SendRequest(context.Background())\\n    if err != nil {\\n        log.Fatalf(\\"could not sendRequest: %v\\", err)\\n    }\\n    defer stream.CloseSend()\\n\\n    sendRegisterTm(stream)\\n    xid := sendGlobalBegin(stream)\\n    sendBranchRegister(stream, xid)\\n    sendGlobalCommit(stream, xid)\\n}\\n\\n// ... Other functions ...\\n\\n```\\n\\nAfter running, the Seata Server console prints as follows:\\n\\n![2024121304.png](../../../static/img/blog/2024121304.png)\\n\\nThe Go client console prints as follows:\\n\\n![2024121305.png](../../../static/img/blog/2024121305.png)\\n\\n# Implementation Principle\\n## Proto Design\\nTo achieve communication with multi-language grpc clients, Seata Server defines grpcMessage.proto, which defines the GrpcMessageProto that assembles various Seata Message objects and the bidirectional stream interface sendRequest for assembling Seata communication requests. Seata Server uses grpcMessage.proto as a medium to achieve communication with multi-language clients.\\n\\n```proto\\nsyntax = \\"proto3\\";\\npackage org.apache.seata.protocol.protobuf;\\nimport \\"google/protobuf/any.proto\\";\\noption java_multiple_files = true;\\noption java_outer_classname = \\"GrpcMessage\\";\\noption java_package = \\"org.apache.seata.core.protocol.generated\\";\\n\\nmessage GrpcMessageProto {\\n    int32 id = 1;\\n    int32 messageType = 2;\\n    map<string, string> headMap = 3;\\n    google.protobuf.Any body = 4;\\n}\\n\\nservice SeataService {\\n    rpc sendRequest (stream GrpcMessageProto) returns (stream GrpcMessageProto);\\n}\\n```\\n\\nIn addition, GrpcSerializer is defined, adapting to Seata\'s serialization SPI system, which is used to achieve the mutual conversion of protobuf byte streams and Seata message objects.\\n\\n## Grpc Protocol Recognition\\nSeata Server implements ProtocolDetectHandler and ProtocolDetector. ProtocolDetectHandler, as a ByteToMessageDecoder, will traverse the ProtocolDetector list when receiving a message to find a ProtocolDetector that can recognize the current message. ProtocolDetector distinguishes Seata protocols, Http1.1 protocols, and Http2 protocols through recognizing magic numbers. Once recognized, the ChannelHandler capable of handling the protocol is added to the current Channel\'s Pipeline.\\n\\n![2024121306.jpeg](../../../static/img/blog/2024121306.jpeg)\\n\\n## Grpc Request Sending and Processing\\nSeata Server implements GrpcEncoder and GrpcDecoder. GrpcEncoder is responsible for converting Seata\'s RpcMessage into GrpcMessageProto recognizable by grpc native clients, filling the header with status, contentType, and other protocol headers for communication with grpc native clients. GrpcEncoder also adapts to grpc protocol specifications, writing the compression bit, length, and message body in the order specified by the grpc protocol into the channel.\\n\\nGrpcDecoder is responsible for processing requests from grpc native clients. Since grpc clients implement request batching in the underlying transmission through a queue flush, GrpcDecoder is also responsible for unpacking a batch of requests. Finally, GrpcDecoder converts the protobuf byte stream into one or more RpcMessages and passes them to the Seata request processor.\\n\\n## Grpc Connection Establishment and Management\\nOn the server side, simply configure a ProtocolDetectHandler to complete the recognition and establishment of various types of connections.\\n\\n```java\\n@Override\\npublic void initChannel(SocketChannel ch) {\\n    ProtocolDetector[] defaultProtocolDetectors = {\\n            new Http2Detector(getChannelHandlers()),\\n            new SeataDetector(getChannelHandlers()),\\n            new HttpDetector()\\n    };\\n    ch.pipeline().addLast(new IdleStateHandler(nettyServerConfig.getChannelMaxReadIdleSeconds(), 0, 0))\\n            .addLast(new ProtocolDetectHandler(defaultProtocolDetectors));\\n}\\n```\\n\\nOn the client side, when obtaining a Channel, if the current communication method is Grpc, an Http2MultiStreamChannel is obtained as the parent Channel, and grpc-related handlers are added to this Channel.\\n\\n```java\\nif (nettyClientConfig.getProtocol().equals(Protocol.GPRC.value)) {\\n    Http2StreamChannelBootstrap bootstrap = new Http2StreamChannelBootstrap(channel);\\n    bootstrap.handler(new ChannelInboundHandlerAdapter() {\\n        @Override\\n        public void handlerAdded(ChannelHandlerContext ctx) throws Exception {\\n            Channel channel = ctx.channel();\\n            channel.pipeline().addLast(new GrpcDecoder());\\n            channel.pipeline().addLast(new GrpcEncoder());\\n            if (channelHandlers != null) {\\n                addChannelPipelineLast(channel, channelHandlers);\\n            }\\n        }\\n    });\\n    channel = bootstrap.open().get();\\n}\\n```\\n\\nPlease note that due to network issues, the parsing of the above links was unsuccessful. If you need the content of the parsed web pages, please check the legality of the web page links and try again. If you do not need the parsing of these links, the question can be answered normally."},{"id":"/how-to-send-message-with-rocketmq-in-seata","metadata":{"permalink":"/blog/how-to-send-message-with-rocketmq-in-seata","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/how-to-send-message-with-rocketmq-in-seata.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/how-to-send-message-with-rocketmq-in-seata.md","title":"How to Send Messages with RocketMQ in Seata","description":"This article mainly introduces how Seata integrates with RocketMQ to send messages","date":"2024-10-15T00:00:00.000Z","formattedDate":"October 15, 2024","tags":[],"readingTime":4.42,"hasTruncateMarker":false,"authors":[{"name":"Zhang Jiawei - Seata PPMC"}],"frontMatter":{"title":"How to Send Messages with RocketMQ in Seata","keywords":["Seata","RocketMQ"],"description":"This article mainly introduces how Seata integrates with RocketMQ to send messages","author":"Zhang Jiawei - Seata PPMC","date":"2024-10-15T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Go Language Client Communication with Seata Server","permalink":"/blog/seata-grpc-client"},"nextItem":{"title":"Seata Raft Configuration Center","permalink":"/blog/seata-raft-config-center"}},"content":"## Background\\n\\nIn distributed transactions, we often encounter scenarios where messages need to be sent, such as notifying the inventory service to reduce inventory after an order is successfully paid.\\nBut how to ensure the consistency between local transactions and message sending? This requires using a distributed transaction solution to solve this problem.\\nSeata, as an open-source distributed transaction solution, provides support for RocketMQ, making it easy to send messages in distributed transactions.\\n\\n## Solution Design\\n![img.png](/img/blog/how-to-send-message-with-rocketmq-in-seata/tcc-mode.png)\\n\\nLet\'s first review the overall process of TCC through the above diagram:\\n\\n1. The Transaction Manager (TM) initiates a global transaction.\\n2. The Resource Manager (RM) tries to execute the prepare method to reserve resources and registers the branch transaction with the Transaction Coordinator (TC).\\n3. If the resource reservation is successful, the Transaction Manager (TM) calls commit to commit the global transaction, and the Transaction Coordinator (TC) notifies the Resource Manager (RM) to commit the branch transaction.\\n4. If the resource reservation fails, the Transaction Manager (TM) calls rollback to roll back the global transaction, and the Transaction Coordinator (TC) notifies the Resource Manager (RM) to roll back the branch transaction.\\n\\n![img.png](/img/blog/how-to-send-message-with-rocketmq-in-seata/seata-rocketmq.png)\\n\\nAfter understanding the overall process of TCC, the above diagram is not difficult to understand. Seata proxies the RocketMQ Producer, automatically converting ordinary messages into RocketMQ transaction messages when the business code needs to send messages, thereby ensuring the consistency of message sending and distributed transactions.\\n\\n## Implementation Principle\\n\\n```java\\npublic class SeataMQProducerFactory {\\n\\n  public static SeataMQProducer createSingle(String nameServer, String producerGroup) throws MQClientException {\\n    return createSingle(nameServer, null, producerGroup, null);\\n  }\\n\\n  public static SeataMQProducer createSingle(String nameServer, String namespace,\\n                                             String groupName, RPCHook rpcHook) throws MQClientException {\\n      defaultProducer = new SeataMQProducer(namespace, groupName, rpcHook);\\n      defaultProducer.start();\\n      return defaultProducer;\\n  }\\n}\\n```\\nFrom the above code, we can see that SeataMQProducerFactory provides methods to create SeataMQProducer. By calling the createSingle method, we can create a SeataMQProducer instance.\\n\\n```java\\n@Override\\npublic SendResult send(Message msg) throws MQClientException, MQBrokerException, RemotingException, InterruptedException {\\n  return send(msg, this.getSendMsgTimeout());\\n}\\n\\n@Override\\npublic SendResult send(Message msg, long timeout) throws MQClientException, MQBrokerException, RemotingException, InterruptedException {\\n  if (RootContext.inGlobalTransaction()) {\\n    if (tccRocketMQ == null) {\\n      throw new RuntimeException(\\"TCCRocketMQ is not initialized\\");\\n    }\\n    return tccRocketMQ.prepare(msg, timeout);\\n  } else {\\n    return super.send(msg, timeout);\\n  }\\n}\\n```\\nFrom the above code, we can see that SeataMQProducer overrides the send method of RocketMQ. By determining whether it is in a global transaction, it decides whether to call the send method of RocketMQ or the prepare method of TccRocketMQ. If the message is sent without participating in a global transaction, it degrades to calling the send method of RocketMQ to send the message.\\n\\n```java\\n@LocalTCC\\npublic class TCCRocketMQImpl implements TCCRocketMQ {\\n\\n  @Override\\n  @TwoPhaseBusinessAction(name = SeataMQProducerFactory.ROCKET_TCC_NAME)\\n  public SendResult prepare(Message message, long timeout) throws MQClientException {\\n    BusinessActionContext context = BusinessActionContextUtil.getContext();\\n    LOGGER.info(\\"RocketMQ message send prepare, xid = {}\\", context.getXid());\\n    Map<String, Object> params = new HashMap<>(8);\\n    SendResult sendResult = producer.doSendMessageInTransaction(message, timeout, context.getXid(), context.getBranchId());\\n    message.setDeliverTimeMs(0);\\n    params.put(ROCKET_MSG_KEY, message);\\n    params.put(ROCKET_SEND_RESULT_KEY, sendResult);\\n    BusinessActionContextUtil.addContext(params);\\n    return sendResult;\\n  }\\n\\n  @Override\\n  public boolean commit(BusinessActionContext context)\\n    throws UnknownHostException, MQBrokerException, RemotingException, InterruptedException, TimeoutException, TransactionException {\\n    Message message = context.getActionContext(ROCKET_MSG_KEY, Message.class);\\n    SendResult sendResult = context.getActionContext(ROCKET_SEND_RESULT_KEY, SendResult.class);\\n    if (message == null || sendResult == null) {\\n      throw new TransactionException(\\"TCCRocketMQ commit but cannot find message and sendResult\\");\\n    }\\n    this.producerImpl.endTransaction(message, sendResult, LocalTransactionState.COMMIT_MESSAGE, null);\\n    LOGGER.info(\\"RocketMQ message send commit, xid = {}, branchId = {}\\", context.getXid(), context.getBranchId());\\n    return true;\\n  }\\n\\n  @Override\\n  public boolean rollback(BusinessActionContext context)\\n    throws UnknownHostException, MQBrokerException, RemotingException, InterruptedException, TransactionException {\\n    Message message = context.getActionContext(ROCKET_MSG_KEY, Message.class);\\n    SendResult sendResult = context.getActionContext(ROCKET_SEND_RESULT_KEY, SendResult.class);\\n    if (message == null || sendResult == null) {\\n      LOGGER.error(\\"TCCRocketMQ rollback but cannot find message and sendResult\\");\\n    }\\n    this.producerImpl.endTransaction(message, sendResult, LocalTransactionState.ROLLBACK_MESSAGE, null);\\n    LOGGER.info(\\"RocketMQ message send rollback, xid = {}, branchId = {}\\", context.getXid(), context.getBranchId());\\n    return true;\\n  }\\n}\\n```\\nWe can see that TCCRocketMQImpl implements the TCCRocketMQ interface and uses the @LocalTCC and @TwoPhaseBusinessAction annotations, indicating that TCCRocketMQImpl is also a TCC branch transaction, and implements the three scenarios of TCC transactions through the prepare, commit, and rollback methods.\\n\\n### prepare Method\\n```java\\n@TwoPhaseBusinessAction(name = SeataMQProducerFactory.ROCKET_TCC_NAME)\\npublic SendResult prepare(Message message, long timeout) throws MQClientException {\\n  BusinessActionContext context = BusinessActionContextUtil.getContext();\\n  LOGGER.info(\\"RocketMQ message send prepare, xid = {}\\", context.getXid());\\n  Map<String, Object> params = new HashMap<>(8);\\n  SendResult sendResult = producer.doSendMessageInTransaction(message, timeout, context.getXid(), context.getBranchId());\\n  message.setDeliverTimeMs(0);\\n  params.put(ROCKET_MSG_KEY, message);\\n  params.put(ROCKET_SEND_RESULT_KEY, sendResult);\\n  BusinessActionContextUtil.addContext(params);\\n  return sendResult;\\n}\\n```\\nIn the prepare method, we send a half-transaction message by calling the producer.doSendMessageInTransaction method and save the message and send result to the BusinessActionContext.\\n\\n### commit Method\\n```java\\n@Override\\npublic boolean commit(BusinessActionContext context)\\n  throws UnknownHostException, MQBrokerException, RemotingException, InterruptedException, TimeoutException, TransactionException {\\n  Message message = context.getActionContext(ROCKET_MSG_KEY, Message.class);\\n  SendResult sendResult = context.getActionContext(ROCKET_SEND_RESULT_KEY, SendResult.class);\\n  if (message == null || sendResult == null) {\\n    throw new TransactionException(\\"TCCRocketMQ commit but cannot find message and sendResult\\");\\n  }\\n  this.producerImpl.endTransaction(message, sendResult, LocalTransactionState.COMMIT_MESSAGE, null);\\n  LOGGER.info(\\"RocketMQ message send commit, xid = {}, branchId = {}\\", context.getXid(), context.getBranchId());\\n  return true;\\n}\\n```\\nIn the commit method, we commit the transaction message by calling the producerImpl.endTransaction method.\\n\\n### rollback Method\\n```java\\n@Override\\npublic boolean rollback(BusinessActionContext context)\\n  throws UnknownHostException, MQBrokerException, RemotingException, InterruptedException, TransactionException {\\n  Message message = context.getActionContext(ROCKET_MSG_KEY, Message.class);\\n  SendResult sendResult = context.getActionContext(ROCKET_SEND_RESULT_KEY, SendResult.class);\\n  if (message == null || sendResult == null) {\\n    LOGGER.error(\\"TCCRocketMQ rollback but cannot find message and sendResult\\");\\n  }\\n  this.producerImpl.endTransaction(message, sendResult, LocalTransactionState.ROLLBACK_MESSAGE, null);\\n  LOGGER.info(\\"RocketMQ message send rollback, xid = {}, branchId = {}\\", context.getXid(), context.getBranchId());\\n  return true;\\n}\\n```\\nIn the rollback method, we roll back the transaction message by calling the producerImpl.endTransaction method."},{"id":"/seata-raft-config-center","metadata":{"permalink":"/blog/seata-raft-config-center","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-raft-config-center.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-raft-config-center.md","title":"Seata Raft Configuration Center","description":"In this blog, I will share the Seata Configuration Center based on Raft and RocksDB design and how it is used.","date":"2024-09-19T00:00:00.000Z","formattedDate":"September 19, 2024","tags":[],"readingTime":9.52,"hasTruncateMarker":false,"authors":[{"name":"JiangYichen - Tsinghua University, participant in Seata Summer of Code"}],"frontMatter":{"title":"Seata Raft Configuration Center","author":"JiangYichen - Tsinghua University, participant in Seata Summer of Code","description":"In this blog, I will share the Seata Configuration Center based on Raft and RocksDB design and how it is used.","date":"2024/09/19","keywords":["seata","distributed transactions","configuration center","Raft","RocksDB"]},"unlisted":false,"prevItem":{"title":"How to Send Messages with RocketMQ in Seata","permalink":"/blog/how-to-send-message-with-rocketmq-in-seata"},"nextItem":{"title":"Seata\'s RPC Communication Source Code Analysis 01(Transport)","permalink":"/blog/seata-rpc-multi-protocol01"}},"content":"Currently seata supports rich third-party configuration center, but consider the convenience of using at the same time in order to reduce the threshold of using seata, in seata-server using the existing sofa-jraft+rocksdb to build a configuration center function, seata-client directly communicate with the seata-server to obtain the seata-related configuration. seata-related configuration , do not need to go to the third-party configuration center to read , to achieve the configuration center since the closed loop .\\n\\n\\n# 2. Design Description\\n## 2.1 Configuration Center\\n\\nIn the current third-party configuration center implementation, the Client and Server are decoupled when it comes to configuration centers. Both the Client and Server access configuration items through the Configuration instance. The initialization behavior for Configuration is consistent on both the Client and Server sides, involving connecting to the configuration center middleware to fetch configurations and add listeners, etc.\\n![img](/img/blog/seata-raft-config-center1.png)\\n\\nWhen the configuration center is implemented using Raft, the configuration data is stored on the Server-side. Therefore, the behavior when initializing the `Configuration` instance differs between the Client and Server sides.\\n\\nTo ensure consistency with the original configuration center logic, both the Client and Server still access configuration items through the `RaftConfiguration` instance without directly interacting with RocksDB.\\n![img](/img/blog/seata-raft-config-center2.png)\\n\\n\\n\\n![img](/img/blog/seata-raft-config-center3.png)\\n\\n`RaftConfiguration` is divided into Server-side and Client-side implementations, returning different configuration instances based on the runtime environment.\\n\\n```Java\\npublic class RaftConfigurationProvider implements ConfigurationProvider {\\n    @Override\\n    public Configuration provide() {\\n        String applicationType = System.getProperty(APPLICATION_TYPE_KEY);\\n        if (APPLICATION_TYPE_SERVER.equals(applicationType)){\\n            return RaftConfigurationServer.getInstance();\\n        }else{\\n            return RaftConfigurationClient.getInstance();\\n        }\\n    }\\n}\\n\\n@SpringBootApplication(scanBasePackages = {\\"org.apache.seata\\"})\\npublic class ServerApplication {\\n    public static void main(String[] args) throws IOException {\\n        System.setProperty(APPLICATION_TYPE_KEY, APPLICATION_TYPE_SERVER);\\n        // run the spring-boot application\\n        SpringApplication.run(ServerApplication.class, args);\\n    }\\n}\\n```\\n\\n\\n## 2.2 Configuration Storage Module\\n![img](/img/blog/seata-raft-config-center4.png)\\n### Abstract Design\\n\\nTo support and extend more KV in-memory key-value pair databases in the future (such as LevelDB, Caffeine), an abstract `ConfigStoreManager` interface and an abstract class `AbstractConfigStoreManager` have been defined, providing the following common methods:\\n\\n- Get: Acquire a specific configuration item named `key` from a given `namespace` and `dataId`.\\n- GetAll: Acquire all configuration items from a given  `namespace` and `dataId`.\\n- Put: Add/Update a configuration item `<key, value>` in a specific `namespace` and `dataId`.\\n- Delete: Delete a configuration item named `key` in a given  `namespace` and `dataId`.\\n- DeleteAll: Delete all configuration items in a given  `namespace` and `dataId`.\\n- Clear: Clear all configurations.\\n- GetAllNamespaces: Acquire all namespaces.\\n- GetAllDataIds: Acquire all configuration dataIds under a specific namespace.\\n- ...\\n\\n`ConfigStoreManagerFactory` and `ConfigStoreManagerProvider`: Configuration storage factory class and provider implemented using SPI mechanism.\\n\\n\\n\\n### Configuration Listening\\n\\nBoth the Server and Client configuration centers need to listen for changes to configuration items.\\n\\nOn the Server-side, since the configurations are stored locally, we can directly intercept the configuration change methods. We define `addConfigListener` and `removeConfigListener` methods in the abstract interface to allow users to add and remove configuration listeners. The specific implementation class handles the listening logic.\\n\\nIn `RocksDBConfigStoreManager`, the `notifyConfigChange()` method is defined to trigger listeners. When performing write-related operations (e.g., Put, Delete), this method notifies listeners about the configuration change, triggering callback events to notify the Server configuration center.\\n\\nOn the Client-side, we implement configuration listening through **configuration versioning** and **long connection mechanisms**. Specifically, the Client establishes a long connection with the Server on startup and periodically refreshes this connection. The Server maintains a `watchMap` to store all client-side listening information. Whenever the Raft state machine executes a configuration update operation, an `ApplicationEvent` event is triggered, which is listened to by the `ClusterConfigWatcherManager`, notifying all clients in the `watchMap` of the configuration change. Additionally, configuration versioning is used for optimization. When establishing a long connection, the Client must provide a version number. If the version number is lower than the version number on the Server-side, the latest configuration is returned directly. If the Server version number is lower than the local version number, the Client considers the Server configuration outdated (possibly due to server downtime or cluster split-brain) and retries the request to other nodes in the cluster.\\n\\n### Multi-Tenancy Solution\\n\\nWhen storing configurations on the Seata-Server, we need to implement multi-tenancy configuration isolation, ensuring that configurations between different tenants are independent and isolated both physically and logically.\\n\\n1. We researched the implementations of several open-source projects using RocksDB and summarized them as follows:\\n   1. JRaft uses a single RocksDB instance with two column families: one for storing Raft entries and the other for storing metadata.\\n   2. TiKV uses two RocksDB instances: raftDB and kvDB. In kvDB, multiple column families are used to store metadata, user data, lock data, etc.\\n   3. Pika creates a RocksDB instance for each data structure (String, Hash, List, Set, Zset), and each instance uses multiple column families to store data, such as Data, Meta.\\n\\nConsidering that the number of tenants is unknown in advance (and thus we cannot create a fixed number of RocksDB instances at startup), we use a single RocksDB instance with multiple column families. Different tenants are distinguished using `namespace`, and logical isolation is achieved by using column families in RocksDB, where one namespace corresponds to one column family. Column families in RocksDB are conceptually similar to tables in relational databases. When performing configuration CRUD operations, the appropriate column family is specified based on the namespace, achieving multi-tenancy isolation. Additionally, a column family named `config_version` is built-in to track the version numbers of the configurations.\\n![img](/img/blog/seata-raft-config-center5.png)\\n\\n# 3. Usage\\n## 3.0 Prepare Configuration File\\n\\nFirst, prepare the configuration file. You can refer to the example configuration file [here](https://github.com/apache/incubator-seata/blob/2.x/script/config-center/config.txt). Place this configuration file in the resource directory of the Seata server project.\\n\\n## 3.1 Server-side Configuration\\n\\nIn the **[application.yml](https://github.com/apache/incubator-seata/blob/develop/script/client/spring/application.yml)** file, add the Raft configuration center settings. For other configurations, refer to the [configuration documentation](https://seata.apache.org/en-us/docs/next/user/configurations).\\n\\n```YAML\\nconfig:\\n  # support: nacos, consul, apollo, zk, etcd3, raft\\n  type: raft\\n  raft:\\n    db:\\n      type: rocksdb  # database type, currently only rocksdb is supported\\n      dir: configStore  # directory for storing db files\\n      destroy-on-shutdown: false  # whether to clear db files on shutdown, default is false\\n      namespace: \'default\'  # namespace\\n      dataId: \'seata.properties\'  # configuration file ID\\n  file:\\n    name: \'file\'  # initial configuration file name\\n\\nserver:\\n  raft:\\n    group: default  # this value represents the group of the Raft cluster; the transaction group on the client must correspond to this value\\n    server-addr: 192.168.241.1:9091, 192.168.241.2:9091, 192.168.241.3:9091  # IP and port of other Raft nodes; the port is the netty port of the node +1000, the default netty port is 8091\\n    snapshot-interval: 600  # take a snapshot every 600 seconds for fast raftlog rolling. However, if there are many transactions in memory, this may cause performance jitter every 600 seconds. You can adjust it to 30 minutes or 1 hour depending on your business needs and test for jitter.\\n    apply-batch: 32  # apply up to 32 actions in one raftlog commit\\n    max-append-bufferSize: 262144  # maximum size of the log storage buffer, default is 256K\\n    max-replicator-inflight-msgs: 256  # maximum number of in-flight requests when pipeline requests are enabled, default is 256\\n    disruptor-buffer-size: 16384  # internal disruptor buffer size, increase this value for high write throughput scenarios; default is 16384\\n    election-timeout-ms: 1000  # timeout for leader re-election if no heartbeat is received\\n    reporter-enabled: false  # enable monitoring of Raft itself\\n    reporter-initial-delay: 60  # interval for monitoring\\n    serialization: jackson  # serialization method, do not change\\n    compressor: none  # compression method for raftlog, e.g., gzip, zstd\\n    sync: true  # log syncing method, default is synchronous syncing\\n```\\n\\nIn Seata-Server, an **initial configuration file** is required as the Server-side configuration file (as mentioned in the previous step). The `file.name` configuration item must match the name of this file. When the Server is first started, this configuration file will be used as the initial configuration for the Raft configuration center. Supported file types include: conf, yaml, properties, txt.\\n\\n> Note: The **initial configuration file** of the nodes in the Raft cluster must be consistent.\\n\\n## 3.2 Console Configuration Management Interface\\nWhen the **Raft mode** is used as the configuration center on the server side, you can manage the configuration center through the built-in configuration management page in [Seata Console](http://127.0.0.1:7091). Users can perform CRUD operations (create, read, update, delete) on configurations stored in the Seata-Server cluster. Note that these operations affect the entire cluster, so changes can be made on **any node** in the cluster, and all operations will be synchronized across the cluster via Raft.\\n\\n> Note: This configuration management page is only available when the configuration center is set to **Raft mode** and is not accessible for other configuration center types.\\n\\n### 3.2.1 Configuration Isolation\\n\\nThe Raft configuration center provides a **namespace** mechanism to achieve multi-tenant configuration isolation. Configurations in different **namespaces** are logically isolated through the underlying storage mechanism. Within the same **namespace**, multiple configuration files can exist, differentiated by **dataId**. A set of configurations is uniquely identified by both **namespace** and **dataId**.\\n\\nFor example:\\n\\n- namespace=default (default), dataId=seata.properties (default)\\n- namespace=dev, dataId=seata-server.properties, dataId=seata-client.yaml\\n- namespace=prop, dataId=seata-server.properties, dataId=seata-client.txt\\n\\n\\n![img](/img/blog/seata-raft-config-center\u914d\u7f6e\u9694\u79bb.png)\\n\\n\\n### 3.2.2 Configuration Upload\\n\\nWhen the server starts, the initial configuration file on the server will automatically be uploaded to the configuration center. In addition, users can manually upload configuration files to a specified **namespace** and **dataId** by clicking the \\"Upload\\" button. Once uploaded to the server\'s configuration center, the client can retrieve the specific configuration file via **namespace** and **dataId**.\\n\\n![img](/img/blog/seata-raft-config-center\u914d\u7f6e\u4e0a\u4f20.png)\\n\\nCurrently, supported configuration file types include txt, text, yaml, and properties. You can refer to the sample configuration files here: [Configuration File Example](https://github.com/apache/incubator-seata/blob/2.x/script/config-center/config.txt).\\n\\n### 3.2.3 Configuration Query\\n\\nAfter selecting the **namespace** and **dataId**, click the \\"**Search**\\" button to query all configuration item information under that configuration. The configuration is presented in a list, where each row represents a configuration item, displayed as **Key** and **Value** pairs.\\n\\n![img](/img/blog/seata-raft-config-center\u914d\u7f6e\u67e5\u8be2.png)\\n\\n### 3.2.4 Configuration Deletion\\n\\nWhen a configuration set is no longer needed, users can delete the configuration data for the specified **namespace** and **dataId**.\\n\\nNote that once this operation is completed, all configuration item information under that configuration will be cleared and cannot be recovered. Please avoid deleting configurations that are currently in use.\\n\\n![img](/img/blog/seata-raft-config-center\u914d\u7f6e\u5220\u9664.png)\\n\\n### 3.2.5 Configuration Modification\\n\\nIn the configuration item list, users can **add**, **modify**, or **delete** a specific configuration item. Once an operation is successful, both the server and client sides will receive the configuration change promptly, and the latest value will be available.\\n\\n- **Add:** Add a new configuration item to the current configuration.\\n\\n\\n![img](/img/blog/seata-raft-config-center\u914d\u7f6e\u4fee\u65391.png)\\n\\n- **Update:** Change the value of a specified configuration item.\\n\\n\\n![img](/img/blog/seata-raft-config-center\u914d\u7f6e\u4fee\u65392.png)\\n\\n- **Delete:** Remove a specified configuration item.\\n\\n\\n![img](/img/blog/seata-raft-config-center\u914d\u7f6e\u4fee\u65393.png)\\n\\n\\n\\n## 3.3 Client-Side Configuration\\n\\nThe client needs to add the following configuration items. The `raft.server-addr` should match the **IP address list** of the server-side Raft cluster.\\n\\n```YAML\\nconfig:\\n    type: raft  # Raft mode\\n    raft:\\n        server-addr: 192.168.241.1:7091, 192.168.241.2:7091, 192.168.241.3:7091  # Raft metadata server addresses\\n        username: \'seata\'  # Authentication\\n        password: \'seata\'  # Authentication\\n        db:\\n            namespace: \'default\'  # Namespace\\n            dataId: \'seata.properties\'  # Configuration file Id\\n```\\n\\nAdditionally, the client needs to include the **HttpClient** dependency to retrieve configuration information from the Seata-Server cluster via HTTP requests.\\n\\n```YAML\\n<dependency>\\n    <groupId>org.apache.httpcomponents</groupId>\\n    <artifactId>httpclient</artifactId>\\n</dependency>\\n```\\n\\nAfter the configuration is complete, when the client application starts, it will subscribe to and retrieve the configuration specified by **namespace** and **dataId** from the server configured in `raft.server-addr`. The client will also automatically fetch the latest configuration when changes are detected through the listener mechanism."},{"id":"/seata-rpc-multi-protocol01","metadata":{"permalink":"/blog/seata-rpc-multi-protocol01","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-rpc-multi-protocol01.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-rpc-multi-protocol01.md","title":"Seata\'s RPC Communication Source Code Analysis 01(Transport)","description":"Overview","date":"2024-08-15T00:00:00.000Z","formattedDate":"August 15, 2024","tags":[],"readingTime":6.785,"hasTruncateMarker":false,"authors":[{"name":"Xie Minghua"}],"frontMatter":{"title":"Seata\'s RPC Communication Source Code Analysis 01(Transport)","author":"Xie Minghua","keywords":["Seata","RPC Module","Protocol"],"date":"2024/08/15"},"unlisted":false,"prevItem":{"title":"Seata Raft Configuration Center","permalink":"/blog/seata-raft-config-center"},"nextItem":{"title":"Seata\'s RPC Communication Source Code Analysis 02(Multi-Version Protocols)","permalink":"/blog/seata-rpc-multi-protocol02"}},"content":"## Overview\\n\\nIn a distributed system, the design of the communication protocol directly affects the reliability and scalability of the system. apache Seata\'s RPC communication protocol provides the basis for data transfer between components, and source code analysis in this regard is another good way to gain a deeper understanding of seata. In the recent version 2.2.0, I refactored Seata\'s communication mechanism to support multi-version protocol compatibility, now that the transformation is complete, I will analyze the source code in the new version from the two aspects of the transport mechanism and communication protocol.\\nThis article is the first one to introduce the Seata transport mechanism.\\n\\nThe main characters of RPC communication in seata are `TC`, `TM` and `RM`, of course, the process may also involve other network interactions such as the registration center and even the configuration center, but these relative contents of the communication mechanism is relatively independent, and will not be discussed in this article.\\n\\nI will take you on an exploration following a few intuitive questions I asked when I first learned about the source code.\\n\\n## Netty in Seata (who\'s transmitting)\\nFirst question: what is the underlying layer of seata communication responsible for the sending of request messages and receiving of request messages? The answer is Netty, and how does Netty work inside Seata? We will explore the core package org.apache.seata.core.rpc.netty to find out.\\n\\n<img src=\\"/img/blog/rpc_multi-protocol/01-netty-class.jpg\\" width=\\"700px\\" />\\n\\nFrom this inheritance hierarchy we can see that `AbstractNettyRemoting` acts as the parent class of the core, which is implemented by RM and TM and Server(TC), and in fact the core send and receive are already implemented inside this class.\\n\\nThe synchronous sending logic is implemented in `sendSync`, the logic for asynchronous sending `sendAsync` is similar and simpler, so I won\'t repeat it here, just get the channel and send it.\\n```java\\nprotected Object sendSync(Channel channel, RpcMessage rpcMessage, long timeoutMillis) throws TimeoutException {\\n        // Non-critical code omitted here\\n\\n        MessageFuture messageFuture = new MessageFuture();\\n        messageFuture.setRequestMessage(rpcMessage);\\n        messageFuture.setTimeout(timeoutMillis);\\n        futures.put(rpcMessage.getId(), messageFuture);\\n\\n        channelWritableCheck(channel, rpcMessage.getBody());\\n\\n        String remoteAddr = ChannelUtil.getAddressFromChannel(channel);\\n        doBeforeRpcHooks(remoteAddr, rpcMessage);\\n\\n        // (netty write)\\n        channel.writeAndFlush(rpcMessage).addListener((ChannelFutureListener) future -> {\\n            if (!future.isSuccess()) {\\n                MessageFuture messageFuture1 = futures.remove(rpcMessage.getId());\\n                if (messageFuture1 != null) {\\n                    messageFuture1.setResultMessage(future.cause());\\n                }\\n                destroyChannel(future.channel());\\n            }\\n        });\\n\\n        try {\\n            Object result = messageFuture.get(timeoutMillis, TimeUnit.MILLISECONDS);\\n            doAfterRpcHooks(remoteAddr, rpcMessage, result);\\n            return result;\\n        } catch (Exception exx) {\\n            // Non-critical code omitted here\\n        }\\n    }\\n```\\nAnd the way messages are received is mainly in the processMessage method, which is called by the classes `AbstractNettyRemotingClient.ClientHandler` and `AbstractNettyRemotingServer.ServerHandler`. ChannelRead, both of which are subclasses of `ChannelDuplexHandler`, are each registered in the client and server bootstrap (why register to the bootstrap to be able to do the receiving?). You have to move to the netty principle for this one)\\n\\n<img src=\\"/img/blog/rpc_multi-protocol/03-netty-handler.jpg\\" width=\\"700px\\" />\\n\\nOnce the message is received it is called into the `processMessage` method of the parent class, let\'s take a look at the source code\\n```java\\nprotected void processMessage(ChannelHandlerContext ctx, RpcMessage rpcMessage) throws Exception {\\n        // Non-critical code\\n        Object body = rpcMessage.getBody();\\n        if (body instanceof MessageTypeAware) {\\n            MessageTypeAware messageTypeAware = (MessageTypeAware) body;\\n            final Pair<RemotingProcessor, ExecutorService> pair = this.processorTable.get((int) messageTypeAware.getTypeCode());\\n            if (pair != null) {\\n                // FIRST is Processor for normal processing, and SECOND is Thread Pool for pooled processing.\\n                if (pair.getSecond() != null) {\\n                    try {\\n                        pair.getSecond().execute(() -> {\\n                            try {\\n                                pair.getFirst().process(ctx, rpcMessage);\\n                            } catch (Throwable th) {\\n                                LOGGER.error(FrameworkErrorCode.NetDispatch.getErrCode(), th.getMessage(), th);\\n                            } finally {\\n                                MDC.clear();\\n                            }\\n                        });\\n                    } catch (RejectedExecutionException e) {\\n                        // Non-critical code\\n                    }\\n                } else {\\n                    try {\\n                        pair.getFirst().process(ctx, rpcMessage);\\n                    } catch (Throwable th) {\\n                        LOGGER.error(FrameworkErrorCode.NetDispatch.getErrCode(), th.getMessage(), th);\\n                    }\\n                }\\n            } else {\\n                LOGGER.error(\\"This message type [{}] has no processor.\\", messageTypeAware.getTypeCode());\\n            }\\n        } else {\\n            LOGGER.error(\\"This rpcMessage body[{}] is not MessageTypeAware type.\\", body);\\n        }\\n    }\\n```\\nThese processors and executors are actually processors registered by the client and server: here are some of the processors, which correspond to different MessageTypes, and here is an example of the registration of some of them (they are registered in the NettyRemotingServer# registerProcessor)\\n```java\\n        super.registerProcessor(MessageType.TYPE_GLOBAL_ROLLBACK, onRequestProcessor, messageExecutor);\\n        super.registerProcessor(MessageType.TYPE_GLOBAL_STATUS, onRequestProcessor, messageExecutor);\\n        super.registerProcessor(MessageType.TYPE_SEATA_MERGE, onRequestProcessor, messageExecutor);\\n        super.registerProcessor(MessageType.TYPE_BRANCH_COMMIT_RESULT, onResponseProcessor, branchResultMessageExecutor);\\n        super.registerProcessor(MessageType.TYPE_BRANCH_ROLLBACK_RESULT, onResponseProcessor, branchResultMessageExecutor);\\n        super.registerProcessor(MessageType.TYPE_REG_RM, regRmProcessor, messageExecutor);\\n        super.registerProcessor(MessageType.TYPE_REG_CLT, regTmProcessor, null);\\n```\\nYou can see that these processors are actually the processors for seata\'s various commit rollbacks and so on.\\n\\n##  NettyChannel in Seata (how channels are managed)\\nSo, the second question, since netty relies on a channel to send and receive, how will this channel come about? Will it always be held? If it breaks, how do we reconnect it? The answer can be found in the `ChannelManager` and the `processor` of the two regs above.\\n\\nWhen RM/TM gets the address of the server and registers (the first time it communicates), if the server can successfully parse the message and find it is a REG message, it will enter `regRmProcessor`/`regTmProcessor`, take TM as an example here.\\n\\n```java\\n// server RegTmProcessor\\n    private void onRegTmMessage(ChannelHandlerContext ctx, RpcMessage rpcMessage) {\\n        RegisterTMRequest message = (RegisterTMRequest) rpcMessage.getBody();\\n        String ipAndPort = NetUtil.toStringAddress(ctx.channel().remoteAddress());\\n        Version.putChannelVersion(ctx.channel(), message.getVersion());\\n        boolean isSuccess = false;\\n        String errorInfo = StringUtils.EMPTY;\\n        try {\\n            if (null == checkAuthHandler || checkAuthHandler.regTransactionManagerCheckAuth(message)) {\\n                // Register the channel in the ChannelManager, it can be expected that after the registration, the server will be able to get the channel when it sendsSync(channel,xxx).\\n                ChannelManager.registerTMChannel(message, ctx.channel());\\n                Version.putChannelVersion(ctx.channel(), message.getVersion());\\n                isSuccess = true;\\n            }\\n        } catch (Exception exx) {\\n            isSuccess = false;\\n            errorInfo = exx.getMessage();\\n            LOGGER.error(\\"TM register fail, error message:{}\\", errorInfo);\\n        }\\n        RegisterTMResponse response = new RegisterTMResponse(isSuccess);\\n        // async response\\n        remotingServer.sendAsyncResponse(rpcMessage, ctx.channel(), response);\\n        // ...\\n    }\\n\\n//    ChannelManager\\n    public static void registerTMChannel(RegisterTMRequest request, Channel channel)\\n        throws IncompatibleVersionException {\\n        RpcContext rpcContext = buildChannelHolder(NettyPoolKey.TransactionRole.TMROLE, request.getVersion(),\\n            request.getApplicationId(),\\n            request.getTransactionServiceGroup(),\\n            null, channel);\\n        rpcContext.holdInIdentifiedChannels(IDENTIFIED_CHANNELS);\\n        String clientIdentified = rpcContext.getApplicationId() + Constants.CLIENT_ID_SPLIT_CHAR + ChannelUtil.getClientIpFromChannel(channel);\\n        ConcurrentMap<Integer, RpcContext> clientIdentifiedMap = CollectionUtils.computeIfAbsent(TM_CHANNELS, clientIdentified, key -> new ConcurrentHashMap<>());\\n        rpcContext.holdInClientChannels(clientIdentifiedMap);\\n    }\\n```\\nThe ChannelManager manages `RM_CHANNELS` and `RM_CHANNELS`, two complex maps, especially RM_CHANNELS which has 4 layers (resourceId -> applicationId -> ip -> port -> RpcContext).\\n\\nHaving said that the server manages the channel, what about the client? This map management is a little simpler, that is, after successful registration in the onRegisterMsgSuccess also use a `NettyClientChannelManager` in registerChannel, subsequent interaction with the server as much as possible with this channel.\\n\\nThe third problem is that the client can create a new channel if the channel is not available,\\nbut what if the server receives it and realizes that it is a new channel?\\nOr what if the server realizes that the channel is not available when it replies asynchronously?\\nThe answer is still in the `NettyClientChannelManager`, which is relatively complex, the client side need to use the channel,\\nin fact, managed by an object pool `nettyClientKeyPool`, which is an apache object pool,\\nso when the channel is unavailable, it will also be managed by this pool.\\nThis is an Apache objectPool, Thus, when the channel is unavailable, it will be created with the help of this pool and then returned to the pool after use.\\nThis object pool actually holds the `RegisterTMRequest` at all times, just as it did when it first came in,\\nso every time a channel is created , a registration occurs.\\n```java\\n// NettyClientChannelManager\\n    public Channel makeObject(NettyPoolKey key) {\\n        InetSocketAddress address = NetUtil.toInetSocketAddress(key.getAddress());\\n        if (LOGGER.isInfoEnabled()) {\\n            LOGGER.info(\\"NettyPool create channel to \\" + key);\\n        }\\n        Channel tmpChannel = clientBootstrap.getNewChannel(address);\\n        Object response;\\n        Channel channelToServer = null;\\n        // key RegisterTMRequest\\n        if (key.getMessage() == null) {\\n            throw new FrameworkException(\\"register msg is null, role:\\" + key.getTransactionRole().name());\\n        }\\n        try {\\n            // a register operation\\n            response = rpcRemotingClient.sendSyncRequest(tmpChannel, key.getMessage());\\n            if (!isRegisterSuccess(response, key.getTransactionRole())) {\\n                rpcRemotingClient.onRegisterMsgFail(key.getAddress(), tmpChannel, response, key.getMessage());\\n            } else {\\n                channelToServer = tmpChannel;\\n                rpcRemotingClient.onRegisterMsgSuccess(key.getAddress(), tmpChannel, response, key.getMessage());\\n            }\\n        }\\n        // ...\\n\\n        return channelToServer;\\n    }\\n```\\n\\n## Summarize\\nIn this article we learned how seata transfers data with the help of netty, to better see the full picture of netty processing, I created a hierarchical diagram\\n\\n<img src=\\"/img/blog/rpc_multi-protocol/00-netty-layer.png\\" width=\\"700px\\" />\\n\\nWe have already talked about the processing of serverHandler/clientHandler and NettyRemoting (including RM, TM, TC) when the request is sent, and we know the process from the external to the netty processor and then to the internal DefaultCoordinator, but we are still missing Decoder/Encoder. Didn\'t talk about it, the parsing/encapsulation of the protocol will be done here, serialization and deserialization will also be done, see [Seata\'s RPC Communication Source Code Analysis 02(Protocol)](seata-rpc-multi-protocol02.md)"},{"id":"/seata-rpc-multi-protocol02","metadata":{"permalink":"/blog/seata-rpc-multi-protocol02","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-rpc-multi-protocol02.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-rpc-multi-protocol02.md","title":"Seata\'s RPC Communication Source Code Analysis 02(Multi-Version Protocols)","description":"Overview","date":"2024-08-15T00:00:00.000Z","formattedDate":"August 15, 2024","tags":[],"readingTime":7.83,"hasTruncateMarker":false,"authors":[{"name":"Xie Minghua"}],"frontMatter":{"title":"Seata\'s RPC Communication Source Code Analysis 02(Multi-Version Protocols)","author":"Xie Minghua","keywords":["Seata","RPC Module","Protocol"],"date":"2024/08/15"},"unlisted":false,"prevItem":{"title":"Seata\'s RPC Communication Source Code Analysis 01(Transport)","permalink":"/blog/seata-rpc-multi-protocol01"},"nextItem":{"title":"How to Write Test Cases in Seata","permalink":"/blog/how-to-write-unit-tests"}},"content":"### Overview\\n\\nIn the previous article,[Seata\'s RPC Communication Source Code Analysis 01(Transport)](seata-rpc-multi-protocol01.md)we introduced the transmission mechanism of RPC communication. In this article, we will continue with the protocol part, completing the unaddressed encode/decode sections in the diagram.\\n\\n<img src=\\"/img/blog/rpc_multi-protocol/00-netty-layer.png\\" width=\\"700px\\" />\\n\\nSimilarly, we will delve into the topic using a question-driven approach. In this article, we aim not only to understand how binary data is parsed into the rpcMsg type but also to explore how different protocol versions are supported. So, the first question is: What does the protocol look like?\\n## Structure of Protocol\\n<img src=\\"/img/blog/rpc_multi-protocol/04-protocol.jpg\\" width=\\"900px\\" />\\n\\nThe diagram illustrates the changes in the protocol before and after version 0.7.1 (you can also refer to the comments in ProtocolDecoderV1, and for older versions, check ProtocolV1Decoder). In the new version, the protocol consists of the following components:\\n\\n- magic-code: 0xdada\\n- protocol-version: Version number\\n- full-length: Total length\\n- head-length: Header length\\n- msgtype: Message type\\n- serializer/codecType: Serialization method\\n- compress: Compression method\\n- requestid: Request ID\\n\\n\\nHere, we will explain the differences in protocol handling across various versions of Seata\'s server:\\n- version`<`0.7.1 : Can only handle the v0 version of the protocol (the upper part of the diagram, which includes the flag section) and cannot recognize other protocol versions.\\n- 0.7.1`<=`version`<`2.2.0 : Can only handle the v1 version of the protocol (the lower part of the diagram) and cannot recognize other protocol versions.\\n- version`>=`2.2.0 : Can recognize and process both v0 and v1 versions of the protocol.\\n\\nSo, how does version 2.2.0 achieve compatibility? Let\'s keep that a mystery for now. Before explaining this, let\'s first take a look at how the v1 encoder and decoder operate. It is important to note that, just like the transmission mechanism we discussed earlier, protocol handling is also shared between the client and server. Therefore, the logic we will discuss below applies to both.\\n## From ByteBuf to RpcMessage (What the Encoder/Decoder Does)\\nFirst`ProtocolDecoderV1`\\n```java\\n    public RpcMessage decodeFrame(ByteBuf frame) {\\n        byte b0 = frame.readByte();\\n        byte b1 = frame.readByte();\\n\\n        // get version\\n        byte version = frame.readByte();\\n        // get header,body,...\\n        int fullLength = frame.readInt();\\n        short headLength = frame.readShort();\\n        byte messageType = frame.readByte();\\n        byte codecType = frame.readByte();\\n        byte compressorType = frame.readByte();\\n        int requestId = frame.readInt();\\n\\n        ProtocolRpcMessageV1 rpcMessage = new ProtocolRpcMessageV1();\\n        rpcMessage.setCodec(codecType);\\n        rpcMessage.setId(requestId);\\n        rpcMessage.setCompressor(compressorType);\\n        rpcMessage.setMessageType(messageType);\\n\\n        // header\\n        int headMapLength = headLength - ProtocolConstants.V1_HEAD_LENGTH;\\n        if (headMapLength > 0) {\\n            Map<String, String> map = HeadMapSerializer.getInstance().decode(frame, headMapLength);\\n            rpcMessage.getHeadMap().putAll(map);\\n        }\\n\\n        if (messageType == ProtocolConstants.MSGTYPE_HEARTBEAT_REQUEST) {\\n            rpcMessage.setBody(HeartbeatMessage.PING);\\n        } else if (messageType == ProtocolConstants.MSGTYPE_HEARTBEAT_RESPONSE) {\\n            rpcMessage.setBody(HeartbeatMessage.PONG);\\n        } else {\\n            int bodyLength = fullLength - headLength;\\n            if (bodyLength > 0) {\\n                byte[] bs = new byte[bodyLength];\\n                frame.readBytes(bs);\\n                // According to the previously extracted compressorType, decompression is performed as needed.\\n                Compressor compressor = CompressorFactory.getCompressor(compressorType);\\n                bs = compressor.decompress(bs);\\n                SerializerType protocolType = SerializerType.getByCode(rpcMessage.getCodec());\\n                if (this.supportDeSerializerTypes.contains(protocolType)) {\\n                    // Since this is the ProtocolDecoderV1 specifically for version 1, the serializer can directly use version1 as input.\\n                    Serializer serializer = SerializerServiceLoader.load(protocolType, ProtocolConstants.VERSION_1);\\n                    rpcMessage.setBody(serializer.deserialize(bs));\\n                } else {\\n                    throw new IllegalArgumentException(\\"SerializerType not match\\");\\n                }\\n            }\\n        }\\n        return rpcMessage.protocolMsg2RpcMsg();\\n    }\\n```\\nSince the encode operation is the exact reverse of the decode operation, we won\u2019t go over it again. Let\u2019s continue by examining the serialize operation.\\nthe serialize comes from `SerializerServiceLoader`\\n```java\\n    public static Serializer load(SerializerType type, byte version) throws EnhancedServiceNotFoundException {\\n        // PROTOBUF\\n        if (type == SerializerType.PROTOBUF) {\\n            try {\\n                ReflectionUtil.getClassByName(PROTOBUF_SERIALIZER_CLASS_NAME);\\n            } catch (ClassNotFoundException e) {\\n                throw new EnhancedServiceNotFoundException(\\"\'ProtobufSerializer\' not found. \\" +\\n                        \\"Please manually reference \'org.apache.seata:seata-serializer-protobuf\' dependency \\", e);\\n            }\\n        }\\n\\n        String key = serialzerKey(type, version);\\n        //Here is a SERIALIZER_MAP, which acts as a cache for serializer classes. The reason for caching is that the scope of SeataSerializer is set to Scope.PROTOTYPE, which prevents the class from being created multiple times.\\n        Serializer serializer = SERIALIZER_MAP.get(key);\\n        if (serializer == null) {\\n            if (type == SerializerType.SEATA) {\\n                // SPI of seata\\n                serializer = EnhancedServiceLoader.load(Serializer.class, type.name(), new Object[]{version});\\n            } else {\\n                serializer = EnhancedServiceLoader.load(Serializer.class, type.name());\\n            }\\n            SERIALIZER_MAP.put(key, serializer);\\n        }\\n        return serializer;\\n    }\\n\\n    public SeataSerializer(Byte version) {\\n        if (version == ProtocolConstants.VERSION_0) {\\n            versionSeataSerializer = SeataSerializerV0.getInstance();\\n        } else if (version == ProtocolConstants.VERSION_1) {\\n            versionSeataSerializer = SeataSerializerV1.getInstance();\\n        }\\n        if (versionSeataSerializer == null) {\\n            throw new UnsupportedOperationException(\\"version is not supported\\");\\n        }\\n    }\\n```\\nWith this, the decoder obtains a Serializer. When the program reaches`rpcMessage.setBody(serializer.deserialize(bs))`,\\nlet\'s take a look at how the deserialize method processes the data.\\n```java\\n    public <T> T deserialize(byte[] bytes) {\\n            return deserializeByVersion(bytes, ProtocolConstants.VERSION_0);\\n    }\\n    private static <T> T deserializeByVersion(byte[] bytes, byte version) {\\n        //The previous part involves validity checks, which we will skip here.\\n        ByteBuffer byteBuffer = ByteBuffer.wrap(bytes);\\n        short typecode = byteBuffer.getShort();\\n        ByteBuffer in = byteBuffer.slice();\\n        //create Codec\\n        AbstractMessage abstractMessage = MessageCodecFactory.getMessage(typecode);\\n        MessageSeataCodec messageCodec = MessageCodecFactory.getMessageCodec(typecode, version);\\n        //codec decode\\n        messageCodec.decode(abstractMessage, in);\\n        return (T) abstractMessage;\\n    }\\n```\\nThis serialize does not contain much logic, the key components is in the MessageCodecFactory and Codec, let\'s delve deeper.\\nYou can see that `MessageCodecFactory` has quite a lot of content, but in a single form, they all return message and codec according to MessageType,\\nso we won\'t show the content of factory here, we will directly look at message and codec, that is, `messageCodec.decode( abstractMessage, in)`,\\nalthough there are still a lot of codec types, but we can see that their structure is similar, parsing each field:\\n```java\\n    // BranchRegisterRequestCodec decode\\n    public <T> void decode(T t, ByteBuffer in) {\\n        BranchRegisterRequest branchRegisterRequest = (BranchRegisterRequest)t;\\n\\n        // get xid\\n        short xidLen = in.getShort();\\n        if (xidLen > 0) {\\n            byte[] bs = new byte[xidLen];\\n            in.get(bs);\\n            branchRegisterRequest.setXid(new String(bs, UTF8));\\n        }\\n        // get branchType\\n        branchRegisterRequest.setBranchType(BranchType.get(in.get()));\\n        short len = in.getShort();\\n        if (len > 0) {\\n            byte[] bs = new byte[len];\\n            in.get(bs);\\n            branchRegisterRequest.setResourceId(new String(bs, UTF8));\\n        }\\n        // get lockKey\\n        int iLen = in.getInt();\\n        if (iLen > 0) {\\n            byte[] bs = new byte[iLen];\\n            in.get(bs);\\n            branchRegisterRequest.setLockKey(new String(bs, UTF8));\\n        }\\n        // get applicationData\\n        int applicationDataLen = in.getInt();\\n        if (applicationDataLen > 0) {\\n            byte[] bs = new byte[applicationDataLen];\\n            in.get(bs);\\n            branchRegisterRequest.setApplicationData(new String(bs, UTF8));\\n        }\\n    }\\n```\\nWell, by this point, we\'ve got the branchRegisterRequest, which can be handed off to the TCInboundHandler for processing.\\n\\nBut the problem is again, we only see the client (RM/TM) has the following kind of code to add encoder/decoder, that is, we know the client are using the current version of encoder/decoder processing:\\n```java\\n        bootstrap.handler(\\n            new ChannelInitializer<SocketChannel>() {\\n                @Override\\n                public void initChannel(SocketChannel ch) {\\n                    ChannelPipeline pipeline = ch.pipeline();\\n                    pipeline.addLast(new IdleStateHandler(nettyClientConfig.getChannelMaxReadIdleSeconds(),nettyClientConfig.getChannelMaxWriteIdleSeconds(),nettyClientConfig.getChannelMaxAllIdleSeconds()))\\n                        .addLast(new ProtocolDecoderV1())\\n                        .addLast(new ProtocolEncoderV1());\\n                    if (channelHandlers != null) {\\n                        addChannelPipelineLast(ch, channelHandlers);\\n                    }\\n                }\\n            });\\n```\\nBut how does server handle it? And what about the promised multi-version protocol?\\n\\n## Multi-version protocol (version recognition and binding)\\nLet\'s start by looking at a class diagram for encoder/decoder:\\n\\n<img src=\\"/img/blog/rpc_multi-protocol/05-encode-decode-class.jpg\\" width=\\"800px\\" />\\n\\nProtocolDecoderV1 we have analyzed, ProtocolEncoderV1 is the reverse operation, it should be better understood, as for ProtocolDecoderV0 and ProtocolEncoderV0, from the diagram you can also see that they are in parallel with v1, except for the operation of v0 (although so far we haven\'t put him to use yet), they are both subclasses of the typical encode and decode in netty, but what about MultiProtocolDecoder? He\'s the protagonist of the MultiProtocolDecoder and is registered into the server\'s bootstrap at startup.\\n\\n```java\\n    protected boolean isV0(ByteBuf in) {\\n        boolean isV0 = false;\\n        in.markReaderIndex();\\n        byte b0 = in.readByte();\\n        byte b1 = in.readByte();\\n        // In fact, identifying the protocol relies on the 3rd byte (b2), as long as it is a normal new version, b2 is the version number greater than 0. For versions below 0.7, b2 is the first bit of the FLAG, which just so happens to be 0 in either case!\\n        // v1/v2/v3 : b2 = version\\n        // v0 : b2 = 0 ,1st byte in FLAG(2byte:0x10/0x20/0x40/0x80)\\n        byte b2 = in.readByte();\\n        if (ProtocolConstants.MAGIC_CODE_BYTES[0] == b0 && ProtocolConstants.MAGIC_CODE_BYTES[1] == b1 && 0 == b2) {\\n            isV0 = true;\\n        }\\n        // The read bytes have to be reset back in order for each version of the decoder to parse them from scratch.\\n        in.resetReaderIndex();\\n        return isV0;\\n    }\\n    protected Object decode(ChannelHandlerContext ctx, ByteBuf in) throws Exception {\\n        ByteBuf frame;\\n        Object decoded;\\n        byte version;\\n        try {\\n            // Identify the version number and get the current version number\\n            if (isV0(in)) {\\n                decoded = in;\\n                version = ProtocolConstants.VERSION_0;\\n            } else {\\n                decoded = super.decode(ctx, in);\\n                version = decideVersion(decoded);\\n            }\\n\\n            if (decoded instanceof ByteBuf) {\\n                frame = (ByteBuf) decoded;\\n                ProtocolDecoder decoder = protocolDecoderMap.get(version);\\n                ProtocolEncoder encoder = protocolEncoderMap.get(version);\\n                try {\\n                    if (decoder == null || encoder == null) {\\n                        throw new UnsupportedOperationException(\\"Unsupported version: \\" + version);\\n                    }\\n                    // First time invoke ,use a well-judged decoder for the operation\\n                    return decoder.decodeFrame(frame);\\n                } finally {\\n                    if (version != ProtocolConstants.VERSION_0) {\\n                        frame.release();\\n                    }\\n                    // First time invoke , bind the encoder and decoder corresponding to the version, which is equivalent to binding the channel\\n                    ctx.pipeline().addLast((ChannelHandler)decoder);\\n                    ctx.pipeline().addLast((ChannelHandler)encoder);\\n                    if (channelHandlers != null) {\\n                        ctx.pipeline().addLast(channelHandlers);\\n                    }\\n                    // After binding, remove itself and do not judge it subsequently\\n                    ctx.pipeline().remove(this);\\n                }\\n            }\\n        } catch (Exception exx) {\\n            LOGGER.error(\\"Decode frame error, cause: {}\\", exx.getMessage());\\n            throw new DecodeException(exx);\\n        }\\n        return decoded;\\n    }\\n\\n    protected byte decideVersion(Object in) {\\n        if (in instanceof ByteBuf) {\\n            ByteBuf frame = (ByteBuf) in;\\n            frame.markReaderIndex();\\n            byte b0 = frame.readByte();\\n            byte b1 = frame.readByte();\\n            if (ProtocolConstants.MAGIC_CODE_BYTES[0] != b0 || ProtocolConstants.MAGIC_CODE_BYTES[1] != b1) {\\n                throw new IllegalArgumentException(\\"Unknown magic code: \\" + b0 + \\", \\" + b1);\\n            }\\n\\n            byte version = frame.readByte();\\n            frame.resetReaderIndex();\\n            return version;\\n        }\\n        return -1;\\n    }\\n```\\nWith the above analysis, v0 finally comes in handy (when a client with an older version registers,\\nthe server assigns it a lower version of encoder/decoder), and we\'ve figured out how multi-version protocols are recognized and bound."},{"id":"/how-to-write-unit-tests","metadata":{"permalink":"/blog/how-to-write-unit-tests","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/how-to-write-unit-tests.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/how-to-write-unit-tests.md","title":"How to Write Test Cases in Seata","description":"This article mainly introduces the testing frameworks already used in Seata and community suggestions on how developers can better write test cases.","date":"2024-02-20T00:00:00.000Z","formattedDate":"February 20, 2024","tags":[],"readingTime":6.065,"hasTruncateMarker":false,"authors":[{"name":"Wang Zhongxiang - trustdecision Technical Expert"}],"frontMatter":{"title":"How to Write Test Cases in Seata","keywords":["Seata","unit test","junit","mockito","assertj"],"description":"This article mainly introduces the testing frameworks already used in Seata and community suggestions on how developers can better write test cases.","author":"Wang Zhongxiang - trustdecision Technical Expert","date":"2024-02-20T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Seata\'s RPC Communication Source Code Analysis 02(Multi-Version Protocols)","permalink":"/blog/seata-rpc-multi-protocol02"},"nextItem":{"title":"Exploring the Journey of Open Source Development in Seata Project","permalink":"/blog/explore-seata-journey"}},"content":"## Background\\nAs the Seata project continues to grow and expand, our contributor community is also continuously growing. With the continuous enhancement of project functionality, the requirements for code quality are also increasing. In this process, we expect every contributor to provide standardized and comprehensive test cases along with their feature code submissions.\\n\\nAn excellent project relies on comprehensive unit tests as a fundamental guarantee. The Test-Driven Development (TDD) concept has been proposed for many years, emphasizing writing test cases before writing functional code. By writing unit tests, developers can gain a deeper understanding of the roles of related classes and methods in the code, grasp the core logic, and become familiar with the running scenarios of various situations. Meanwhile, unit tests also provide stable and secure protection for open-source projects, ensuring the quality and stability of the code when accepting contributor submissions. Unit testing is the first line of defense for quality assurance. Effective unit testing can detect over 90% of code bugs in advance and prevent code deterioration. During project refactoring and evolution, unit testing plays a crucial role, ensuring that the refactored code continues to function properly without introducing new bugs.\\n\\nIn the community\'s view, contributing reasonable test case code is equally important as contributing functional code. To help developers write high-quality test cases, this article provides some basic standards and recommendations.\\n\\n## Recommended Frameworks\\nThe community currently uses the following three frameworks to write test cases:\\n\\n### junit5\\njunit is the most commonly used unit testing framework in Java, used for writing and running repeatable test cases.\\n\\n\\n```java\\n        <junit-jupiter.version>5.8.2</junit-jupiter.version>\\n        <dependency>\\n            <groupId>org.junit</groupId>\\n            <artifactId>junit-bom</artifactId>\\n            <version>${junit-jupiter.version}</version>\\n        </dependency>\\n```\\n\\n### mockito\\n[mockito](https://javadoc.io/static/org.mockito/mockito-core/5.10.0/org/mockito/Mockito.html)It is a mock framework mainly used for mock testing. It can simulate any bean managed by Spring, mock method return values, simulate throwing exceptions, etc. This allows us to complete testing and verification in situations where some dependencies are missing.\\n\\n```java\\n        <mockito.version>4.11.0</mockito.version>\\n        <dependency>\\n            <groupId>org.mockito</groupId>\\n            <artifactId>mockito-core</artifactId>\\n            <version>${mockito.version}</version>\\n        </dependency>\\n        <dependency>\\n            <groupId>org.mockito</groupId>\\n            <artifactId>mockito-junit-jupiter</artifactId>\\n            <version>${mockito.version}</version>\\n        </dependency>\\n        <dependency>\\n            <groupId>org.mockito</groupId>\\n            <artifactId>mockito-inline</artifactId>\\n            <version>${mockito.version}</version>\\n        </dependency>\\n```\\n### assertj\\nassertj is an assertion library that provides a set of easy-to-use and highly readable assertion methods. When junit\'s assertions are difficult to meet, assertj can be used for assertions.\\n\\nPlease note: We manage the versions of these three libraries uniformly in the pom.xml of seata-dependencies.\\n\\n\\n```java\\n        <assertj-core.version>3.12.2</assertj-core.version>\\n        <dependency>\\n            <groupId>org.assertj</groupId>\\n            <artifactId>assertj-core</artifactId>\\n            <version>${assertj-core.version}</version>\\n        </dependency>\\n```\\n## Specifications\\nWe have referenced the Alibaba Java Development Manual and compiled some suggestions and specifications, divided into different levels. Among them, the [[mandatory]] parts must be strictly adhered to by developers. The community will review the code according to the mandatory rules when merging it. The [[recommended]] and [[reference]] parts are provided to help everyone better understand our considerations and principles for test cases.\\n\\n##### 1. [[mandatory]] Unit tests must adhere to the AIR principle.\\n\\nExplanation: Good unit tests, from a macro perspective, possess characteristics of automation, independence, and repeatability.\\n- A: Automatic\\n- I: Independent\\n- R: Repeatable\\n\\n##### 2. [[mandatory]] Unit tests should be fully automated and non-interactive.\\nTest cases are usually executed periodically, and the execution process must be fully automated to be meaningful. Tests that require manual inspection of output results are not good unit tests. System.out should not be used for manual verification in unit tests; assert must be used for verification.\\n\\n##### 3. [[mandatory]] Maintain the independence of unit tests. To ensure the stability, reliability, and ease of maintenance of unit tests, unit test cases must not call each other or depend on the execution order.\\nCounterexample: method2 depends on the execution of method1, with the result of method1 being used as input for method2.\\n\\n##### 4. [[mandatory]] Unit tests must be repeatable and unaffected by external environments.\\nExplanation: Unit tests are usually included in continuous integration, and unit tests are executed each time code is checked in. If unit tests depend on external environments (network, services, middleware, etc.), it can lead to the unavailability of the continuous integration mechanism.\\n\\nExample: To avoid being affected by external environments, it is required to design the code to inject dependencies into the SUT. During testing, use a DI framework like Spring to inject a local (in-memory) implementation or a Mock implementation.\\n\\n##### 5. [[mandatory]] For unit tests, ensure that the granularity of testing is small enough to facilitate precise issue localization. The granularity of unit testing is at most at the class level, generally at the method level.\\nExplanation: Only with small granularity can errors be quickly located when they occur. Unit tests are not responsible for checking cross-class or cross-system interaction logic; that is the domain of integration testing.\\n\\n##### 6. [[mandatory]] Incremental code for core business, core applications, and core modules must ensure that unit tests pass.\\nExplanation: Add unit tests promptly for new code. If new code affects existing unit tests, promptly make corrections.\\n\\n##### 7. [[mandatory]] Unit test code must be written in the following project directory: src/test/java; it is not allowed to be written in the business code directory.\\nExplanation: This directory is skipped during source code compilation, and the unit test framework defaults to scanning this directory.\\n\\n##### 8. [[mandatory]] The basic goal of unit testing: achieve a statement coverage of 70%; the statement coverage and branch coverage of core modules must reach 100%.\\nExplanation: As mentioned in the application layering of project conventions, DAO layer, Manager layer, and highly reusable Service should all undergo unit testing.\\n\\n##### 9. [[recommended]] When writing unit test code, adhere to the BCDE principle to ensure the delivery quality of the tested modules.\\n- B: Border, boundary value testing, including loop boundaries, special values, special time points, data sequences, etc.\\n- C: Correct, correct input, and expected results.\\n- D: Design, combined with design documents, to write unit tests.\\n- E: Error, forced error message input (such as: illegal data, exceptional processes, business allowance outside, etc.), and expected results.\\n\\n##### 10. [[recommended]] For database-related operations such as queries, updates, and deletions, do not assume that the data in the database exists, or directly manipulate the database to insert data. Please use program insertion or data import to prepare data.\\nCounterexample: In a unit test for deleting a row of data, manually add a row directly into the database as the deletion target. However, this newly added row of data does not comply with the business insertion rules, resulting in abnormal test results.\\n\\n##### 11. [[recommended]] For database-related unit tests, an automatic rollback mechanism can be set to prevent dirty data from being left in the database due to unit testing. Alternatively, clear prefix and suffix identifiers can be used for data generated by unit testing.\\n\\n##### 12. [[recommended]] For code that is untestable, necessary refactoring should be done at the appropriate time to make the code testable, avoiding writing non-standard test code just to meet testing requirements.\\n\\n##### 13. [[recommended]] Unit tests, as a means of quality assurance, should complete the writing and verification of unit tests before submitting a pull request.\\n\\n##### 14. [[reference]] To facilitate unit testing, business code should avoid the following situations:\\n- Doing too much in constructors.\\n- Having too many global variables and static methods.\\n- Having too many external dependencies.\\n- Having too many conditional statements.\\nExplanation: For multiple conditional statements, it is recommended to refactor using guard clauses, strategy patterns, state patterns, etc."},{"id":"/explore-seata-journey","metadata":{"permalink":"/blog/explore-seata-journey","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/explore-seata-journey.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/explore-seata-journey.md","title":"Exploring the Journey of Open Source Development in Seata Project","description":"In this article, I will share my journey as a developer in the Seata community, along with the experiences and insights I have gained during this adventure.","date":"2023-11-27T00:00:00.000Z","formattedDate":"November 27, 2023","tags":[],"readingTime":11.105,"hasTruncateMarker":false,"authors":[{"name":"Yinxiangkun - Tsinghua University, participant in Seata Summer of Code"}],"frontMatter":{"title":"Exploring the Journey of Open Source Development in Seata Project","keywords":["Seata","open source","Summer of Code","distributed transactions"],"description":"In this article, I will share my journey as a developer in the Seata community, along with the experiences and insights I have gained during this adventure.","author":"Yinxiangkun - Tsinghua University, participant in Seata Summer of Code","date":"2023-11-27T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"How to Write Test Cases in Seata","permalink":"/blog/how-to-write-unit-tests"},"nextItem":{"title":"Seata-Raft Storage Mode in Depth and Getting Started","permalink":"/blog/seata-raft-detailed-explanation"}},"content":"> Seata is an open-source distributed transaction solution dedicated to providing high-performance and user-friendly distributed transaction services in a microservices architecture. During this year\'s Summer of Code event, I joined the Apache Seata (Incubator) community, completed the Summer of Code project, and have been actively involved in the community ever since. I was fortunate to share my developer experience at the YunQi Developer Show during the Cloud Conferen\\n\\n## Relevant Background\\n\\nBefore formally introducing my experiences, I would like to provide some relevant background information to explain why I chose to participate in open source and how I got involved. There are various motivations for participating in open source, and here are some of the main reasons I believe exist:\\n\\n- **Learning**: Participating in open source provides us with the opportunity to contribute to open-source projects developed by different organizations, interact with industry experts, and offers learning opportunities.\\n- **Skill Enhancement**: In my case, I usually work with Java and Python for backend development. However, when participating in the Seata project, I had the chance to learn the Go language, expanding my backend technology stack. Additionally, as a student, it\'s challenging to encounter production-level frameworks or applications, and the open-source community provided me with this opportunity.\\n- **Interest**: Many of my friends are passionate about open source, enjoying programming and being enthusiastic about open source.\\n- **Job Seeking**: Participating in open source can enrich our portfolio, adding weight to resumes.\\n- **Work Requirements**: Sometimes, involvement in open source is to address work-related challenges or meet job requirements.\\n\\nThese are some reasons for participating in open source. For me, learning, skill enhancement, and interest are the primary motivations. Whether you are a student or a working professional, if you have the willingness to participate in open source, don\'t hesitate. Anyone can contribute to open-source projects. Age, gender, occupation, and location are not important; the key is your passion and curiosity about open-source projects.\\n\\n**The opportunity for me to participate in open source arose when I joined the Open Source Promotion Plan (OSPP) organized by the Institute of Software, Chinese Academy of Sciences.**\\n\\nOSPP is an open-source activity for university developers. The community releases open-source projects, and student developers complete project development under the guidance of mentors. The completed results are contributed to the community, merged into the community repository, and participants receive project bonuses and certificates. OSPP is an excellent opportunity to enter the open-source community, and it was my first formal encounter with open-source projects. This experience opened a new door for me. I deeply realized that participating in the construction of open-source projects, sharing your technical achievements, and enabling more developers to use what you contribute is a joyful and meaningful endeavor.\\n\\nThe image below, officially released by OSPP, shows that the number of participating communities and students has been increasing year by year since 2020, and the event is getting better. This year, a total of 133 community projects were involved, each providing several topics, with each student selecting only one topic. Choosing a community to participate in and finding a suitable topic in such a large number of communities is a relatively complex task.\\n\\n![img](/img/blog/explore-seata-ospp.png)\\n\\n**Considering factors such as community activity, technical stack compatibility, and guidance for newcomers, I ultimately chose to join the Seata community.**\\n\\nSeata is an open-source distributed transaction framework that provides a complete distributed transaction solution, including AT, TCC, Saga, and XA transaction modes, and supports multiple programming languages and data storage solutions. Since its open source in 2019, Seata has been around for **5** years, with over **300** contributors in the community. The project has received **24k+** stars and is a mature community. Seata is compatible with **10+** mainstream RPC frameworks and RDBMS, has integration relationships with **20+** communities, and is applied to business systems by **thousands** of customers. It can be considered the de facto standard for distributed transaction solutions.\\n\\n![img](/img/blog/explore-seata-apache.png)\\n\\n### Seata\'s Journey to Apache Incubator\\n\\nOn October 29, 2023, Seata was formally donated to the Apache Software Foundation and became an incubating project. After incubation, Seata is expected to become the first top-level distributed transaction framework project under the Apache Software Foundation. This donation will propel Seata to a broader development scope, profoundly impacting ecosystem construction, and benefiting more developers. This significant milestone also opens up broader development opportunities for Seata.\\n\\n## Development Journey\\n\\n**Having introduced some basic information, the following sections will delve into my development journey in the Seata community.**\\n\\nBefore officially starting development, I undertook several preparatory steps. Given Seata\'s five years of development and the accumulation of hundreds of thousands of lines of code, direct involvement requires a certain learning curve. I share some preparatory experiences in the hope of providing inspiration.\\n\\n1. **Documentation and Blogs as Primary Resources**\\n   - Text materials such as documentation and blogs help newcomers quickly understand project background and code structure.\\n   - Official documentation is the primary reference material, providing insights into everything the official documentation deems necessary to know.\\n   ![img](/img/blog/explore-seata-docs.png)\\n   - Blogs, secondary to official documentation, are often written by developers or advanced users. Blogs may delve deeper into specific topics, such as theoretical models of projects, project structure, and source code analysis of specific modules.\\n   ![img](/img/blog/explore-seata-blogs.png)\\n   - Public accounts (such as WeChat) are similar to blogs, generally containing technical articles. An advantage of public accounts is the ability to subscribe for push notifications, allowing for technical reading during spare time.\\n   ![img](/img/blog/explore-seata-pubs.png)\\n   - Additionally, slides from online or offline community presentations and meetups provide meaningful textual materials.\\n   ![img](/img/blog/explore-seata-slides.png)\\n   - Apart from official materials, many third-party resources are available for learning, such as understanding specific implementations and practices through user-shared use cases, exploring the project\'s ecosystem through integration documentation from third-party communities, and learning through video tutorials. However, among all these materials, I consider official documentation and blogs to be the most helpful.\\n\\n2. **Familiarizing Yourself with the Framework**\\n   - Not all text materials need to be thoroughly read. Understanding is superficial if confined to paper. Practice should commence when you feel you understand enough. The \\"Get Started\\" section in the official documentation is a step-by-step guide to understanding the project\'s basic workflow.\\n   - Another approach is to find examples or demonstrations provided by the official project, build and run them, understand the meanings of code and configurations, and learn about the project\'s requirements, goals, existing features, and architecture through usage.\\n   - For instance, Seata has a repository named \\"seata-samples\\" containing over 20 use cases, covering scenarios like Seata integration with Dubbo, integration with SCA, and Nacos integration. These examples cover almost all supported scenarios.\\n\\n3. **Roughly Reading Source Code to Grasp Main Logic**\\n   - In the preparation phase, roughly reading the source code to grasp the project\'s main logic is crucial. Efficiently understanding a project\'s core content is a skill that requires long-term accumulation.\\n   - First, through the previously mentioned preparation steps, understanding the project\'s concepts, interactions, and process models is helpful.\\n   - Taking Seata as an example, through official documentation and practical operations, you can understand the three roles in Seata\'s transaction domain: TC (Transaction Coordinator), TM (Transaction Manager), and RM (Resource Manager). TC, deployed independently as a server, maintains the state of global and branch transactions, crucial for Seata\'s high availability. TM interacts with TC, defining the start, commit, or rollback of global transactions. RM manages resources for branch transaction processing, interacts with TC to register branch transactions and report branch transaction states, and drives branch transaction commit or rollback. After roughly understanding the interaction between these roles, grasping the project\'s main logic becomes easier.\\n   ![img](/img/solution.png)\\n   - Having a mental impression of these models makes it easier to extract the main logic from the source code. For example, analyzing the Seata TC transaction coordinator, as a server-side application deployed independently of the business, involves starting the server locally and tracking it through the startup class. This analysis can reveal some initialization logic, such as service registration and initialization of global locks. Tracking the code through RPC calls can reveal how TC persists global and branch transactions and how it drives global transaction commit or rollback.\\n   - However, for embedded client framework code without a startup class entry point for analysis, starting with a sample can be effective. Finding references to framework code in a sample allows for code reading. For instance, a crucial annotation in Seata is `GlobalTransaction`, used to identify a global transaction. To understand how TM analyzes this annotation, one can use the IDE\'s search function to find the interceptor for `GlobalTransaction` and analyze its logic.\\n   - Here\'s a tip: Unit tests often focus on the functional aspects of a single module. Reading unit tests can reveal a module\'s input-output, logic boundaries, and understanding the code through the unit test\'s call chain is an essential means of understanding the source code.\\n\\n**With everything prepared, the next step is to actively participate in the community.**\\n\\n## Ways to Contribute and Personal Insights\\n\\nThere are various ways to participate, with one of the most common being to check the project\'s Issues list. Communities often mark issues suitable for new contributors with special labels such as \\"good-first-issue,\\" \\"contributions-welcome,\\" and \\"help-wanted.\\" Interested tasks can be filtered through these labels.\\n\\n![img](/img/blog/explore-seata-issues.png)\\n\\nIn addition to Issues, GitHub provides a discussion feature where you can participate in open discussions and gain new ideas.\\n\\n![img](/img/blog/explore-seata-discussion.png)\\n\\nFurthermore, communities often hold regular meetings, such as weekly or bi-weekly meetings, where you can stay updated on the community\'s latest progress, ask questions, and interact with other community members.\\n\\n## Summary and Insights\\n\\nI initially joined the Seata community through the Open Source Summer Program. I completed my project, implemented new features for Seata Saga, and carried out a series of optimizations. However, I didn\'t stop there. My open-source experience with Seata provided me with the most valuable developer experience in my student career. Over time, I continued to stay active in the community through the aforementioned participation methods. This was mainly due to the following factors:\\n\\n1. **Communication and Networking:** The mentorship system provided crucial support. During development, the close collaboration between my mentor and me played a key role in adapting to community culture and workflow. My mentor not only helped me acclimate to the community but also provided design ideas and shared work-related experiences and insights, all of which were very helpful for my development. Additionally, Seata community founder Ming Cai provided a lot of assistance, including establishing contacts with other students, helping with code reviews, and offering many opportunities.\\n\\n2. **Positive Feedback:** During Seata\'s development, I experienced a virtuous cycle. Many details provided positive feedback, such as my contributions being widely used and beneficial to users, and the recognition of my development efforts by the community. This positive feedback strengthened my desire to continue contributing to the Seata community.\\n\\n3. **Skill Enhancement:** Participating in Seata development greatly enhanced my abilities. Here, I could learn production-level code, including performance optimization, interface design, and techniques for boundary judgment. I could directly participate in the operation of an open-source project, including project planning, scheduling, and communication. Additionally, I gained insights into how a distributed transaction framework is designed and implemented.\\n\\nIn addition to these valuable developer experiences, I gained some personal insights into participating in open source. To inspire other students interested in joining open-source communities, I made a simple summary:\\n\\n1. **Understand and Learn Community Culture and Values:** Every open-source community has different cultures and values. Understanding a community\'s culture and values is crucial for successful participation. Observing and understanding the daily development and communication styles of other community members is a good way to learn community culture. Respect others\' opinions and embrace different viewpoints in the community.\\n\\n2. **Dare to Take the First Step:** Don\'t be afraid of challenges; taking the first step is key to participating in open-source communities. You can start by tackling issues labeled \\"good-first-issue\\" or by contributing to documentation, unit tests, etc. Overcoming the fear of difficulties, actively trying, and learning are crucial.\\n\\n3. **Have Confidence in Your Work:** Don\'t doubt your abilities. Everyone starts from scratch, and no one is born an expert. Participating in open-source communities is a process of learning and growth that requires continuous practice and experience accumulation.\\n\\n4. **Actively Participate in Discussions, Keep Learning Different Technologies:** Don\'t hesitate to ask questions, whether about specific project technologies or challenges in the development process. Also, don\'t limit yourself to one domain. Try to learn and master different programming languages, frameworks, and tools. This broadens your technical perspective and provides valuable insights for the project.\\n\\n---\\n\\nThrough my open-source journey, I accumulated valuable experience and skills. This not only helped me grow into a more valuable developer but also gave me a profound understanding of the power of open-source communities. However, I am not just an individual participant; I represent a part of the Seata community. Seata, as a continuously growing and evolving open-source project, has tremendous potential and faces new challenges. Therefore, I want to emphasize the importance of the Seata community and its future potential. It has entered the incubation stage of the Apache Software Foundation, a significant milestone that will bring broader development opportunities for Seata. Seata welcomes more developers and contributors to join us. Let\'s work together to drive the development of this open-source project and contribute to the advancement of the distributed transaction field."},{"id":"/seata-raft-detailed-explanation","metadata":{"permalink":"/blog/seata-raft-detailed-explanation","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-raft-detailed-explanation.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-raft-detailed-explanation.md","title":"Seata-Raft Storage Mode in Depth and Getting Started","description":"From traditional storage and computing separation to integrated storage and computing relying on distributed consensus algorithms to ensure transaction data consistency under high availability mode, what changes has Seata 2.x made? This article will provide a detailed introduction to the architecture and performance comparison.","date":"2023-10-13T00:00:00.000Z","formattedDate":"October 13, 2023","tags":[],"readingTime":16.765,"hasTruncateMarker":false,"authors":[{"name":"funkye"}],"frontMatter":{"title":"Seata-Raft Storage Mode in Depth and Getting Started","description":"From traditional storage and computing separation to integrated storage and computing relying on distributed consensus algorithms to ensure transaction data consistency under high availability mode, what changes has Seata 2.x made? This article will provide a detailed introduction to the architecture and performance comparison.","keywords":["fescar","seata","distributed transactions","raft"],"author":"funkye","date":"2023/10/13"},"unlisted":false,"prevItem":{"title":"Exploring the Journey of Open Source Development in Seata Project","permalink":"/blog/explore-seata-journey"},"nextItem":{"title":"Seata:Bridging Data and Applications","permalink":"/blog/seata-connect-data-and-application"}},"content":"- [1. Overview](#)\\n- [2. Architecture Introduction](#)\\n- [3. Deployment and Usage](#)\\n- [4. Benchmark Comparison](#)\\n- [5. Conclusion](#)\\n\\n# 1. Overview\\n\\nSeata is an open-source distributed transaction solution with over 24000 stars and a highly active community. It is dedicated to providing high-performance and user-friendly distributed transaction services in microservices architecture.\\n\\nCurrently, Seata\'s distributed transaction data storage modes include file, db, and redis. This article focuses on the architecture, deployment and usage, benchmark comparison of Seata-Server Raft mode. It explores why Seata needs Raft and provides insights into the process from research and comparison to design, implementation, and knowledge accumulation.\\n\\nPresenter: Jianbin  Chen(funkye) github id: [funky-eyes](https://github.com/funky-eyes)\\n\\n# 2. Architecture Introduction\\n\\n## 2.1 What is Raft Mode?\\n\\nFirstly, it is essential to understand what the Raft distributed consensus algorithm is. The following excerpt is a direct quote from the official documentation of sofa-jraft:\\n\\n```\\nRAFT is a novel and easy-to-understand distributed consensus replication protocol proposed by Diego Ongaro and John Ousterhout at Stanford University. It serves as the central coordination component in the RAMCloud project. Raft is a Leader-Based variant of Multi-Paxos, providing a more complete and clear protocol description compared to protocols like Paxos, Zab, View Stamped Replication. It also offers clear descriptions of node addition and deletion. As a replication state machine, Raft is the most fundamental component in distributed systems, ensuring ordered replication and execution of commands among multiple nodes, guaranteeing consistency when the initial states of multiple nodes are consistent.\\n\\nIn summary, Seata\'s Raft mode is based on the Sofa-Jraft component, implementing the ability to ensure the data consistency and high availability of Seata-Server itself.\\n\\n```\\n## 2.2 Why Raft Mode is Needed\\n\\nAfter understanding the definition of Seata-Raft mode, you might wonder whether Seata-Server is now unable to ensure consistency and high availability. Let\'s explore how Seata-Server currently achieves this from the perspectives of consistency and high availability.\\n\\n### 2.2.1 Existing Storage Modes\\n\\nIn the current Seata design, the role of the Server is to ensure the correct execution of the two-phase commit for transactions. However, this depends on the correct storage of transaction records. To ensure that transaction records are not lost, it is necessary to drive all Seata-RM instances to perform the correct two-phase commit behavior while maintaining correct state. So, how does Seata currently store transaction states and records?\\n\\nFirstly, let\'s introduce the three transaction storage modes supported by Seata: file, db, and redis. In terms of consistency ranking, the db mode provides the best guarantee for transaction records, followed by the asynchronous flushing of the file mode, and finally the aof and rdb modes of redis.\\n\\nTo elaborate:\\n\\n- The file mode is Seata\'s self-implemented transaction storage method. It stores transaction information on the local disk in a sequential write manner. For performance considerations, it defaults to asynchronous mode and stores transaction information in memory to ensure consistency between memory and disk data. In the event of Seata-Server (TC) unexpected crash, it reads transaction information from the disk upon restarting and restores it to memory for the continuation of transaction contexts.\\n\\n- The db mode is another implementation of Seata\'s abstract transaction storage manager (AbstractTransactionStoreManager). It relies on databases such as PostgreSQL, MySQL, Oracle, etc., to perform transaction information operations. Consistency is guaranteed by the local transactions of the database, and data persistence is the responsibility of the database.\\n\\n- Redis, similar to db, is a transaction storage method using Jedis and Lua scripts. It performs transaction operations using Lua scripts, and in Seata 2.x, all operations (such as lock competition) are handled using Lua scripts. Data storage is similar to db, relying on the storage side (Redis) to ensure data consistency. Like db, redis adopts a computation and storage separation architecture design in Seata.\\n\\n\\n### 2.2.2 High Availability\\n\\nHigh availability is simply the ability of a cluster to continue running normally after the main node crashes. The common approach is to deploy multiple nodes providing the same service and use a registry center to real-time sense the online and offline status of the main node for timely switching to an available node.\\n\\nIt may seem that deploying a few more machines is all that\'s needed. However, there is a problem behind it \u2013 how to ensure that multiple nodes operate as a whole. If one node crashes, another node can seamlessly take over the work of the crashed node, including handling the data of the crashed node. The answer to solving this problem is simple: in a computation and storage separation architecture, store data in a shared middleware. Any node can access this shared storage area to obtain transaction information for all nodes\' operations, thus achieving high availability.\\n\\nHowever, the prerequisite is that computation and storage must be separated. Why is the integration of computation and storage not feasible? This brings us to the implementation of the File mode. As described earlier, the File mode stores data on local disks and node memory, with no synchronization in data writing operations. This means that the current File mode cannot achieve high availability and only supports single-machine deployment. For basic quick start and simple use, the File mode has lower applicability, and the high-performance, memory-based File mode is practically no longer used in production environments.\\n\\n## 2.3 How is Seata-Raft Designed?\\n\\n### 2.3.1 Design Principles\\n\\nThe design philosophy of Seata-Raft mode is to encapsulate the File mode, which is unable to achieve high availability, and use the Raft algorithm to synchronize data between multiple TCs. This mode ensures data consistency among multiple TCs when using the File mode and replaces asynchronous flushing operations with Raft logs and snapshots for data recovery.\\n\\n![flow](/img/blog/Dingtalk_20230105203431.jpg)\\n\\nIn the Seata-Raft mode, the client-side, upon startup, retrieves its transaction group (e.g., default) and the IP addresses of relevant Raft cluster nodes from the configuration center. By sending a request to the control port of Seata-Server, the client can obtain metadata for the Raft cluster corresponding to the default group, including leader, follower, and learner member nodes. Subsequently, the client monitors (watches) any member nodes of non-leader nodes.\\n\\nAssuming that TM initiates a transaction, and the leader node in the local metadata points to the address of TC1, TM will only interact with TC1. When TC1 adds global transaction information, through the Raft protocol, denoted as step 1 in the diagram, TC1 sends the log to other nodes. Step 2 represents the response of follower nodes to log reception. When more than half of the nodes (such as TC2) accept and respond successfully, the state machine (FSM) on TC1 will execute the action of adding a global transaction.\\n\\n![watch](/img/blog/Dingtalk_20230105204423.jpg)\\n![watch2](/img/blog/Dingtalk_20230105211035.jpg)\\n\\nIf TC1 crashes or a reelection occurs, what happens? Since the metadata has been obtained during the initial startup, the client will execute the watch follower node\'s interface to update the local metadata information. Therefore, subsequent transaction requests will be sent to the new leader (e.g., TC2). Meanwhile, TC1\'s data has already been synchronized to TC2 and TC3, ensuring data consistency. Only at the moment of the election, if a transaction happens to be sent to the old leader, it will be actively rolled back to ensure data correctness.\\n\\nIt is important to note that in this mode, if a transaction is in the phase of sending resolution requests or the one-phase process has not yet completed at the moment of the election, and it happens exactly during the election, these transactions will be actively rolled back. This is because the RPC node has crashed or a reelection has occurred, and there is currently no implemented RPC retry. The TM side has a default retry mechanism of 5 times, but due to the approximately 1s-2s time required for the election, transactions in the \'begin\' state may not successfully resolve, so they are prioritized for rollback to release locks, avoiding impacting the correctness of other business.\\n\\n### 2.3.2 Fault Recovery\\n\\nIn Seata, when a TC experiences a failure, the data recovery process is as follows:\\n\\n![recover](/img/blog/Dingtalk_20230106231817.jpg)\\n\\nAs shown in the above diagram:\\n\\n- Check for the Latest Data Snapshot: Firstly, the system checks for the existence of the latest data snapshot file. The data snapshot is a one-time full copy of the in-memory data state. If there is a recent data snapshot, the system directly loads it into memory.\\n\\n- Replay Based on Raft Logs After Snapshot: If there is the latest snapshot or no snapshot file, the system replays the data based on the previously recorded Raft logs. Each request in Seata-Server ultimately goes through the ServerOnRequestProcessor for processing, then moves to the specific coordinator class (DefaultCoordinator or RaftCoordinator), and further proceeds to the specific business code (DefaultCore) for the corresponding transaction processing (e.g., begin, commit, rollback).\\n\\n- After the log replay is complete, the leader initiates log synchronization and continues to execute the related transaction\'s add, delete, and modify actions.\\n\\nThrough these steps, Seata can achieve data recovery after a failure. It first attempts to load the latest snapshot, if available, to reduce replay time. Then, it replays based on Raft logs to ensure the consistency of data operations. Finally, through the log synchronization mechanism, it ensures data consistency among multiple nodes.\\n\\n### 2.3.3 Business Processing Synchronization Process\\n\\n![flow](/img/blog/Dingtalk_20230106230931.jpg)\\nFor the case where the client side is obtaining the latest metadata while a business thread is executing operations such as begin, commit, or registry, Seata adopts the following handling:\\n\\n- On the client side:\\n\\n    - If the client is executing operations like begin, commit, or registry, and at this moment, it needs to obtain the latest metadata, the RPC request from the client might fail since the leader may no longer exist or is not the current leader. \\n    - If the request fails, the client receives an exception response, and in this case, the client needs to roll back based on the request result.\\n\\n- TC side for detecting the old leader:\\n\\n    - On the TC side, if the client\'s request reaches the old leader node, TC checks if it is the current leader. If it is not the leader, it rejects the request.\\n    - If it is the leader but fails midway, such as failing during the process of submitting a task to the state machine, the creation of the task (createTask) fails due to the current state not being the leader. In this case, the client also receives a response with an exception.\\n    - The old leader\'s task submission also fails, ensuring the consistency of transaction information.\\n\\nThrough the above handling, when the client obtains the latest metadata while a business operation is in progress, Seata ensures data consistency and transaction correctness. If the client\'s RPC request fails, it triggers a rollback operation. On the TC side, detection of the old leader and the failure of task submission prevent inconsistencies in transaction information. This way, the client\'s data can also maintain consistency.\\n\\n## 3. Usage and Deployment\\nIn terms of usage and deployment, the community adheres to the principles of minimal intrusion and minimal changes. Therefore, the overall deployment should be straightforward. The following sections introduce deployment changes separately for the client and server sides.\\n\\n### 3.1 Client\\nFirstly, those familiar with the use of registry configuration centers should be aware of the `seata.registry.type` configuration item in Seata\'s configuration, supporting options like Nacos, ZooKeeper, etcd, Redis, etc. After version 2.0, a configuration item for Raft was added.\\n\\n```\\n   registry:\\n      type: raft\\n      raft:\\n         server-addr: 192.168.0.111:7091, 192.168.0.112:7091, 192.168.0.113:7091\\n```\\nSwitch the `registry.type` to \'raft\' and configure the address for obtaining Raft-related metadata, which is unified as the IP of the seata-server + HTTP port. Then, it is essential to configure the traditional transaction group.\\n\\n```\\nseata:\\n   tx-service-group: default_tx_group\\n   service:\\n      vgroup-mapping:\\n         default_tx_group: default\\n```\\nIf the current transaction group used is `default_tx_group`, then the corresponding Seata cluster/group is \'default\'. There is a corresponding relationship, and this will be further explained in the server deployment section.\\nWith this, the changes on the client side are complete.\\n\\n### 3.2 Server\\nFor server-side changes, there might be more adjustments, involving familiarity with some tuning parameters and configurations. Of course, default values can be used without any modifications.\\n\\n```\\nseata:\\n  server:\\n    raft:\\n      group: default # This value represents the group of this raft cluster, and the value corresponding to the client\'s transaction group should match it.\\n      server-addr: 192.168.0.111:9091,192.168.0.112:9091,192.168.0.113:9091 # IP and port of the 3 nodes, the port is the netty port of the node + 1000, default netty port is 8091\\n      snapshot-interval: 600 # Take a snapshot every 600 seconds for fast rolling of raftlog. However, making a snapshot every 600 seconds may cause business response time jitter if there is too much transaction data in memory. But it is friendly for fault recovery and faster node restart. You can adjust it to 30 minutes, 1 hour, etc., according to the business. You can test whether there is jitter on your own, and find a balance point between rt jitter and fault recovery.\\n      apply-batch: 32 # At most, submit raftlog once for 32 batches of actions\\n      max-append-bufferSize: 262144 # Maximum size of the log storage buffer, default is 256K\\n      max-replicator-inflight-msgs: 256 # In the case of enabling pipeline requests, the maximum number of in-flight requests, default is 256\\n      disruptor-buffer-size: 16384 # Internal disruptor buffer size. If it is a scenario with high write throughput, you need to appropriately increase this value. Default is 16384\\n      election-timeout-ms: 1000 # How long without a leader\'s heartbeat to start a new election\\n      reporter-enabled: false # Whether the monitoring of raft itself is enabled\\n      reporter-initial-delay: 60 # Interval of monitoring\\n      serialization: jackson # Serialization method, do not change\\n      compressor: none # Compression method for raftlog, such as gzip, zstd, etc.\\n      sync: true # Flushing method for raft log, default is synchronous flushing\\n  config:\\n    # support: nacos, consul, apollo, zk, etcd3\\n    type: file # This configuration can choose different configuration centers\\n  registry:\\n    # support: nacos, eureka, redis, zk, consul, etcd3, sofa\\n    type: file # Non-file registration center is not allowed in raft mode\\n  store:\\n    # support: file, db, redis, raft\\n    mode: raft # Use raft storage mode\\n    file:\\n      dir: sessionStore # This path is the storage location of raftlog and related transaction logs, default is relative path, it is better to set a fixed location\\n```\\nIn 3 or more nodes of seata-server, after configuring the above parameters, you can directly start it, and you will see similar log output, which means the cluster has started successfully:\\n\\n```\\n2023-10-13 17:20:06.392  WARN --- [Rpc-netty-server-worker-10-thread-1] [com.alipay.sofa.jraft.rpc.impl.BoltRaftRpcFactory] [ensurePipeline] []: JRaft SET bolt.rpc.dispatch-msg-list-in-default-executor to be false for replicator pipeline optimistic.\\n2023-10-13 17:20:06.439  INFO --- [default/PeerPair[10.58.16.231:9091 -> 10.58.12.217:9091]-AppendEntriesThread0] [com.alipay.sofa.jraft.storage.impl.LocalRaftMetaStorage] [save] []: Save raft meta, path=sessionStore/raft/9091/default/raft_meta, term=4, votedFor=0.0.0.0:0, cost time=25 ms\\n2023-10-13 17:20:06.441  WARN --- [default/PeerPair[10.58.16.231:9091 -> 10.58.12.217:9091]-AppendEntriesThread0] [com.alipay.sofa.jraft.core.NodeImpl] [handleAppendEntriesRequest] []: Node <default/10.58.16.231:9091> reject term_unmatched AppendEntriesRequest from 10.58.12.217:9091, term=4, prevLogIndex=4, prevLogTerm=4, localPrevLogTerm=0, lastLogIndex=0, entriesSize=0.\\n2023-10-13 17:20:06.442  INFO --- [JRaft-FSMCaller-Disruptor-0] [io.seata.server.cluster.raft.RaftStateMachine] [onStartFollowing] []: groupId: default, onStartFollowing: LeaderChangeContext [leaderId=10.58.12.217:9091, term=4, status=Status[ENEWLEADER<10011>: Raft node receives message from new leader with higher term.]].\\n2023-10-13 17:20:06.449  WARN --- [default/PeerPair[10.58.16.231:9091 -> 10.58.12.217:9091]-AppendEntriesThread0] [com.alipay.sofa.jraft.core.NodeImpl] [handleAppendEntriesRequest] []: Node <default/10.58.16.231:9091> reject term_unmatched AppendEntriesRequest from 10.58.12.217:9091, term=4, prevLogIndex=4, prevLogTerm=4, localPrevLogTerm=0, lastLogIndex=0, entriesSize=0.\\n2023-10-13 17:20:06.459  INFO --- [Bolt-default-executor-4-thread-1] [com.alipay.sofa.jraft.core.NodeImpl] [handleInstallSnapshot] []: Node <default/10.58.16.231:9091> received InstallSnapshotRequest from 10.58.12.217:9091, lastIncludedLogIndex=4, lastIncludedLogTerm=4, lastLogId=LogId [index=0, term=0].\\n2023-10-13 17:20:06.489  INFO --- [Bolt-conn-event-executor-13-thread-1] [com.alipay.sofa.jraft.rpc.impl.core.ClientServiceConnectionEventProcessor] [onEvent] []: Peer 10.58.12.217:9091 is connected\\n2023-10-13 17:20:06.519  INFO --- [JRaft-Group-Default-Executor-0] [com.alipay.sofa.jraft.util.Recyclers] [<clinit>] []: -Djraft.recyclers.maxCapacityPerThread: 4096.\\n2023-10-13 17:20:06.574  INFO --- [JRaft-Group-Default-Executor-0] [com.alipay.sofa.jraft.storage.snapshot.local.LocalSnapshotStorage] [destroySnapshot] []: Deleting snapshot sessionStore/raft/9091/default/snapshot/snapshot_4.\\n2023-10-13 17:20:06.574  INFO --- [JRaft-Group-Default-Executor-0] [com.alipay.sofa.jraft.storage.snapshot.local.LocalSnapshotStorage] [close] []: Renaming sessionStore/raft/9091/default/snapshot/temp to sessionStore/raft/9091/default/snapshot/snapshot_4.\\n2023-10-13 17:20:06.689  INFO --- [JRaft-FSMCaller-Disruptor-0] [io.seata.server.cluster.raft.snapshot.session.SessionSnapshotFile] [load] []: on snapshot load start index: 4\\n2023-10-13 17:20:06.694  INFO --- [JRaft-FSMCaller-Disruptor-0] [io.seata.server.cluster.raft.snapshot.session.SessionSnapshotFile] [load] []: on snapshot load end index: 4\\n2023-10-13 17:20:06.694  INFO --- [JRaft-FSMCaller-Disruptor-0] [io.seata.server.cluster.raft.RaftStateMachine] [onSnapshotLoad] []: groupId: default, onSnapshotLoad cost: 110 ms.\\n2023-10-13 17:20:06.694  INFO --- [JRaft-FSMCaller-Disruptor-0] [io.seata.server.cluster.raft.RaftStateMachine] [onConfigurationCommitted] []: groupId: default, onConfigurationCommitted: 10.58.12.165:9091,10.58.12.217:9091,10.58.16.231:9091.\\n2023-10-13 17:20:06.705  INFO --- [JRaft-FSMCaller-Disruptor-0] [com.alipay.sofa.jraft.storage.snapshot.SnapshotExecutorImpl] [onSnapshotLoadDone] []: Node <default/10.58.16.231:9091> onSnapshotLoadDone, last_included_index: 4\\nlast_included_term: 4\\npeers: \\"10.58.12.165:9091\\"\\npeers: \\"10.58.12.217:9091\\"\\npeers: \\"10.58.16.231:9091\\"\\n\\n2023-10-13 17:20:06.722  INFO --- [JRaft-Group-Default-Executor-1] [com.alipay.sofa.jraft.storage.impl.RocksDBLogStorage] [lambda$truncatePrefixInBackground$2] []: Truncated prefix logs in data path: sessionStore/raft/9091/default/log from log index 1 to 5, cost 0 ms.\\n```\\n### 3.3 faq\\n- Once the `seata.raft.server-addr` is configured, cluster scaling or shrinking must be done through the server\'s openapi. Directly changing this configuration and restarting won\'t take effect. The API for this operation is `/metadata/v1/changeCluster?raftClusterStr=new_cluster_list`.\\n\\n- If the addresses in `server-addr:` are all on the local machine, you need to add a 1000 offset to the netty ports of different servers on the local machine. For example, if `server.port: 7092`, the netty port will be 8092, and the raft election and communication port will be 9092. You need to add the startup parameter `-Dserver.raftPort=9092`. On Linux, this can be specified using `export JAVA_OPT=\\"-Dserver.raftPort=9092\\"`.\\n\\n\\n## 4. Performance Test Comparison\\n\\nPerformance testing is divided into two scenarios. To avoid data hotspots and thread optimization, the client side initializes 3 million items and uses jdk21 virtual threads + Spring Boot3 + Seata AT for testing. Garbage collection is handled with the ZGC generational garbage collector. The testing tool used is Alibaba Cloud PTS. Server-side is uniformly configured with jdk21 (not yet adapted for virtual threads). Server configurations are as follows:\\n- TC: 4c8g * 3\\n- Client: 4c * 8G * 1\\n- Database: Alibaba Cloud RDS 4c16g\\n\\n- 64 concurrent performance test only increases the performance of the `@GlobalTransactional` annotated interface with empty submissions.\\n- Random 3 million data items are used for inventory deduction in a 32 concurrent scenario for 10 minutes.\\n\\n### 4.1 1.7.1 db mode\\n![raft pressure test model](https://img.alicdn.com/imgextra/i3/O1CN011dNh3H1UK8G5prQAg_!!6000000002498-0-tps-731-333.jpg)\\n\\n#### Empty submission 64C\\n![db64-2](https://img.alicdn.com/imgextra/i2/O1CN01pE1Anf1nRtgcnlx9t_!!6000000005087-0-tps-622-852.jpg)\\n\\n#### Random inventory deduction 32C\\n![db32-2](https://img.alicdn.com/imgextra/i2/O1CN016hZkJC20OJax9ce31_!!6000000006839-0-tps-624-852.jpg)\\n\\n### 4.2 2.0 raft mode\\n![raft pressure test model](https://img.alicdn.com/imgextra/i2/O1CN01nNL6oe1X95YcQQEjs_!!6000000002880-0-tps-773-353.jpg)\\n\\n#### Empty submission 64C\\n![raft64-2](https://img.alicdn.com/imgextra/i1/O1CN01rs1ykr1dhnH8qnXj3_!!6000000003768-0-tps-631-851.jpg)\\n\\n#### Random inventory deduction 32C\\n![raft32c-2](https://img.alicdn.com/imgextra/i4/O1CN015OwA2k20enquV7Yfu_!!6000000006875-0-tps-624-856.jpg)\\n\\n### 4.3 Test Result Comparison\\n32 concurrent random inventory deduction scenario with 3 million items\\n\\n| tps avg | tps max | count | rt | error | Storage Type |\\n| --- | --- | --- | --- | --- | --- |\\n| 1709 (42%\u2191) | 2019 (21%\u2191) | 1228803 (42%\u2191) | 13.86ms (30%\u2193) | 0 | Raft |\\n| 1201 | 1668 | 864105 | 19.86ms | 0 | DB |\\n\\n64 concurrent empty pressure on `@GlobalTransactional` interface (test peak limit is 8000)\\n\\n| tps avg | tps max | count | rt | error | Storage Type |\\n| --- | --- | --- | --- | --- | --- |\\n| 5704 (20%\u2191) | 8062 (30%\u2191) | 4101236 (20%\u2191) | 7.79ms (19%\u2193) | 0 | Raft |\\n| 4743 | 6172 | 3410240 | 9.65ms | 0 | DB |\\n\\nIn addition to the direct comparison of the above data, by observing the curves of the pressure test, it can be seen that under the raft mode, TPS and RT are more stable, with less jitter, and better performance and throughput.\\n\\n\\n## 5. Summary\\n\\nIn the future development of Seata, performance, entry threshold, and deployment and operation costs are directions that we need to pay attention to and continuously optimize. After the introduction of the raft mode, Seata has the following characteristics:\\n\\n1. In terms of storage, after the separation of storage and computation, Seata\'s upper limit for optimization has been raised, making it more self-controlled.\\n2. Lower deployment costs, no need for additional registration centers, storage middleware.\\n3. Lower entry threshold, no need to learn other knowledge such as registration centers; one can directly use Seata Raft.\\n\\nIn response to industry trends, some open-source projects such as ClickHouse and Kafka have started to abandon the use of ZooKeeper and instead adopt self-developed solutions, such as ClickKeeper and KRaft. These solutions ensure the storage of metadata and other information by themselves, reducing the need for third-party dependencies, thus reducing operational and learning costs. These features are mature and worth learning from.\\n\\nOf course, currently, solutions based on the Raft mode may not be mature enough and may not fully meet the beautiful descriptions above. However, precisely because of such theoretical foundations, the community should strive in this direction, gradually bringing practice closer to the theoretical requirements. Here, all students interested in Seata are welcome to join the community, contributing to the development of Seata!"},{"id":"/seata-connect-data-and-application","metadata":{"permalink":"/blog/seata-connect-data-and-application","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-connect-data-and-application.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-connect-data-and-application.md","title":"Seata:Bridging Data and Applications","description":"This article introduces the past, present, and future evolution of Seata.","date":"2023-06-30T00:00:00.000Z","formattedDate":"June 30, 2023","tags":[],"readingTime":13.905,"hasTruncateMarker":false,"authors":[{"name":"Ji Min - Founder of the Seata Open Source Community, Leader of the Distributed Transactions Team"}],"frontMatter":{"title":"Seata:Bridging Data and Applications","keywords":["Seata","Distributed Transactions","Data Consistency","Microservices"],"description":"This article introduces the past, present, and future evolution of Seata.","author":"Ji Min - Founder of the Seata Open Source Community, Leader of the Distributed Transactions Team","date":"June 30, 2023"},"unlisted":false,"prevItem":{"title":"Seata-Raft Storage Mode in Depth and Getting Started","permalink":"/blog/seata-raft-detailed-explanation"},"nextItem":{"title":"Observability Practices in Seata","permalink":"/blog/seata-observable-practice"}},"content":"This article mainly introduces the evolutionary journey of distributed transactions from internal development to commercialization and open source, as well as the current progress and future planning of the Seata community.\\nSeata is an open-source distributed transaction solution designed to provide a comprehensive solution for distributed transactions under modern microservices architecture. Seata offers complete distributed transaction solutions, including AT, TCC, Saga, and XA transaction modes, supporting various programming languages and data storage schemes. Seata also provides easy-to-use APIs, extensive documentation, and examples to facilitate quick development and deployment for enterprises applying Seata.\\n**Seata\'s advantages lie in its high availability, high performance, and high scalability, and it does not require extra complex operations for horizontal scaling.** Seata is currently used in thousands of customer business systems on Alibaba Cloud, and its reliability has been recognized and applied by major industry manufacturers.\\nAs an open-source project, the Seata community is also expanding continuously, becoming an important platform for developers to exchange, share, and learn, attracting more and more attention and support from enterprises.\\nToday, I will primarily share about Seata on the following three topics:\\n- **From TXC/GTS to Seata**\\n- **Latest developments in the Seata community**\\n- **Future planning for the Seata community**\\n  <br/>\\n### From TXC/GTS to Seata\\n#### The Origin of Distributed Transactions\\n![Product Matrix](/img/blog/\u4ea7\u54c1\u77e9\u9635.jpg)\\nSeata is internally codenamed TXC (taobao transaction constructor) within Alibaba, a name with a strong organizational structure flavor. TXC originated from Alibaba\'s Wushi (Five Color Stones) project, which in ancient mythology were the stones used by the goddess N\xfcwa to mend the heavens, symbolizing Alibaba\'s important milestone in the evolution from monolithic architecture to distributed architecture. During this project, a batch of epoch-making Internet middleware was developed, including the well-known \\"Big Three\\":\\n- **HSF service invocation framework**\\n  Solves service communication issues after the transition from monolithic applications to service-oriented architectures.\\n- **TDDL database sharding framework**\\n  Addresses storage capacity and connection count issues of databases at scale.\\n- **MetaQ messaging framework**\\n  Addresses asynchronous invocation issues.\\n  The birth of the Big Three satisfied the basic requirements of microservices-based business development, but the data consistency issues that arose after microservices were not properly addressed, lacking a unified solution. The likelihood of data consistency issues in microservices is much higher than in monolithic applications, and the increased complexity of moving from in-process calls to network calls exacerbates the production of exceptional scenarios. The increase in service hops also makes it impossible for upstream and downstream services to coordinate data rollback in the event of a business processing exception. TXC was born to address the pain points of data consistency at the application architecture layer, and the core data consistency scenarios it aimed to address included:\\n- **Consistency across services.** Coordinates rollback of upstream and downstream service nodes in the event of system exceptions such as call timeouts and business exceptions.\\n- **Data consistency in database sharding.** Ensures internal transactions during logical SQL operations on business layers are consistent across different data shards.\\n- **Data consistency in message sending.** Addresses the inconsistency between data operations and successful message sending.\\n  To overcome the common scenarios encountered, TXC was seamlessly integrated with the Big Three. When businesses use the Big Three for development, they are completely unaware of TXC\'s presence in the background, do not have to consider the design of data consistency, and leave it to the framework to ensure, allowing businesses to focus more on their own development, greatly improving development efficiency.\\n  <br/>\\n  ![GTS Architecture](/img/blog/GTS\u67b6\u6784.jpg)\\n  TXC has been widely used within Alibaba Group for many years and has been baptized by the surging traffic of large-scale events like Singles\' Day, significantly improving business development efficiency and ensuring data accuracy, eliminating financial and reputational issues caused by data inconsistencies. With the continuous evolution of the architecture, **a standard three-node cluster can now handle peak values of nearly 100K TPS and millisecond-level transaction processing. In terms of availability and performance, it has reached a four-nines SLA guarantee, ensuring no failures throughout the year even in unattended conditions.**\\n  <br/>\\n#### The Evolution of Distributed Transactions\\nThe birth of new things is always accompanied by doubts. Is middleware capable of ensuring data consistency reliable? The initial birth of TXC was just a vague theory, lacking theoretical models and engineering practice. After we conducted MVP (Minimum Viable Product) model testing and promoted business deployment, we often encountered faults and frequently had to wake up in the middle of the night to deal with issues, wearing wristbands to sleep to cope with emergency responses. These were the most painful years I went through technically after taking over the team.\\n![Evolution of Distributed Transactions](/img/blog/\u5206\u5e03\u5f0f\u4e8b\u52a1\u6f14\u8fdb.jpg)\\nSubsequently, we had extensive discussions and systematic reviews. We first needed to define the consistency problem. Were we to achieve majority consensus consistency like RAFT, solve database consistency issues like Google Spanner, or something else? Looking at the top-down layered structure from the application node, it mainly includes development frameworks, service invocation frameworks, data middleware, database drivers, and databases. We had to decide at which layer to solve the data consistency problem. We compared the consistency requirements, universality, implementation complexity, and business integration costs faced when solving data consistency issues at different levels. In the end, we weighed the pros and cons, decided to keep the implementation complexity to ourselves, and adopted the AT mode initially as a consistency component. We needed to ensure high consistency, but not be locked into specific database implementations, ensuring the generality of scenarios and the business integration costs were low enough to be easily implemented. This is also why TXC initially adopted the AT mode.\\n**A distributed transaction is not just a framework; it\'s a system.** We defined the consistency problem in theory, abstractly conceptualized modes, roles, actions, and isolation, etc. From an engineering practice perspective, we defined the programming model, including low-intrusion annotations, simple method templates, and flexible APIs, and defined basic and enhanced transaction capabilities (e.g., how to support a large number of activities at low cost), as well as capabilities in operations, security, performance, observability, and high availability.\\n![Transaction Logical Model](/img/blog/\u4e8b\u52a1\u903b\u8f91\u6a21\u578b.jpg)\\nWhat problems do distributed transactions solve? A classic and tangible example is the money transfer scenario. The transfer process includes subtracting balance and adding balance, how do we ensure the atomicity of the operation? Without any intervention, these two steps may encounter various problems, such as account B being canceled or service call timeouts, etc.\\n**Timeout issues have always been a difficult problem to solve in distributed applications**; we cannot accurately know whether service B has executed and in what order. From a data perspective, this means the money in account B may not be successfully added. After the service-oriented transformation, each node only has partial information, while the transaction itself requires global coordination of all nodes, thus requiring a centralized role with a god\'s-eye view, capable of obtaining all information, which is the **TC (transaction coordinator)**, used to globally coordinate the transaction state. The **TM (Transaction Manager)** is the role that drives the generation of transaction proposals. However, even gods nod off, and their judgments are not always correct, so we need an **RM (resource manager)** role to verify the authenticity of the transaction as a representative of the soul. This is TXC\'s most basic philosophical model. We have methodologically verified that its data consistency is very complete, of course, our cognition is bounded. Perhaps the future will prove we were turkey engineers, but under current circumstances, its model is already sufficient to solve most existing problems.\\n![Distributed Transaction Performance](/img/blog/\u5206\u5e03\u5f0f\u4e8b\u52a1\u6027\u80fd.jpg)\\n**After years of architectural evolution, from the perspective of transaction single-link latency, TXC takes an average of about 0.2 milliseconds to process at the start of the transaction and about 0.4 milliseconds for branch registration, with the entire transaction\'s additional latency within the millisecond range. This is also the theoretical limit value we have calculated. In terms of throughput, the TPS of a single node reaches 30,000 times/second, and the TPS of a standard cluster is close to 100,000 times/second.**\\n<br/>\\n#### Seata Open Source\\nWhy go open source? This is a question many people have asked me. In 2017, we commercialized the GTS (Global Transaction Service) product sold on Alibaba Cloud, with both public and private cloud forms. At this time, the internal group developed smoothly, but we encountered various problems in the process of commercialization. The problems can be summed up in two main categories: **First, developers are quite lacking in the theory of distributed transactions,** most people do not even understand what local transactions are, let alone distributed transactions. **Second, there are problems with product maturity,** often encountering various strange scenario issues, leading to a sharp rise in support and delivery costs, and R&D turning into after-sales customer service.\\nWe reflected on why we encountered so many problems. The main issue here is that Alibaba Group internally has a unified language stack and unified technology stack, and our polishing of specific scenarios is very mature. Serving Alibaba, one company, and serving thousands of enterprises on the cloud is fundamentally different, which also made us realize that our product\'s scenario ecology was not well developed. On GitHub, more than 80% of open-source software is basic software, and basic software primarily solves the problem of scenario universality, so it cannot be locked in by a single enterprise, like Linux, which has a large number of community distributions. Therefore, in order to make our product better, we chose to open source and co-build with developers to popularize more enterprise users.\\n![Alibaba Open Source](/img/blog/\u963f\u91cc\u5f00\u6e90.jpg)\\nAlibaba\'s open-source journey has gone through three main stages. **The first stage is the stage where Dubbo is located, where developers contribute out of love,** Dubbo has been open sourced for over 10 years, and time has fully proven that Dubbo is an excellent open-source software, and its microkernel plugin extensibility design is an important reference for me when I initially open sourced Seata. When designing software, we need to consider which is more important between extensibility and performance, whether we are doing a three-year design, a five-year design, or a ten-year design that meets business development. While solving the 0-1 service call problem, can we predict the governance problems after the 1-100 scale-up?\\n**The second stage is the closed loop of open source and commercialization, where commercialization feeds back into the open-source community, promoting the development of the open-source community.** I think cloud manufacturers are more likely to do open source well for the following reasons:\\n- First, the cloud is a scaled economy, which must be established on a stable and mature kernel foundation, packaging its product capabilities including high availability, maintenance-free, and elasticity on top of it. An unstable kernel will inevitably lead to excessive delivery and support costs, and high penetration of the R&D team\'s support Q&A will prevent large-scale replication, and high penetration rates will prevent rapid evolution and iteration of products.\\n- Second, commercial products know business needs better. Our internal technical teams often YY requirements from a development perspective, and what they make is not used by anyone, and thus does not form a value conversion. The business requirements collected through commercialization are all real, so its open source kernel must also evolve in this direction. Failure to evolve in this direction will inevitably lead to architectural splits on both sides, increasing the team\'s maintenance costs.\\n- Finally, the closed loop of open source and commercialization can promote better development of both parties. If the open-source kernel often has various problems, would you believe that its commercial product is good enough?\\n  **The third stage is systematization and standardization.** First, systematization is the basis of open-source solutions. Alibaba\'s open-source projects are mostly born out of internal e-commerce scenario practices. For example, Higress is used to connect Ant Group\'s gateways; Nacos carries services with millions of instances and tens of millions of connections; Sentinel provides degradation and throttling capabilities for high availability during major promotions; and Seata ensures transaction data consistency. This set of systematized open-source solutions is designed based on the best practices of Alibaba\'s e-commerce ecosystem. Second, standardization is another important feature. Taking OpenSergo as an example, it is both a standard and an implementation. In the past few years, the number of domestic open-source projects has exploded. However, the capabilities of various open-source products vary greatly, and many compatibility issues arise when integrating with each other. Therefore, open-source projects like OpenSergo can define some standardized capabilities and interfaces and provide some implementations, which will greatly help the development of the entire open-source ecosystem.\\n  <br/>\\n### Latest Developments in the Seata Community\\n#### Introduction to the Seata Community\\n![Community Introduction](/img/blog/\u793e\u533a\u7b80\u4ecb.jpg)\\n**At present, Seata has open-sourced 4 transaction modes, including AT, TCC, Saga, and XA, and is actively exploring other viable transaction solutions.** Seata has integrated with more than 10 mainstream RPC frameworks and relational databases, and has integrated or been integrated relationships with more than 20 communities. In addition, we are also exploring languages other than Java in the multi-language system, such as Golang, PHP, Python, and JS.\\nSeata has been applied to business systems by thousands of customers. Seata applications have become more mature, with successful cooperation with the community in the financial business scenarios of CITIC Bank and Everbright Bank, and successfully adopted into core accounting systems. The landing of microservices systems in financial scenarios is very stringent, which also marks a new level of maturity for Seata\'s kernel.\\n<br/>\\n#### Seata Ecosystem Expansion\\n![Ecosystem Expansion](/img/blog/\u6269\u5c55\u751f\u6001.jpg)\\n**Seata adopts a microkernel and plugin architecture design, exposing rich extension points in APIs, registry configuration centers, storage modes, lock control, SQL parsers, load balancing, transport, protocol encoding and decoding, observability, and more.** This allows businesses to easily perform flexible extensions and select technical components.\\n<br/>\\n#### Seata Application Cases\\n![Application Cases](/img/blog/\u5e94\u7528\u6848\u4f8b.jpg)\\n**Case 1: China Aviation Information\'s Air Travel Project**\\nThe China Aviation Information Air Travel project introduced Seata in the 0.2 version to solve the data consistency problem of ticket and coupon business, greatly improving development efficiency, reducing asset losses caused by data inconsistency, and enhancing user interaction experience.\\n**Case 2: Didi Chuxing\'s Two-Wheeler Business Unit**\\nDidi Chuxing\'s Two-Wheeler Business Unit introduced Seata in version 0.6.1, solving the data consistency problem of business processes such as blue bicycles, electric vehicles, and assets, optimizing the user experience, and reducing asset loss.\\n**Case 3: Meituan\'s Infrastructure**\\nMeituan\'s infrastructure team developed the internal distributed transaction solution Swan based on the open-source Seata project, which is used to solve distributed transaction problems within Meituan\'s various businesses.\\n**Case 4: Hema Town**\\nHema Town uses Seata to control the flower-stealing process in game interactions, significantly shortening the development cycle from 20 days to 5 days, effectively reducing development costs.\\n<br/>\\n#### Evolution of Seata Transaction Modes\\n![Mode Evolution](/img/blog/\u6a21\u5f0f\u6f14\u8fdb.jpg)\\n<br/>\\n#### Current Progress of Seata\\n- Support for Oracle and PostgreSQL multi-primary keys.\\n- Support for Dubbo3.\\n- Support for Spring Boot3.\\n- Support for JDK 17.\\n- Support for ARM64 images.\\n- Support for multiple registration models.\\n- Extended support for various SQL syntaxes.\\n- Support for GraalVM Native Image.\\n- Support for Redis lua storage mode.\\n  <br/>\\n### Seata 2.x Development Planning\\n![Development Planning](/img/blog/\u53d1\u5c55\u89c4\u5212.jpg)\\nMainly includes the following aspects:\\n- **Storage/Protocol/Features**\\n  Explore storage and computing separation in Raft cluster mode; better experience, unify the current 4 transaction mode APIs; compatible with GTS protocol; support Saga annotations; support distributed lock control; support data perspective insight and governance.\\n- **Ecosystem**\\n  Support more databases, more service frameworks, while exploring support for the domestic trust creation ecosystem; support the MQ ecosystem; further enhance APM support.\\n- **Solutions**\\n  In addition to supporting microservices ecosystems, explore multi-cloud solutions; closer to cloud-native solutions; add security and traffic protection capabilities; achieve self-convergence of core components in the architecture.\\n- **Multi-Language Ecosystem**\\n  Java is the most mature in the multi-language ecosystem, continue to improve other supported programming languages, while exploring Transaction Mesh solutions that are independent of languages.\\n- **R&D Efficiency/Experience**\\n  Improve test coverage, prioritize quality, compatibility, and stability; restructure the official website\'s documentation to improve the hit rate of document searches; simplify operations and deployment on the experience side, achieve one-click installation and metadata simplification; console supports transaction control and online analysis capabilities.\\n\\nIn one sentence, the 2.x plan is summarized as: **Bigger scenarios, bigger ecosystems, from usable to user-friendly.**\\n<br/>\\n### Contact Information for the Seata Community\\n![Contact Information](/img/blog/\u8054\u7cfb\u65b9\u5f0f.jpg)"},{"id":"/seata-observable-practice","metadata":{"permalink":"/blog/seata-observable-practice","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-observable-practice.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-observable-practice.md","title":"Observability Practices in Seata","description":"This article explores and discusses Seata\'s practices in the field of observability.","date":"2023-06-25T00:00:00.000Z","formattedDate":"June 25, 2023","tags":[],"readingTime":7.765,"hasTruncateMarker":false,"authors":[{"name":"rong.liu-Seata"}],"frontMatter":{"title":"Observability Practices in Seata","keywords":["Seata","distributed transaction","data consistency","microservices","observability"],"description":"This article explores and discusses Seata\'s practices in the field of observability.","author":"rong.liu-Seata","date":"2023/06/25"},"unlisted":false,"prevItem":{"title":"Seata:Bridging Data and Applications","permalink":"/blog/seata-connect-data-and-application"},"nextItem":{"title":"seata-go 1.2.0 Ready for Production Environment!!!","permalink":"/blog/seata-go-1.2.0"}},"content":"## Introduction to Seata\\nSeata is the predecessor of Alibaba Group\'s massively used middleware for ensuring consistency of distributed transactions, and Seata is its open source product, maintained by the community. Before introducing Seata, let\'s discuss with you some problematic scenarios that we often encounter in the course of our business development.\\n### Business Scenarios\\nOur business in the development process, basically from a simple application, and gradually transitioned to a huge scale, complex business applications. These complex scenarios inevitably encounter distributed transaction management problems, Seata\'s emergence is to solve these distributed scenarios of transaction management problems. Introduce a few of the classic scenarios:\\n#### Scenario 1: distributed transactions in the scenario of split library and split table\\n![image.png](/img/blog/metrics-\u5206\u5e93\u5206\u8868\u573a\u666f\u4e0b\u7684\u5206\u5e03\u5f0f\u4e8b\u52a1.png)\\nInitially, our business was small and lightweight, and a single database was able to secure our data links. However, as the business scale continues to grow and the business continues to become more complex, usually a single database encounters bottlenecks in terms of capacity and performance. The usual solution is to evolve to a split-database, split-table architecture. At this point, that is, the introduction of **split library and split table scenario **distributed transaction scenarios.\\n#### Scenario 2: Distributed Transactions in a Cross-Service Scenario\\n![image.png](/img/blog/metrics-\u8de8\u670d\u52a1\u573a\u666f\u4e0b\u7684\u5206\u5e03\u5f0f\u4e8b\u52a1.png)\\nA solution to reduce the complexity of a monolithic application: application microservice splitting. After splitting, our product consists of multiple microservice components with different functions, each of which uses independent database resources. When it comes to data consistency scenarios involving cross-service calls, distributed transactions are introduced **in cross-service scenarios**.\\n### Seata architecture\\n![image.png](/img/blog/metrics-Seata\u67b6\u6784.png)\\nIts core components are mainly as follows:\\n\\n- **Transaction Coordinator (TC)**.\\n\\nTransaction coordinator that maintains the running state of the global transaction and is responsible for coordinating and driving the commit or rollback of the global transaction.\\n\\n- **Transaction Manager (TM)**\\n\\nControls global transaction boundaries, responsible for opening a global transaction and ultimately initiating a global commit or global rollback resolution, TM defines the boundaries of a global transaction.\\n\\n- **Resource Manager (RM)**\\n\\nControls branch transactions and is responsible for branch registration, status reporting, and receiving commands from the Transaction Coordinator to drive the commit and rollback of branch (local) transactions.RM is responsible for defining the boundaries and behaviour of branch transactions.\\n## Seata\'s observable practices\\n### Why do we need observables?\\n\\n- *# Distributed transaction message links are more complex *#\\n\\nSeata, while solving these problems of user ease of use and distributed transaction consistency, requires multiple interactions between TC and TM, RM, especially when the links of microservices become complex, the interaction links of Seata will also increase in positive correlation. In this case, we actually need to introduce observable capabilities to observe and analyse things links.\\n\\n- **Anomalous links, hard to locate for troubleshooting, no way to optimise performance***\\n\\nWhen troubleshooting Seata\'s abnormal transaction links, the traditional approach requires looking at logs, which is troublesome to retrieve. With the introduction of observable capabilities, it helps us to visually analyse links and quickly locate problems; providing a basis for optimising time-consuming transaction links.\\n\\n- **Visualisation, Data Quantification**\\n\\nThe visualisation capability allows users to have an intuitive feeling of transaction execution; with quantifiable data, it helps users to assess resource consumption and plan budgets.\\n### Overview of observable capabilities\\n| **observable dimensions** | **seata desired capabilities** | **technology selection reference** |\\n | --- | --- | --- |\\n| Metrics | Functional dimensions: can be grouped and isolated by business, capture important metrics such as total number of transactions, elapsed time and so on.\\nPerformance level: high volume performance, plug-ins loaded on-demand.\\nArchitecture: Reduce third-party dependency, server-side and client-side can adopt a unified architecture to reduce technical complexity.\\nCompatibility level: at least compatible with Prometheus ecosystem | Prometheus: industry-leading position in the field of metrics storage and query.\\nOpenTelemetry: the de facto standard for observable data collection and specification. It does not store, display, or analyse the data itself | Tracing | Functionality level\\n| Tracing | Functionality: Full-link tracing of distributed transaction lifecycle, reacting to distributed transaction execution performance consumption.\\nEase of use: simple and easy to access for users using seata | SkyWalking: using Java\'s Agent probe technology, high efficiency, simple and easy to use. | Logging\\n| Logging | Functionality: logging all lifecycle information of the server and the client\\nEase of use: can quickly match global transactions to corresponding link logs based on XID | Alibaba Cloud Service\\nELK | Alibaba Cloud Service\\n\\n### Metrics dimension\\n#### Design Ideas\\n\\n1. Seata as an integrated data consistency framework, the Metrics module will use as few third-party dependencies as possible to reduce the risk of conflicts.\\n2. the Metrics module will strive for higher metrics performance and lower resource overhead to minimise the side effects of turning it on.\\n3. When configured, whether Metrics is activated and how the data is published depends on the corresponding configuration; if the configuration is turned on, Metrics is automatically activated and the metrics data will be published via prometheusexporter by default.\\n4. do not use Spring, use SPI (Service Provider Interface) to load extensions\\n#### module design\\n![\u56fe\u7247 1.png](/img/blog/metrics-\u6a21\u5757\u8bbe\u8ba1.png)\\n\\n- seata-metrics-core: Metrics core module, organises (loads) 1 Registry and N Exporters according to configuration;\\n- seata-metrics-api: defines the Meter metrics interface, the Registry metrics registry interface;\\n- seata-metrics-exporter-prometheus: built-in implementation of prometheus-exporter;\\n- seata-metrics-registry-compact: built-in Registry implementation and lightweight implementation of Gauge, Counter, Summay, Timer metrics;\\n#### metrics module workflow\\n![\u56fe\u7247 1.png](/img/blog/metrics-\u6a21\u5757\u5de5\u4f5c\u6d41.png)\\nThe above figure shows the workflow of the metrics module, which works as follows:\\n\\n1. load Exporter and Registry implementation classes based on configuration using SPI mechanism;\\n2. based on the message subscription and notification mechanism, listen for state change events for all global transactions and publish to EventBus;\\n3. event subscribers consume events and write the generated metrics to Registry;\\n4. the monitoring system (e.g. prometheus) pulls data from the Exporter.\\n#### TC Core Metrics\\n![image.png](/img/blog/metrics-TC\u6838\u5fc3\u6307\u6807.png)\\n#### TM Core Metrics\\n![image.png](/img/blog/metrics-TM\u6838\u5fc3\u6307\u6807.png)\\n#### RM Core Metrics\\n![image.png](/img/blog/metrics-RM\u6838\u5fc3\u6307\u6807.png)\\n#### Large Cap Showcase\\n![lQLPJxZhZlqESU3NBpjNBp6w8zYK6VbMgzYCoKVrWEDWAA_1694_1688.png](/img/blog/metrics-\u5927\u76d8\u5c55\u793a.png)\\n### Tracing dimensions\\n#### Why does Seata need tracing?\\n\\n1. for the business side, how much of a drain on business performance will the introduction of Seata cause? Where is the main time consumption? How to optimise the business logic? These are all unknown.\\n2. All the message records of Seata are persisted through the log to fall the disc, but the log is very unfriendly to users who do not understand Seata. Can we improve the efficiency of transaction link scheduling by accessing Tracing?\\n3. for novice users, can through Tracing records, quickly understand the working principle of seata, reduce the threshold of seata use.\\n#### Seata\'s tracing solution\\n\\n- Seata defines Header information in a custom RPC message protocol;\\n- SkyWalking intercepts the specified RPC message and injects tracing related span information;\\n- The lifecycle scope of the span is defined with the RPC message sending & receiving as the threshold.\\n\\nBased on the above approach, Seata implements transaction-wide tracing, please refer to [Accessing Skywalking for [Seata application | Seata-server]](/docs/user/apm/skywalking/) for specific access.\\n#### tracing effects\\n\\n- Based on the demo scenario:\\n1. user requests transaction service\\n2. transaction service locks inventory\\n3. the transaction service creates a bill\\n4. The billing service performs a debit\\n\\n![image.png](/img/blog/metrics-tracing\u6548\u679c-\u4e1a\u52a1\u903b\u8f91\u56fe.png)\\n\\n- Transaction link for GlobalCommit success (example)\\n\\n![image.png](/img/blog/metrics-tracing\u6548\u679c-tracing\u94fe1.png)\\n![image.png](/img/blog/metrics-tracing\u6548\u679c-tracing\u94fe2.png)\\n![image.png](/img/blog/metrics-tracing\u6548\u679c-tracing\u94fe3.png)\\n### Logging dimension\\n#### Design Ideas\\n![image.png](/img/blog/metrics-logging\u8bbe\u8ba1\u601d\u8def.png)\\nLogging is the bottom of the observable dimensions. Placed at the bottom, in fact, is the design of our log format, only a good log format, we can make it a better collection, modular storage and display. On top of it, is the log collection, storage, monitoring, alarms, data visualisation, these modules are more ready-made tools, such as Ali\'s SLS logging service, and ELK\'s set of technology stack, we are more overhead costs, access complexity, ecological prosperity, etc. as a consideration.\\n#### Log format design\\nHere we take a log format of Seata-Server as a case study:\\n![image.png](/img/blog/metrics-logging\u65e5\u5fd7\u6548\u679c.png)\\n\\n- Thread pool canonical naming: When there are more thread pools and threads, canonical thread naming can clearly show the execution order of the threads that are executed in an unordered way.\\n- Traceability of method class names: Quickly locate specific code blocks.\\n- Key Runtime Information Transparency: Focus on highlighting key logs and not printing non-critical logs to reduce log redundancy.\\n- Extensible message format: Reduce the amount of code modification by extending the output format of the message class.\\n## Summary & Outlook\\n#### Metrics\\nSummary: basically achieve quantifiable and observable distributed transactions.\\nProspect: more granular metrics, broader ecological compatibility.\\n#### Tracing\\nSummary: Traceability of the whole chain of distributed transactions.\\nProspect: trace transaction links according to xid, and quickly locate the root cause of abnormal links.\\n#### Logging\\nSummary: Structured log format.\\nProspect: Evolution of log observable system."},{"id":"/seata-go-1.2.0","metadata":{"permalink":"/blog/seata-go-1.2.0","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-go-1.2.0.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-go-1.2.0.md","title":"seata-go 1.2.0 Ready for Production Environment!!!","description":"seata-go 1.2.0, ready for production environment!!!","date":"2023-06-08T00:00:00.000Z","formattedDate":"June 8, 2023","tags":[],"readingTime":1.43,"hasTruncateMarker":false,"authors":[{"name":"Seata Community"}],"frontMatter":{"title":"seata-go 1.2.0 Ready for Production Environment!!!","author":"Seata Community","keywords":["seata","distributed transaction","golang","1.2.0"],"description":"seata-go 1.2.0, ready for production environment!!!","date":"2023/06/08"},"unlisted":false,"prevItem":{"title":"Observability Practices in Seata","permalink":"/blog/seata-observable-practice"},"nextItem":{"title":"6 Major Topics Now Open for Selection | Welcome to Apply for Seata Open Source Summer","permalink":"/blog/iscas2023"}},"content":"## Production-ready seata-go 1.2.0 is here!\\n\\nSeata is an open source distributed transaction solution that provides high-performance and easy-to-use distributed transaction services.\\n\\n### Release overview\\n\\nSeata-go version 1.2.0 supports the XA schema, a distributed transaction processing specification proposed by the X/Open organisation, which has the advantage of being non-intrusive to business code. Currently, Seata-go\'s XA mode supports MySQL databases. So far, seata-go has gathered three transaction modes: AT, TCC and XA. Main features of XA mode.\\n\\n- Supported XA data source proxy https://github.com/apache/incubator-seata-go-samples/tree/main/xa\\n- XA transaction mode is supported.\\n  XA related sampes can be found in the following example: https://github.com/apache/incubator-seata-go-samples/tree/main/xa\\n\\n### feature\\n\\n- [[#467](https://github.com/apache/incubator-seata-go/pull/467)] implements XA schema support for MySQL.\\n- [[#534](https://github.com/apache/incubator-seata-go/pull/534)] Load balancing for session support.\\n\\n### bugfix\\n\\n- [[#540](https://github.com/apache/incubator-seata-go/pull/540)] Fix a bug in initialising xa mode.\\n- [[#545](https://github.com/apache/incubator-seata-go/pull/545)] Fix a bug in getting db version number in xa mode.\\n- [[#548](https://github.com/apache/incubator-seata-go/pull/548)] Fix bug where starting xa fails.\\n- [[#556](https://github.com/apache/incubator-seata-go/pull/556)] Fix bug in xa datasource.\\n- [[#562](https://github.com/apache/incubator-seata-go/pull/562)] Fix bug with committing xa global transaction\\n- [[#564](https://github.com/apache/incubator-seata-go/pull/564)] Fix bug committing xa branching transactions\\n- [[#566](https://github.com/apache/incubator-seata-go/pull/566)] Fix bug with local transactions using xa data source.\\n\\n### optimise\\n\\n- [[#523](https://github.com/apache/incubator-seata-go/pull/523)] Optimise CI process\\n- [[#525](https://github.com/apache/incubator-seata-go/pull/525)] rename jackson serialisation to json\\n- [[#532](https://github.com/apache/incubator-seata-go/pull/532)] Remove duplicate code\\n- [[#536](https://github.com/apache/incubator-seata-go/pull/536)] optimise go import code formatting\\n- [[#554](https://github.com/apache/incubator-seata-go/pull/554)] optimise xa mode performance\\n- [[#561](https://github.com/apache/incubator-seata-go/pull/561)] Optimise xa mode logging output\\n\\n### test\\n\\n- [[#535](https://github.com/apache/incubator-seata-go/pull/535)] Add integration tests.\\n\\n### doc\\n\\n- [[#550](https://github.com/apache/incubator-seata-go/pull/550)] Added changelog for version 1.2.0.\\n\\n### contributors\\n\\nThanks to these contributors for their code commits. Please report an unintended omission.\\n\\n- [georgehao](https://github.com/georgehao)\\n- [luky116](https://github.com/luky116)\\n- [jasondeng1997](https://github.com/jasondeng1997)\\n- [106umao](https://github.com/106umao)\\n- [wang1309](https://github.com/wang1309)\\n- [iSuperCoder](https://github.com/iSuperCoder)\\n- [Charlie17Li](https://github.com/Charlie17Li)\\n- [Code-Fight](https://github.com/Code-Fight)\\n- [Kirhaku](https://github.com/Kirhaku)\\n- [Vaderkai](https://github.com/VaderKai)\\n\\n#### Link\\n\\n- https://github.com/apache/incubator-seata\\n- https://github.com/apache/incubator-seata-go\\n- https://github.com/apache/incubator-seata-samples\\n- https://github.com/apache/incubator-seata-go-samples\\n- https://seata.apache.org"},{"id":"/iscas2023","metadata":{"permalink":"/blog/iscas2023","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/iscas2023.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/iscas2023.md","title":"6 Major Topics Now Open for Selection | Welcome to Apply for Seata Open Source Summer","description":"Welcome everyone to register for Seata Open Source Summer 2023 topics","date":"2023-05-12T00:00:00.000Z","formattedDate":"May 12, 2023","tags":[],"readingTime":3.755,"hasTruncateMarker":false,"authors":[{"name":"Seata Community"}],"frontMatter":{"title":"6 Major Topics Now Open for Selection | Welcome to Apply for Seata Open Source Summer","author":"Seata Community","date":"2023/05/12","keywords":["open source summer","seata","distributed transaction"]},"unlisted":false,"prevItem":{"title":"seata-go 1.2.0 Ready for Production Environment!!!","permalink":"/blog/seata-go-1.2.0"},"nextItem":{"title":"Seata 1.6.0 Released with Significant Performance Improvement","permalink":"/blog/seata-1.6.0"}},"content":"### Welcome everyone to register for Seata Open Source Summer 2023 topics\\nThe registration period for Open Source Summer 2023 runs from **April 29th to June 4th**, and we welcome registration for Seata 2023 topics! Here, you will have the opportunity to delve into the theory and application of distributed transactions, and collaborate with students from different backgrounds to complete practical projects. We look forward to your active participation and contribution, jointly promoting the development of the distributed transaction field.\\n\\n![summer2023-1](/img/blog/summer2023-1.jpg)\\n\\n### Seata Open Source Summer 2023\\nOpen Source Summer is a summer open source activity initiated and long-term supported by the Institute of Software, Chinese Academy of Sciences, as part of the \\"Open Source Software Supply Chain Lighting Program\\", aimed at encouraging students to actively participate in the development and maintenance of open source software, cultivating and discovering more outstanding developers, promoting the vigorous development of excellent open source software communities, and assisting in the construction of the open source software supply chain.\\n\\nParticipating students will collaborate remotely online with senior mentors to participate in the development of various organizational projects in the open source community and receive bonuses, gifts, and certificates. **These gains are not only a highlight on future graduation resumes but also a brilliant starting point towards becoming top developers.** Each project is divided into two difficulty levels: basic and advanced, with corresponding project completion bonuses of RMB 8,000 and RMB 12,000 before tax, respectively.\\n\\n### Introduction to the Seata Community\\n**Seata** is an open-source distributed transaction solution, with over 23K+ stars on GitHub, dedicated to providing high-performance and easy-to-use distributed transaction services under the microservices architecture. Before being open-sourced, Seata played a role as a middleware for distributed data consistency within Alibaba, where almost every transaction needed to use Seata. After undergoing the baptism of Double 11\'s massive traffic, it provided strong technical support for business.\\n\\n### Summary of Seata Community Open Source Summer 2023 Project Topics\\nThe Seata community recommends 6 selected project topics for the Open Source Summer 2023 organizing committee. You can visit the following link to make your selection:  \\nhttps://summer-ospp.ac.cn/org/orgdetail/064c15df-705c-483a-8fc8-02831370db14?lang=zh  \\nPlease communicate promptly with the respective mentors and prepare project application materials, and register through the official channels (the following topics are not listed in any particular order):\\n\\n![seata2023-2](/img/blog/summer2023-2.png)\\n\\n#### Project One: Implementation of NamingServer for Service Discovery and Registration\\n\\n**Difficulty:** Advanced\\n\\n**Project Community Mentor:** Chen Jianbin\\n\\n**Mentor\'s Contact Email:** 364176773@qq.com\\n\\n**Project Description:**  \\nCurrently, Seata\'s service exposure and discovery mainly rely on third-party registration centers. With the evolution and development of the project, it brings additional learning and usage costs. Most mainstream middleware with servers have begun to evolve their own service self-loop and control and provide components or functions with higher compatibility and reliability to the server, such as Kafka\'s KRaft, RocketMQ\'s NameServer, ClickHouse\'s ClickHouse Keeper, etc. To address the above problems and architectural evolution requirements, Seata needs to build its own NamingServer to ensure more stability and reliability.\\n\\n**Project Link:**\\nhttps://summer-ospp.ac.cn/org/prodetail/230640380?list=org&navpage=org\\n\\n...\\n\\n(Projects Two to Six translated in the same manner)\\n\\n...\\n\\n### How to Participate in Seata Open Source Summer 2023 and Quickly Select a Project?\\n**Welcome to communicate with each mentor through the above contact information and prepare project application materials.**\\n\\nDuring the project participation period, students can work online from anywhere in the world. The completion of Seata-related projects needs to be submitted to the Seata community repository as a PR by **September 30th**, so please prepare early.\\n\\n![seata2023-3](/img/blog/summer2023-3.png)\\n\\n**To obtain information about mentors and other information during the project, you can scan the QR code below to enter the DingTalk group for communication** \u2014\u2014 Understand various projects in the Seata community, meet Seata community open source mentors, and help with subsequent applications.\\n\\n![summer2023-4](/img/blog/summer2023-4.jpg)\\n\\n#### Reference Materials:\\n**Seata Website:** https://seata.apache.org/\\n\\n**Seata GitHub:** https://github.com/seata\\n\\n**Open Source Summer Official Website:** https://summer-ospp.ac.cn/org/orgdetail/ab188e59-fab8-468f-bc89-bdc2bd8b5e64?lang=zh\\n\\nIf students are interested in other areas of microservices projects, they can also try to apply, such as:\\n\\n- For students interested in **microservice configuration registration centers**, they can try to apply for [Nacos Open Source Summer](https://nacos.io/zh-cn/blog/iscas2023.html);\\n- For students interested in **microservice frameworks and RPC frameworks**, they can try to apply for [Spring Cloud Alibaba Open Source Summer](https://summer-ospp.ac.cn/org/orgdetail/41d68399-ed48-4d6d-9d4d-3ff4128dc132?lang=zh) and [Dubbo Open Source Summer](https://summer-ospp.ac.cn/org/orgdetail/a7f6e2ad-4acc-47f8-9471-4e54b9a166a6?lang=zh);\\n- For students interested in **cloud-native gateways**, they can try to apply for [Higress Open Source Summer](https://higress.io/zh-cn/blog/ospp-2023);\\n- For students interested in **distributed high-availability protection**, they can try to apply for [Sentinel Open Source Summer](https://summer-ospp.ac. cn/org/orgdetail/5e879522-bd90-4a8b-bf8b-b11aea48626b?lang=zh);\\n- For students interested in **microservices governance**, they can try to apply for [OpenSergo Open Source Summer](https://summer-ospp.ac. cn/org/orgdetail/aaff4eec-11b1-4375-997d-5eea8f51762b?lang=zh)."},{"id":"/seata-1.6.0","metadata":{"permalink":"/blog/seata-1.6.0","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-1.6.0.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-1.6.0.md","title":"Seata 1.6.0 Released with Significant Performance Improvement","description":"Seata 1.6.0 Released with Significant Performance Improvement","date":"2022-12-17T00:00:00.000Z","formattedDate":"December 17, 2022","tags":[],"readingTime":5.13,"hasTruncateMarker":false,"authors":[{"name":"Seata Community"}],"frontMatter":{"title":"Seata 1.6.0 Released with Significant Performance Improvement","author":"Seata Community","keywords":["seata","distributed transaction","1.6.0"],"description":"Seata 1.6.0 Released with Significant Performance Improvement","date":"2022/12/17"},"unlisted":false,"prevItem":{"title":"6 Major Topics Now Open for Selection | Welcome to Apply for Seata Open Source Summer","permalink":"/blog/iscas2023"},"nextItem":{"title":"Seata 1.5.2 Released with XID Load Balancing Support","permalink":"/blog/seata-1.5.2"}},"content":"### Seata 1.6.0 Released with Significant Performance Improvement\\n\\nSeata is an open-source distributed transaction solution that provides high performance and easy-to-use distributed transaction services.\\n\\n**Download Links for seata-server:**\\n\\n[source](https://github.com/apache/incubator-seata/archive/v1.6.0.zip) |\\n[binary](https://github.com/apache/incubator-seata/releases/download/v1.6.0/seata-server-1.6.0.zip)\\n\\nUpdates in this version:\\n\\n### feature\uff1a\\n- [[#4863](https://github.com/apache/incubator-seata/pull/4863)] Support for multiple primary keys in Oracle and PostgreSQL\\n- [[#4649](https://github.com/apache/incubator-seata/pull/4649)] Support for multiple registry centers in seata-server\\n- [[#4779](https://github.com/apache/incubator-seata/pull/4779)] Support for Apache Dubbo3\\n- [[#4479](https://github.com/apache/incubator-seata/pull/4479)] TCC annotations can now be added to interfaces and implementation classes\\n- [[#4877](https://github.com/apache/incubator-seata/pull/4877)] Client SDK supports JDK17\\n- [[#4914](https://github.com/apache/incubator-seata/pull/4914)] Support for update join syntax for MySQL\\n- [[#4542](https://github.com/apache/incubator-seata/pull/4542)] Support for Oracle timestamp type\\n- [[#5111](https://github.com/apache/incubator-seata/pull/5111)] Support for Nacos contextPath configuration\\n- [[#4802](https://github.com/apache/incubator-seata/pull/4802)] Dockerfile supports arm64\\n\\n### Bug Fixes:\\n- [[#4780](https://github.com/apache/incubator-seata/pull/4780)] Fixed the issue where TimeoutRollbacked event wasn\'t sent after a successful timeout rollback.\\n- [[#4954](https://github.com/apache/incubator-seata/pull/4954)] Fixed NullPointerException when the output expression was incorrect.\\n- [[#4817](https://github.com/apache/incubator-seata/pull/4817)] Fixed the problem with non-standard configuration in higher versions of Spring Boot.\\n- [[#4838](https://github.com/apache/incubator-seata/pull/4838)] Fixed the issue where undo log wasn\'t generated when using Statement.executeBatch().\\n- [[#4533](https://github.com/apache/incubator-seata/pull/4533)] Fixed inaccurate metric data caused by duplicate events handling for handleRetryRollbacking.\\n- [[#4912](https://github.com/apache/incubator-seata/pull/4912)] Fixed the issue where mysql InsertOnDuplicateUpdate couldn\'t correctly match column names due to inconsistent case.\\n- [[#4543](https://github.com/apache/incubator-seata/pull/4543)] Fixed support for Oracle nclob data type.\\n- [[#4915](https://github.com/apache/incubator-seata/pull/4915)] Fixed the problem of not obtaining ServerRecoveryProperties attributes.\\n- [[#4919](https://github.com/apache/incubator-seata/pull/4919)] Fixed the issue where XID\'s port and address appeared as null:0.\\n- [[#4928](https://github.com/apache/incubator-seata/pull/4928)] Fixed NPE issue in rpcContext.getClientRMHolderMap.\\n- [[#4953](https://github.com/apache/incubator-seata/pull/4953)] Fixed the issue where InsertOnDuplicateUpdate could bypass primary key modification.\\n- [[#4978](https://github.com/apache/incubator-seata/pull/4978)] Fixed kryo support for cyclic dependencies.\\n- [[#4985](https://github.com/apache/incubator-seata/pull/4985)] Fixed the issue of duplicate undo_log id.\\n- [[#4874](https://github.com/apache/incubator-seata/pull/4874)] Fixed startup failure with OpenJDK 11.\\n- [[#5018](https://github.com/apache/incubator-seata/pull/5018)] Fixed server startup failure issue due to loader path using relative path in startup script.\\n- [[#5004](https://github.com/apache/incubator-seata/pull/5004)] Fixed the issue of duplicate row data in mysql update join.\\n- [[#5032](https://github.com/apache/incubator-seata/pull/5032)] Fixed the abnormal SQL statement in mysql InsertOnDuplicateUpdate due to incorrect calculation of condition parameter fill position.\\n- [[#5033](https://github.com/apache/incubator-seata/pull/5033)] Fixed NullPointerException issue in SQL statement of InsertOnDuplicateUpdate due to missing insert column field.\\n- [[#5038](https://github.com/apache/incubator-seata/pull/5038)] Fixed SagaAsyncThreadPoolProperties conflict issue.\\n- [[#5050](https://github.com/apache/incubator-seata/pull/5050)] Fixed the issue where global status under Saga mode wasn\'t correctly changed to Committed.\\n- [[#5052](https://github.com/apache/incubator-seata/pull/5052)] Fixed placeholder parameter issue in update join condition.\\n- [[#5031](https://github.com/apache/incubator-seata/pull/5031)] Fixed the issue of using null value index as query condition in InsertOnDuplicateUpdate.\\n- [[#5075](https://github.com/apache/incubator-seata/pull/5075)] Fixed the inability to intercept SQL statements with no primary key and unique index in InsertOnDuplicateUpdate.\\n- [[#5093](https://github.com/apache/incubator-seata/pull/5093)] Fixed accessKey loss issue after seata server restart.\\n- [[#5092](https://github.com/apache/incubator-seata/pull/5092)] Fixed the issue of incorrect AutoConfiguration order when seata and jpa are used together.\\n- [[#5109](https://github.com/apache/incubator-seata/pull/5109)] Fixed NPE issue when @GlobalTransactional is not applied on RM side.\\n- [[#5098](https://github.com/apache/incubator-seata/pull/5098)] Disabled oracle implicit cache for Druid.\\n- [[#4860](https://github.com/apache/incubator-seata/pull/4860)] Fixed metrics tag override issue.\\n- [[#5028](https://github.com/apache/incubator-seata/pull/5028)] Fixed null value issue in insert on duplicate SQL.\\n- [[#5078](https://github.com/apache/incubator-seata/pull/5078)] Fixed interception issue for SQL statements without primary keys and unique keys.\\n- [[#5097](https://github.com/apache/incubator-seata/pull/5097)] Fixed accessKey loss issue when Server restarts.\\n- [[#5131](https://github.com/apache/incubator-seata/pull/5131)] Fixed issue where XAConn cannot rollback when in active state.\\n- [[#5134](https://github.com/apache/incubator-seata/pull/5134)] Fixed issue where hikariDataSource auto proxy fails in some cases.\\n- [[#5163](https://github.com/apache/incubator-seata/pull/5163)] Fixed compilation failure in higher versions of JDK.\\n\\n### Optimization:\\n- [[#4681](https://github.com/apache/incubator-seata/pull/4681)] Optimized the process of competing locks.\\n- [[#4774](https://github.com/apache/incubator-seata/pull/4774)] Optimized mysql8 dependency in seataio/seata-server image.\\n- [[#4750](https://github.com/apache/incubator-seata/pull/4750)] Optimized the release of global locks in AT branch to not use xid.\\n- [[#4790](https://github.com/apache/incubator-seata/pull/4790)] Added automatic OSSRH github action publishing.\\n- [[#4765](https://github.com/apache/incubator-seata/pull/4765)] XA mode in mysql8.0.29 and above no longer holds connection to the second phase.\\n- [[#4797](https://github.com/apache/incubator-seata/pull/4797)] Optimized all github actions scripts.\\n- [[#4800](https://github.com/apache/incubator-seata/pull/4800)] Added NOTICE file.\\n- [[#4761](https://github.com/apache/incubator-seata/pull/4761)] Used hget instead of hmget in RedisLocker.\\n- [[#4414](https://github.com/apache/incubator-seata/pull/4414)] Removed log4j dependency.\\n- [[#4836](https://github.com/apache/incubator-seata/pull/4836)] Improved readability of BaseTransactionalExecutor#buildLockKey(TableRecords rowsIncludingPK) method.\\n- [[#4865](https://github.com/apache/incubator-seata/pull/4865)] Fixed security vulnerabilities in Saga visualization designer GGEditor.\\n- [[#4590](https://github.com/apache/incubator-seata/pull/4590)] Dynamic configuration support for automatic degradation switch.\\n- [[#4490](https://github.com/apache/incubator-seata/pull/4490)] Optimized tccfence record table to delete by index.\\n- [[#4911](https://github.com/apache/incubator-seata/pull/4911)] Added header and license checks.\\n- [[#4917](https://github.com/apache/incubator-seata/pull/4917)] Upgraded package-lock.json to fix vulnerabilities.\\n- [[#4924](https://github.com/apache/incubator-seata/pull/4924)] Optimized pom dependencies.\\n- [[#4932](https://github.com/apache/incubator-seata/pull/4932)] Extracted default values for some configurations.\\n- [[#4925](https://github.com/apache/incubator-seata/pull/4925)] Optimized javadoc comments.\\n- [[#4921](https://github.com/apache/incubator-seata/pull/4921)] Fixed security vulnerabilities in console module and upgraded skywalking-eyes version.\\n- [[#4936](https://github.com/apache/incubator-seata/pull/4936)] Optimized storage configuration reading.\\n- [[#4946](https://github.com/apache/incubator-seata/pull/4946)] Passed SQL exceptions encountered when acquiring locks to the client.\\n- [[#4962](https://github.com/apache/incubator-seata/pull/4962)] Optimized build configuration and corrected base image of docker image.\\n- [[#4974](https://github.com/apache/incubator-seata/pull/4974)] Removed limitation on querying globalStatus quantity under redis mode.\\n- [[#4981](https://github.com/apache/incubator-seata/pull/4981)] Improved error message when tcc fence record cannot be found.\\n- [[#4995](https://github.com/apache/incubator-seata/pull/4995)] Fixed duplicate primary key query conditions in the SQL statement after mysql InsertOnDuplicateUpdate.\\n- [[#5047](https://github.com/apache/incubator-seata/pull/5047)] Removed unused code.\\n- [[#5051](https://github.com/apache/incubator-seata/pull/5051)] When undolog generates dirty write during rollback, throw exception BranchRollbackFailed_Unretriable.\\n- [[#5075](https://github.com/apache/incubator-seata/pull/5075)] Intercept insert on duplicate update statements without primary keys and unique indexes.\\n- [[#5104](https://github.com/apache/incubator-seata/pull/5104)] ConnectionProxy is no longer dependent on druid.\\n- [[#5124](https://github.com/apache/incubator-seata/pull/5124)] Support deleting TCC fence record table for oracle.\\n- [[#4468](https://github.com/apache/incubator-seata/pull/4968)] Support kryo 5.3.0.\\n- [[#4807](https://github.com/apache/incubator-seata/pull/4807)] Optimized image and OSS repository publishing pipelines.\\n- [[#4445](https://github.com/apache/incubator-seata/pull/4445)] Optimized transaction timeout judgment.\\n- [[#4958](https://github.com/apache/incubator-seata/pull/4958)] Optimized execution of triggerAfterCommit() for timeout transactions.\\n- [[#4582](https://github.com/apache/incubator-seata/pull/4582)] Optimized transaction sorting in redis storage mode.\\n- [[#4963](https://github.com/apache/incubator-seata/pull/4963)] Added ARM64 pipeline CI testing.\\n- [[#4434](https://github.com/apache/incubator-seata/pull/4434)] Removed seata-server CMS GC parameters.\\n\\n### Testing:\\n- [[#4411](https://github.com/apache/incubator-seata/pull/4411)] Tested Oracle database AT mode type support.\\n- [[#4794](https://github.com/apache/incubator-seata/pull/4794)] Refactored code and attempted to fix unit test `DataSourceProxyTest.getResourceIdTest()`.\\n- [[#5101](https://github.com/apache/incubator-seata/pull/5101)] Fixed ClassNotFoundException issue in zk registration and configuration center `DataSourceProxyTest.getResourceIdTest()`.\\n\\nSpecial thanks to the following contributors for their code contributions. If there are any unintentional omissions, please report.\\n\\n\x3c!-- \u8bf7\u786e\u4fdd\u60a8\u7684 GitHub ID \u5728\u4ee5\u4e0b\u5217\u8868\u4e2d --\x3e\\n- [slievrly](https://github.com/slievrly)\\n- [renliangyu857](https://github.com/renliangyu857)\\n- [wangliang181230](https://github.com/wangliang181230)\\n- [funky-eyes](https://github.com/funky-eyes)\\n- [tuwenlin](https://github.com/tuwenlin)\\n- [conghuhu](https://github.com/conghuhu)\\n- [a1104321118](https://github.com/a1104321118)\\n- [duanqiaoyanyu](https://github.com/duanqiaoyanyu)\\n- [robynron](https://github.com/robynron)\\n- [lcmvs](https://github.com/lcmvs)\\n- [github-ganyu](https://github.com/github-ganyu)\\n- [1181954449](https://github.com/1181954449)\\n- [zw201913](https://github.com/zw201913)\\n- [wingchi-leung](https://github.com/wingchi-leung)\\n- [AlexStocks](https://github.com/AlexStocks)\\n- [liujunlin5168](https://github.com/liujunlin5168)\\n- [pengten](https://github.com/pengten)\\n- [liuqiufeng](https://github.com/liuqiufeng)\\n- [yujianfei1986](https://github.com/yujianfei1986)\\n- [Bughue](https://github.com/Bughue)\\n- [AlbumenJ](https://github.com/AlbumenJ)\\n- [doubleDimple](https://github.com/doubleDimple)\\n- [jsbxyyx](https://github.com/jsbxyyx)\\n- [tuwenlin](https://github.com/tuwenlin)\\n- [CrazyLionLi](https://github.com/JavaLionLi)\\n- [whxxxxx](https://github.com/whxxxxx)\\n- [neillee95](https://github.com/neillee95)\\n- [crazy-sheep](https://github.com/crazy-sheep)\\n- [zhangzq7](https://github.com/zhangzq7)\\n- [l81893521](https://github.com/l81893521)\\n- [zhuyoufeng](https://github.com/zhuyoufeng)\\n- [xingfudeshi](https://github.com/xingfudeshi)\\n- [odidev](https://github.com/odidev)\\n- [miaoxueyu](https://github.com/miaoxueyu)\\n\\nAt the same time, we have received many valuable issues and suggestions from the community, and we are very grateful to everyone.\\n\\n#### Link\\n\\n- **Seata:** https://github.com/apache/incubator-seata\\n- **Seata-Samples:** https://github.com/apache/incubator-seata-samples\\n- **Release:** https://github.com/apache/incubator-seata/releases\\n- **WebSite:** https://seata.apache.org"},{"id":"/seata-1.5.2","metadata":{"permalink":"/blog/seata-1.5.2","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-1.5.2.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-1.5.2.md","title":"Seata 1.5.2 Released with XID Load Balancing Support","description":"Seata 1.5.2 Released with XID Load Balancing Support","date":"2022-07-12T00:00:00.000Z","formattedDate":"July 12, 2022","tags":[],"readingTime":2.295,"hasTruncateMarker":false,"authors":[{"name":"Seata Community"}],"frontMatter":{"title":"Seata 1.5.2 Released with XID Load Balancing Support","author":"Seata Community","keywords":["seata","distributed transaction","1.5.2"],"description":"Seata 1.5.2 Released with XID Load Balancing Support","date":"2022/07/12"},"unlisted":false,"prevItem":{"title":"Seata 1.6.0 Released with Significant Performance Improvement","permalink":"/blog/seata-1.6.0"},"nextItem":{"title":"Alibaba Seata Resolves Idempotence, Dangling, and Empty Rollback Issues in TCC Mode","permalink":"/blog/seata-tcc-fence"}},"content":"### Seata 1.5.2 Released with XID Load Balancing Support\\n\\nSeata is an open-source distributed transaction solution that provides high-performance and easy-to-use distributed transaction services.\\n\\n**seata-server Download Links:**\\n\\n[source](https://github.com/apache/incubator-seata/archive/v1.5.2.zip) |\\n[binary](https://github.com/apache/incubator-seata/releases/download/v1.5.2/seata-server-1.5.2.zip)\\n\\nThe key updates in this version include:\\n\\n### Features:\\n- [[#4661](https://github.com/apache/incubator-seata/pull/4713)] Added support for XID load balancing algorithm.\\n- [[#4676](https://github.com/apache/incubator-seata/pull/4676)] Added support for Seata server to expose services through SLB when using Nacos as the registry center.\\n- [[#4642](https://github.com/apache/incubator-seata/pull/4642)] Added support for parallel processing of client batch requests.\\n- [[#4567](https://github.com/apache/incubator-seata/pull/4567)] Added support for the `find_in_set` function in the WHERE condition.\\n\\n### Bug Fixes:\\n- [[#4515](https://github.com/apache/incubator-seata/pull/4515)] Fixed an issue where SeataTCCFenceAutoConfiguration on the develop branch throws a ClassNotFoundException when the client does not use a DB.\\n- [[#4661](https://github.com/apache/incubator-seata/pull/4661)] Fixed SQL exceptions when using PostgreSQL in the console.\\n- [[#4667](https://github.com/apache/incubator-seata/pull/4682)] Fixed an exception when updating the map in RedisTransactionStoreManager on the develop branch.\\n- [[#4678](https://github.com/apache/incubator-seata/pull/4678)] Fixed the issue of cache penetration when the property `transport.enableRmClientBatchSendRequest` is not configured.\\n- [[#4701](https://github.com/apache/incubator-seata/pull/4701)] Fixed the issue of missing command line parameters.\\n- [[#4607](https://github.com/apache/incubator-seata/pull/4607)] Fixed a defect in skipping global lock verification.\\n- [[#4696](https://github.com/apache/incubator-seata/pull/4696)] Fixed the insertion issue when using the Oracle storage mode.\\n- [[#4726](https://github.com/apache/incubator-seata/pull/4726)] Fixed a possible NPE issue when sending messages in batches.\\n- [[#4729](https://github.com/apache/incubator-seata/pull/4729)] Fixed the issue of incorrect setting of `AspectTransactional.rollbackForClassName`.\\n- [[#4653](https://github.com/apache/incubator-seata/pull/4653)] Fixed the exception of non-numeric primary key in INSERT_ON_DUPLICATE.\\n\\n### Optimizations:\\n- [[#4650](https://github.com/apache/incubator-seata/pull/4650)] Fixed a security vulnerability.\\n- [[#4670](https://github.com/apache/incubator-seata/pull/4670)] Optimized the number of threads in the `branchResultMessageExecutor` thread pool.\\n- [[#4662](https://github.com/apache/incubator-seata/pull/4662)] Optimized the monitoring metrics for rolling back transactions.\\n- [[#4693](https://github.com/apache/incubator-seata/pull/4693)] Optimized the console navigation bar.\\n- [[#4700](https://github.com/apache/incubator-seata/pull/4700)] Fixed failures in the execution of maven-compiler-plugin and maven-resources-plugin.\\n- [[#4711](https://github.com/apache/incubator-seata/pull/4711)] Separated the lib dependency during deployment.\\n- [[#4720](https://github.com/apache/incubator-seata/pull/4720)] Optimized pom descriptions.\\n- [[#4728](https://github.com/apache/incubator-seata/pull/4728)] Upgraded the logback version dependency to 1.2.9.\\n- [[#4745](https://github.com/apache/incubator-seata/pull/4745)] Added support for mysql8 driver in the distribution package.\\n- [[#4626](https://github.com/apache/incubator-seata/pull/4626)] Used `easyj-maven-plugin` plugin instead of `flatten-maven-plugin` to fix compatibility issues between `shade` plugin and `flatten` plugin.\\n- [[#4629](https://github.com/apache/incubator-seata/pull/4629)] Checked the constraint relationship before and after updating the globalSession status.\\n- [[#4662](https://github.com/apache/incubator-seata/pull/4662)] Optimized the readability of EnhancedServiceLoader.\\n\\n### Tests:\\n- [[#4544](https://github.com/apache/incubator-seata/pull/4544)] Optimized the jackson package dependency issue in TransactionContextFilterTest.\\n- [[#4731](https://github.com/apache/incubator-seata/pull/4731)] Fixed unit test issues in AsyncWorkerTest and LockManagerTest.\\n\\nA big thanks to the contributors for their valuable code contributions. If inadvertently omitted, please report.\\n\\n\\n\x3c!-- Make sure your GitHub ID is in the list below --\x3e\\n- [slievrly](https://github.com/slievrly)\\n- [pengten](https://github.com/pengten)\\n- [YSF-A](https://github.com/YSF-A)\\n- [tuwenlin](https://github.com/tuwenlin)\\n- [2129zxl](https://github.com/2129zxl)\\n- [Ifdevil](https://github.com/Ifdevil)\\n- [wingchi-leung](https://github.com/wingchi-leung)\\n- [liurong](https://github.com/robynron)\\n- [opelok-z](https://github.com/opelok-z)\\n- [funky-eyes](https://github.com/funky-eyes)\\n- [Smery-lxm](https://github.com/Smery-lxm)\\n- [lvekee](https://github.com/lvekee)\\n- [doubleDimple](https://github.com/doubleDimple)\\n- [wangliang181230](https://github.com/wangliang181230)\\n- [Bughue](https://github.com/Bughue)\\n- [AYue-94](https://github.com/AYue-94)\\n- [lingxiao-wu](https://github.com/lingxiao-wu)\\n- [caohdgege](https://github.com/caohdgege)\\n\\nAt the same time, we have received many valuable issues and suggestions from the community, thank you very much.\\n\\n#### Link\\n\\n- **Seata:** https://github.com/apache/incubator-seata\\n- **Seata-Samples:** https://github.com/apache/incubator-seata-samples\\n- **Release:** https://github.com/apache/incubator-seata/releases\\n- **WebSite:** https://seata.io"},{"id":"/seata-tcc-fence","metadata":{"permalink":"/blog/seata-tcc-fence","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-tcc-fence.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-tcc-fence.md","title":"Alibaba Seata Resolves Idempotence, Dangling, and Empty Rollback Issues in TCC Mode","description":"Seata version 1.5.1 from Alibaba has finally resolved the issues of idempotence, dangling, and empty rollback in TCC (Try-Confirm-Cancel) mode. This article mainly explains how Seata addresses these problems.","date":"2022-06-25T00:00:00.000Z","formattedDate":"June 25, 2022","tags":[],"readingTime":10.97,"hasTruncateMarker":false,"authors":[{"name":"Zhu Jinjun"}],"frontMatter":{"title":"Alibaba Seata Resolves Idempotence, Dangling, and Empty Rollback Issues in TCC Mode","author":"Zhu Jinjun","keywords":["Seata","TCC","idempotence","dangling","empty rollback"],"description":"Seata version 1.5.1 from Alibaba has finally resolved the issues of idempotence, dangling, and empty rollback in TCC (Try-Confirm-Cancel) mode. This article mainly explains how Seata addresses these problems.","date":"2022/06/25"},"unlisted":false,"prevItem":{"title":"Seata 1.5.2 Released with XID Load Balancing Support","permalink":"/blog/seata-1.5.2"},"nextItem":{"title":"In-Depth Analysis of Seata TCC Mode (1)","permalink":"/blog/seata-tcc"}},"content":"Today, let\'s talk about how the new version (1.5.1) of Alibaba\'s Seata resolves the issues of idempotence, dangling, and empty rollback in TCC mode.\\n\\n## 1 TCC \\n\\nTCC mode is the most classic solution for distributed transactions. It divides the distributed transaction into two phases. In the try phase, resources are reserved for each branch transaction. If all branch transactions successfully reserve resources, the global transaction proceeds to the commit phase for committing the transaction globally. However, if any node fails to reserve resources, the global transaction enters the cancel phase to rollback the transaction globally.\\n\\nTaking traditional order, inventory, and account services as an example, in the try phase, resources are attempted to be reserved by inserting orders, deducting inventory, and deducting amounts. These three services require local transaction commits, and the resources can be transferred to an intermediate table. In the commit phase, the resources reserved in the try phase are transferred to the final table. In the cancel phase, the resources reserved in the try phase are released, such as returning the account amount to the customer\'s account.\\n\\n**Note: The try phase must involve committing local transactions. For example, when deducting the order amount, the money must be deducted from the customer\'s account. If it is not deducted, there will be a problem in the commit phase if the customer\'s account does not have enough money.**\\n\\n### 1.1 try-commit\\n\\nIn the try phase, resources are first reserved, and then they are deducted in the commit phase. The diagram below illustrates this process:\\n\\n![fence-try-commit](/img/blog/fence-try-commit.png)\\n\\n\\n### 1.2 try-cancel\\n\\nIn the try phase, resources are first reserved. If the deduction of inventory fails, leading to a rollback of the global transaction, the resources are released in the cancel phase. The diagram below illustrates this process:\\n\\n![fence-try-cancel](/img/blog/fence-try-cancel.png)\\n\\n\\n## 2 TCC Advantages\\n\\nThe biggest advantage of TCC mode is its high efficiency. In the try phase, the resource locking in TCC mode is not a true lock, but rather a real local transaction submission that reserves resources in an intermediate state without the need for blocking and waiting. Therefore, it is more efficient than other modes.\\n\\nAdditionally, the TCC mode can be optimized as follows:\\n\\n### 2.1 Asynchronous Commit\\n\\nAfter the try phase succeeds, instead of immediately entering the confirm/cancel phase, it is considered that the global transaction has already ended. A scheduled task is started to asynchronously execute the confirm/cancel phase, which involves deducting or releasing resources. This approach can greatly improve performance.\\n\\n### 2.2 Same-Database Mode\\n\\nIn the TCC mode, there are three roles:\\n\\n- TM: Manages the global transaction, including starting the global transaction and committing/rolling back the global transaction.\\n- RM: Manages the branch transaction.\\n- TC: Manages the state of the global transaction and branch transactions.\\n\\nThe diagram below is from the Seata official website:\\n\\n![fence-fiffrent-db](/img/blog/fence-fiffrent-db.png)\\n\\nWhen TM starts a global transaction, RM needs to send a registration message to TC, and TC saves the state of the branch transaction. When TM requests a commit or rollback, TC needs to send commit or rollback messages to RM. In this way, in a distributed transaction with two branch transactions, there are four RPCs between TC and RM.\\n\\nAfter optimization, the process is as shown in the diagram below:\\n\\nTC saves the state of the global transaction. When TM starts a global transaction, RM no longer needs to send a registration message to TC. Instead, it saves the state of the branch transaction locally. After TM sends a commit or rollback message to TC, the asynchronous thread in RM first retrieves the uncommitted branch transactions saved locally, and then sends a message to TC to obtain the state of the global transaction in which the local branch transaction is located, in order to determine whether to commit or rollback the local transaction.\\n\\nWith this optimization, the number of RPCs is reduced by 50%, resulting in a significant performance improvement.\\n\\n## 3 RM Code Example\\n\\nTaking the inventory service as an example, the RM inventory service interface code is as follows:\\n```Java\\n@LocalTCC\\npublic interface StorageService {\\n\\n    /**\\n     * decrease\\n     * @param xid \\n     * @param productId \\n     * @param count \\n     * @return\\n     */\\n    @TwoPhaseBusinessAction(name = \\"storageApi\\", commitMethod = \\"commit\\", rollbackMethod = \\"rollback\\", useTCCFence = true)\\n    boolean decrease(String xid, Long productId, Integer count);\\n\\n    /**\\n     * commit\\n     * @param actionContext\\n     * @return\\n     */\\n    boolean commit(BusinessActionContext actionContext);\\n\\n    /**\\n     * rollback\\n     * @param actionContext\\n     * @return\\n     */\\n    boolean rollback(BusinessActionContext actionContext);\\n}\\n```\\nBy using the `@LocalTCC` annotation, when the RM is initialized, it registers a branch transaction with the TC. The `try` phase method (e.g., `decrease` method) is annotated with `@TwoPhaseBusinessAction`, which defines the branch transaction\'s `resourceId`, `commit` method, `cancel` method, and the `useTCCFence` property, which will be explained in the next section.\\n\\n## 4 Issues with TCC\\n\\nThere are three major issues with the TCC pattern: idempotence, suspension, and empty rollback. In version 1.5.1 of Seata, a transaction control table named `tcc_fence_log` is introduced to address these issues. The `useTCCFence` property mentioned in the previous `@TwoPhaseBusinessAction` annotation is used to enable or disable this mechanism, with a default value of `false`.\\n\\nThe creation SQL statement for the `tcc_fence_log` table (in MySQL syntax) is as follows:\\n\\n```SQL\\nCREATE TABLE IF NOT EXISTS `tcc_fence_log`\\n(\\n    `xid`           VARCHAR(128)  NOT NULL COMMENT \'global id\',\\n    `branch_id`     BIGINT        NOT NULL COMMENT \'branch id\',\\n    `action_name`   VARCHAR(64)   NOT NULL COMMENT \'action name\',\\n    `status`        TINYINT       NOT NULL COMMENT \'status(tried:1;committed:2;rollbacked:3;suspended:4)\',\\n    `gmt_create`    DATETIME(3)   NOT NULL COMMENT \'create time\',\\n    `gmt_modified`  DATETIME(3)   NOT NULL COMMENT \'update time\',\\n    PRIMARY KEY (`xid`, `branch_id`),\\n    KEY `idx_gmt_modified` (`gmt_modified`),\\n    KEY `idx_status` (`status`)\\n) ENGINE = InnoDB\\nDEFAULT CHARSET = utf8mb4;\\n```\\n\\n### 4.1 Idempotence\\n\\nDuring the commit/cancel phase, if the TC does not receive a response from the branch transaction, it needs to retry the operation. Therefore, it is necessary for the branch transaction to support idempotence.\\n\\nLet\'s take a look at how this is addressed in the new version. The following code is from the `TCCResourceManager` class:\\n\\n```Java\\n@Override\\npublic BranchStatus branchCommit(BranchType branchType, String xid, long branchId, String resourceId,\\n\\t\\t\\t\\t\\t\\t\\t\\t String applicationData) throws TransactionException {\\n\\tTCCResource tccResource = (TCCResource)tccResourceCache.get(resourceId);\\n\\tObject targetTCCBean = tccResource.getTargetBean();\\n\\tMethod commitMethod = tccResource.getCommitMethod();\\n\\ttry {\\n\\t\\t//BusinessActionContext\\n\\t\\tBusinessActionContext businessActionContext = getBusinessActionContext(xid, branchId, resourceId,\\n\\t\\t\\tapplicationData);\\n\\t\\tObject[] args = this.getTwoPhaseCommitArgs(tccResource, businessActionContext);\\n\\t\\tObject ret;\\n\\t\\tboolean result;\\n\\t\\t//whether the useTCCFence property is set to true\\n\\t\\tif (Boolean.TRUE.equals(businessActionContext.getActionContext(Constants.USE_TCC_FENCE))) {\\n\\t\\t\\ttry {\\n\\t\\t\\t\\tresult = TCCFenceHandler.commitFence(commitMethod, targetTCCBean, xid, branchId, args);\\n\\t\\t\\t} catch (SkipCallbackWrapperException | UndeclaredThrowableException e) {\\n\\t\\t\\t\\tthrow e.getCause();\\n\\t\\t\\t}\\n\\t\\t} else {\\n\\t\\t}\\n\\t\\tLOGGER.info(\\"TCC resource commit result : {}, xid: {}, branchId: {}, resourceId: {}\\", result, xid, branchId, resourceId);\\n\\t\\treturn result ? BranchStatus.PhaseTwo_Committed : BranchStatus.PhaseTwo_CommitFailed_Retryable;\\n\\t} catch (Throwable t) {\\n\\t\\treturn BranchStatus.PhaseTwo_CommitFailed_Retryable;\\n\\t}\\n}\\n```\\nThe above code shows that when executing the commit method of the branch transaction, it first checks if the `useTCCFence` property is `true`. If it is `true`, it follows the `commitFence` logic in the `TCCFenceHandler` class; otherwise, it follows the normal commit logic.\\n\\nThe `commitFence` method in the `TCCFenceHandler` class calls the `commitFence` method of the same class. The code is as follows:\\n\\n```Java\\npublic static boolean commitFence(Method commitMethod, Object targetTCCBean,\\n\\t\\t\\t\\t\\t\\t\\t\\t  String xid, Long branchId, Object[] args) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tTCCFenceDO tccFenceDO = TCC_FENCE_DAO.queryTCCFenceDO(conn, xid, branchId);\\n\\t\\t\\tif (tccFenceDO == null) {\\n\\t\\t\\t\\tthrow new TCCFenceException(String.format(\\"TCC fence record not exists, commit fence method failed. xid= %s, branchId= %s\\", xid, branchId),\\n\\t\\t\\t\\t\\t\\tFrameworkErrorCode.RecordAlreadyExists);\\n\\t\\t\\t}\\n\\t\\t\\tif (TCCFenceConstant.STATUS_COMMITTED == tccFenceDO.getStatus()) {\\n\\t\\t\\t\\tLOGGER.info(\\"Branch transaction has already committed before. idempotency rejected. xid: {}, branchId: {}, status: {}\\", xid, branchId, tccFenceDO.getStatus());\\n\\t\\t\\t\\treturn true;\\n\\t\\t\\t}\\n\\t\\t\\tif (TCCFenceConstant.STATUS_ROLLBACKED == tccFenceDO.getStatus() || TCCFenceConstant.STATUS_SUSPENDED == tccFenceDO.getStatus()) {\\n\\t\\t\\t\\tif (LOGGER.isWarnEnabled()) {\\n\\t\\t\\t\\t\\tLOGGER.warn(\\"Branch transaction status is unexpected. xid: {}, branchId: {}, status: {}\\", xid, branchId, tccFenceDO.getStatus());\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\treturn false;\\n\\t\\t\\t}\\n\\t\\t\\treturn updateStatusAndInvokeTargetMethod(conn, commitMethod, targetTCCBean, xid, branchId, TCCFenceConstant.STATUS_COMMITTED, status, args);\\n\\t\\t} catch (Throwable t) {\\n\\t\\t\\tstatus.setRollbackOnly();\\n\\t\\t\\tthrow new SkipCallbackWrapperException(t);\\n\\t\\t}\\n\\t});\\n}\\n```\\nFrom the code, we can see that when committing the transaction, it first checks if there is a record in the `tcc_fence_log` table. If a record exists, it checks the transaction execution status and returns. This ensures idempotence by avoiding duplicate commits if the transaction status is already `STATUS_COMMITTED`. If there is no record in the `tcc_fence_log` table, a new record is inserted for later retry detection.\\n\\nThe rollback logic is similar to the commit logic and is implemented in the `rollbackFence` method of the `TCCFenceHandler` class.\\n\\n### 4.2 Empty Rollback\\n\\nIn the scenario shown in the following diagram, the account service consists of a cluster of two nodes. During the try phase, the account service on Node 1 encounters a failure. Without considering retries, the global transaction must reach the end state, requiring a cancel operation to be performed on the account service.\\n\\n![fence-empty-rollback](/img/blog/fence-empty-rollback.png)\\n\\nSeata\'s solution is to insert a record into the `tcc_fence_log` table during the try phase, with the `status` field set to `STATUS_TRIED`. During the rollback phase, it checks if the record exists, and if it doesn\'t, the rollback operation is not executed. The code is as follows:\\n\\n```Java\\n//TCCFenceHandler \\npublic static Object prepareFence(String xid, Long branchId, String actionName, Callback<Object> targetCallback) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tboolean result = insertTCCFenceLog(conn, xid, branchId, actionName, TCCFenceConstant.STATUS_TRIED);\\n\\t\\t\\tLOGGER.info(\\"TCC fence prepare result: {}. xid: {}, branchId: {}\\", result, xid, branchId);\\n\\t\\t\\tif (result) {\\n\\t\\t\\t\\treturn targetCallback.execute();\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tthrow new TCCFenceException(String.format(\\"Insert tcc fence record error, prepare fence failed. xid= %s, branchId= %s\\", xid, branchId),\\n\\t\\t\\t\\t\\t\\tFrameworkErrorCode.InsertRecordError);\\n\\t\\t\\t}\\n\\t\\t} catch (TCCFenceException e) {\\n\\t\\t} catch (Throwable t) {\\n\\t\\t}\\n\\t});\\n}\\n```\\nThe processing logic in the Rollback phase is as follows:\\n\\n```Java\\n//TCCFenceHandler \\npublic static boolean rollbackFence(Method rollbackMethod, Object targetTCCBean,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tString xid, Long branchId, Object[] args, String actionName) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tTCCFenceDO tccFenceDO = TCC_FENCE_DAO.queryTCCFenceDO(conn, xid, branchId);\\n\\t\\t\\t// non_rollback\\n\\t\\t\\tif (tccFenceDO == null) {\\n\\t\\t\\t\\t//The rollback logic is not executed\\n\\t\\t\\t\\treturn true;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tif (TCCFenceConstant.STATUS_ROLLBACKED == tccFenceDO.getStatus() || TCCFenceConstant.STATUS_SUSPENDED == tccFenceDO.getStatus()) {\\n\\t\\t\\t\\t\\tLOGGER.info(\\"Branch transaction had already rollbacked before, idempotency rejected. xid: {}, branchId: {}, status: {}\\", xid, branchId, tccFenceDO.getStatus());\\n\\t\\t\\t\\t\\treturn true;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tif (TCCFenceConstant.STATUS_COMMITTED == tccFenceDO.getStatus()) {\\n\\t\\t\\t\\t\\tif (LOGGER.isWarnEnabled()) {\\n\\t\\t\\t\\t\\t\\tLOGGER.warn(\\"Branch transaction status is unexpected. xid: {}, branchId: {}, status: {}\\", xid, branchId, tccFenceDO.getStatus());\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\treturn false;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\treturn updateStatusAndInvokeTargetMethod(conn, rollbackMethod, targetTCCBean, xid, branchId, TCCFenceConstant.STATUS_ROLLBACKED, status, args);\\n\\t\\t} catch (Throwable t) {\\n\\t\\t\\tstatus.setRollbackOnly();\\n\\t\\t\\tthrow new SkipCallbackWrapperException(t);\\n\\t\\t}\\n\\t});\\n}\\n```\\nupdateStatusAndInvokeTargetMethod method executes the following SQL:\\n\\n```sql\\nupdate tcc_fence_log set status = ?, gmt_modified = ?\\n    where xid = ? and  branch_id = ? and status = ? ;\\n```\\nAs we can see, it updates the value of the status field in the tcc_fence_log table from STATUS_TRIED to STATUS_ROLLBACKED. If the update is successful, the rollback logic is executed.\\n\\n### 4.3 Hanging\\nHanging refers to a situation where, due to network issues, the RM did not receive the try instruction initially, but after executing the rollback, the RM receives the try instruction and successfully reserves resources. This leads to the inability to release the reserved resources, as shown in the following diagram:\\n\\n![fence-suspend](/img/blog/fence-suspend.png)\\n\\nSeata solves this problem by checking if there is a record for the current xid in the tcc_fence_log table before executing the rollback method. If there is no record, it inserts a new record into the tcc_fence_log table with the status STATUS_SUSPENDED and does not perform the rollback operation. The code is as follows:\\n\\n```Java\\npublic static boolean rollbackFence(Method rollbackMethod, Object targetTCCBean,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tString xid, Long branchId, Object[] args, String actionName) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tTCCFenceDO tccFenceDO = TCC_FENCE_DAO.queryTCCFenceDO(conn, xid, branchId);\\n\\t\\t\\t// non_rollback\\n\\t\\t\\tif (tccFenceDO == null) {\\n\\t\\t\\t\\tboolean result = insertTCCFenceLog(conn, xid, branchId, actionName, TCCFenceConstant.STATUS_SUSPENDED);\\n\\t\\t\\t\\treturn true;\\n\\t\\t\\t} else {\\n\\t\\t\\t}\\n\\t\\t\\treturn updateStatusAndInvokeTargetMethod(conn, rollbackMethod, targetTCCBean, xid, branchId, TCCFenceConstant.STATUS_ROLLBACKED, status, args);\\n\\t\\t} catch (Throwable t) {\\n\\t\\t}\\n\\t});\\n}\\n```\\nWhen executing the try phase method, a record for the current xid is first inserted into the tcc_fence_log table, which causes a primary key conflict. The code is as follows:\\n```Java\\n//TCCFenceHandler \\npublic static Object prepareFence(String xid, Long branchId, String actionName, Callback<Object> targetCallback) {\\n\\treturn transactionTemplate.execute(status -> {\\n\\t\\ttry {\\n\\t\\t\\tConnection conn = DataSourceUtils.getConnection(dataSource);\\n\\t\\t\\tboolean result = insertTCCFenceLog(conn, xid, branchId, actionName, TCCFenceConstant.STATUS_TRIED);\\n\\t\\t} catch (TCCFenceException e) {\\n\\t\\t\\tif (e.getErrcode() == FrameworkErrorCode.DuplicateKeyException) {\\n\\t\\t\\t\\tLOGGER.error(\\"Branch transaction has already rollbacked before,prepare fence failed. xid= {},branchId = {}\\", xid, branchId);\\n\\t\\t\\t\\taddToLogCleanQueue(xid, branchId);\\n\\t\\t\\t}\\n\\t\\t\\tstatus.setRollbackOnly();\\n\\t\\t\\tthrow new SkipCallbackWrapperException(e);\\n\\t\\t} catch (Throwable t) {\\n\\t\\t}\\n\\t});\\n}\\n```\\nNote: The queryTCCFenceDO method in the SQL statement uses for update, so there is no need to worry about not being able to determine the execution result of the local transaction in the rollback method due to the inability to obtain records from the tcc_fence_log table.\\n\\n### 5 Summary\\nTCC mode is a very important transaction mode in distributed transactions. However, idempotence, hanging, and empty rollback have always been issues that need to be considered in TCC mode. The Seata framework perfectly solves these problems in version 1.5.1.\\nThe operations on the tcc_fence_log table also need to consider transaction control. Seata uses a proxy data source to execute the operations on the tcc_fence_log table and the RM business operations in the same local transaction. This ensures that the local operations and the operations on the tcc_fence_log table succeed or fail together."},{"id":"/seata-tcc","metadata":{"permalink":"/blog/seata-tcc","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-tcc.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-tcc.md","title":"In-Depth Analysis of Seata TCC Mode (1)","description":"Seata currently supports AT mode, XA mode, TCC mode, and SAGA mode. Previous articles have talked more about non-intrusive AT mode. Today, we will introduce TCC mode, which is also a two-phase commit.","date":"2022-01-18T00:00:00.000Z","formattedDate":"January 18, 2022","tags":[],"readingTime":11.245,"hasTruncateMarker":false,"authors":[{"name":"Zhang Chenghui"}],"frontMatter":{"title":"In-Depth Analysis of Seata TCC Mode (1)","author":"Zhang Chenghui","keywords":["Seata\u3001distributed transaction\u3001TCC"],"description":"Seata currently supports AT mode, XA mode, TCC mode, and SAGA mode. Previous articles have talked more about non-intrusive AT mode. Today, we will introduce TCC mode, which is also a two-phase commit.","date":"2022/01/18"},"unlisted":false,"prevItem":{"title":"Alibaba Seata Resolves Idempotence, Dangling, and Empty Rollback Issues in TCC Mode","permalink":"/blog/seata-tcc-fence"},"nextItem":{"title":"In-Depth Analysis of Seata AT Mode Transaction Isolation Levels and Global Lock Design","permalink":"/blog/seata-at-lock"}},"content":"Seata currently supports AT mode, XA mode, TCC mode, and SAGA mode. Previous articles have talked more about non-intrusive AT mode. Today, we will introduce TCC mode, which is also a two-phase commit.\\n\\n# What is TCC\\n\\nTCC is a two-phase commit protocol in distributed transactions. Its full name is Try-Confirm-Cancel. Their specific meanings are as follows:\\n\\n1. Try: Check and reserve business resources;\\n2. Confirm: Commit the business transaction, i.e., the commit operation. If Try is successful, this step will definitely be successful;\\n3. Cancel: Cancel the business transaction, i.e., the rollback operation. This step will release the resources reserved in Try.\\n\\nTCC is an intrusive distributed transaction solution. All three operations need to be implemented by the business system itself, which has a significant impact on the business system. The design is relatively complex, but the advantage is that TCC does not rely on the database. It can manage resources across databases and applications, and can implement an atomic operation for different data access through intrusive coding, better solving the distributed transaction problems in various complex business scenarios.\\n\\n<img src=\\"/img/blog/20220116160157.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\n# Seata TCC mode\\n\\nSeata TCC mode follows the same principle as the general TCC mode. Let\'s first use Seata TCC mode to implement a distributed transaction:\\n\\nSuppose there is a business that needs to use service A and service B to complete a transaction operation. We define a TCC interface for this service in service A:\\n\\n```java\\npublic interface TccActionOne {\\n    @TwoPhaseBusinessAction(name = \\"DubboTccActionOne\\", commitMethod = \\"commit\\", rollbackMethod = \\"rollback\\")\\n    public boolean prepare(BusinessActionContext actionContext, @BusinessActionContextParameter(paramName = \\"a\\") String a);\\n\\n    public boolean commit(BusinessActionContext actionContext);\\n\\n    public boolean rollback(BusinessActionContext actionContext);\\n}\\n```\\n\\nSimilarly, we define a TCC interface for this service in service B:\\n\\n```java\\npublic interface TccActionTwo {\\n    @TwoPhaseBusinessAction(name = \\"DubboTccActionTwo\\", commitMethod = \\"commit\\", rollbackMethod = \\"rollback\\")\\n    public void prepare(BusinessActionContext actionContext, @BusinessActionContextParameter(paramName = \\"b\\") String b);\\n\\n    public void commit(BusinessActionContext actionContext);\\n\\n    public void rollback(BusinessActionContext actionContext);\\n}\\n```\\n\\nIn the business system, we start a global transaction and execute the TCC reserve resource methods for service A and service B:\\n\\n```java\\n@GlobalTransactional\\npublic String doTransactionCommit(){\\n    // Service A transaction participant\\n    tccActionOne.prepare(null,\\"one\\");\\n    // Service B transaction participant\\n    tccActionTwo.prepare(null,\\"two\\");\\n}\\n```\\n\\nThe example above demonstrates the implementation of a global transaction using Seata TCC mode. It can be seen that the TCC mode also uses the `@GlobalTransactional` annotation to initiate a global transaction, while the TCC interfaces of Service A and Service B are transaction participants. Seata treats a TCC interface as a Resource, also known as a TCC Resource.\\n\\nTCC interfaces can be RPC or internal JVM calls, meaning that a TCC interface has both a sender and a caller identity. In the example above, the TCC interface is the sender in Service A and Service B, and the caller in the business system. If the TCC interface is a Dubbo RPC, the caller is a dubbo:reference and the sender is a dubbo:service.\\n\\n<img src=\\"/img/blog/20220116161933.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nWhen Seata starts, it scans and parses the TCC interfaces. If a TCC interface is a sender, Seata registers the TCC Resource with the TC during startup, and each TCC Resource has a resource ID. If a TCC interface is a caller, Seata proxies the caller and intercepts the TCC interface calls. Similar to the AT mode, the proxy intercepts the call to the Try method, registers a branch transaction with the TC, and then executes the original RPC call.\\n\\nWhen the global transaction decides to commit/rollback, the TC will callback to the corresponding participant service to execute the Confirm/Cancel method of the TCC Resource using the resource ID registered by the branch.\\n\\n# How Seata Implements TCC Mode\\n\\nFrom the above Seata TCC model, it can be seen that the TCC mode in Seata also follows the TC, TM, RM three-role model. How to implement TCC mode in these three-role models? I mainly summarize the implementation as resource parsing, resource management, and transaction processing.\\n\\n## Resource Parsing\\n\\nResource parsing is the process of parsing and registering TCC interfaces. As mentioned earlier, TCC interfaces can be RPC or internal JVM calls. In the Seata TCC module, there is a remoting module that is specifically used to parse TCC interfaces with the `TwoPhaseBusinessAction` annotation:\\n\\n<img src=\\"/img/blog/20220116175059.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nThe `RemotingParser` interface mainly has methods such as `isRemoting`, `isReference`, `isService`, `getServiceDesc`, etc. The default implementation is `DefaultRemotingParser`, and the parsing of various RPC protocols is executed in `DefaultRemotingParser`. Seata has already implemented parsing of Dubbo, HSF, SofaRpc, and LocalTCC RPC protocols while also providing SPI extensibility for additional RPC protocol parsing classes.\\n\\nDuring the Seata startup process, the `GlobalTransactionScanner` annotation is used for scanning and executes the following method:\\n\\n`io.seata.spring.util.TCCBeanParserUtils#isTccAutoProxy`\\n\\nThe purpose of this method is to determine if the bean has been TCC proxied. In the process, it first checks if the bean is a Remoting bean. If it is, it calls the `getServiceDesc` method to parse the remoting bean, and if it is a sender, it registers the resource:\\n\\nio.seata.rm.tcc.remoting.parser.DefaultRemotingParser#parserRemotingServiceInfo\\n\\n```java\\npublic RemotingDesc parserRemotingServiceInfo(Object bean, String beanName, RemotingParser remotingParser){\\n    RemotingDesc remotingBeanDesc = remotingParser.getServiceDesc(bean, beanName);\\n    if(remotingBeanDesc == null){\\n    return null;\\n    }\\n    remotingServiceMap.put(beanName, remotingBeanDesc);\\n\\n    Class<?> interfaceClass = remotingBeanDesc.getInterfaceClass();\\n    Method[] methods = interfaceClass.getMethods();\\n    if (remotingParser.isService(bean, beanName)) {\\n        try {\\n            //service bean, registry resource\\n            Object targetBean = remotingBeanDesc.getTargetBean();\\n            for (Method m : methods) {\\n                TwoPhaseBusinessAction twoPhaseBusinessAction = m.getAnnotation(TwoPhaseBusinessAction.class);\\n                if (twoPhaseBusinessAction != null) {\\n                    TCCResource tccResource = new TCCResource();\\n                    tccResource.setActionName(twoPhaseBusinessAction.name());\\n                    tccResource.setTargetBean(targetBean);\\n                    tccResource.setPrepareMethod(m);\\n                    tccResource.setCommitMethodName(twoPhaseBusinessAction.commitMethod());\\n                    tccResource.setCommitMethod(interfaceClass.getMethod(twoPhaseBusinessAction.commitMethod(),\\n                    twoPhaseBusinessAction.commitArgsClasses()));\\n                    tccResource.setRollbackMethodName(twoPhaseBusinessAction.rollbackMethod());\\n                    tccResource.setRollbackMethod(interfaceClass.getMethod(twoPhaseBusinessAction.rollbackMethod(),\\n                    twoPhaseBusinessAction.rollbackArgsClasses()));\\n                    // set argsClasses\\n                    tccResource.setCommitArgsClasses(twoPhaseBusinessAction.commitArgsClasses());\\n                    tccResource.setRollbackArgsClasses(twoPhaseBusinessAction.rollbackArgsClasses());\\n                    // set phase two method\'s keys\\n                    tccResource.setPhaseTwoCommitKeys(this.getTwoPhaseArgs(tccResource.getCommitMethod(),\\n                    twoPhaseBusinessAction.commitArgsClasses()));\\n                    tccResource.setPhaseTwoRollbackKeys(this.getTwoPhaseArgs(tccResource.getRollbackMethod(),\\n                    twoPhaseBusinessAction.rollbackArgsClasses()));\\n                    // registry tcc resource\\n                    DefaultResourceManager.get().registerResource(tccResource);\\n                }\\n            }\\n        } catch (Throwable t) {\\n            throw new FrameworkException(t, \\"parser remoting service error\\");\\n        }\\n    }\\n    if (remotingParser.isReference(bean, beanName)) {\\n        // reference bean, TCC proxy\\n        remotingBeanDesc.setReference(true);\\n    }\\n    return remotingBeanDesc;\\n    }\\n```\\n\\nThe above method first calls the parsing class `getServiceDesc` method to parse the remoting bean and puts the parsed `remotingBeanDesc` into the local cache `remotingServiceMap`. At the same time, it calls the parsing class `isService` method to determine if it is the initiator. If it is the initiator, it parses the content of the `TwoPhaseBusinessAction` annotation to generate a `TCCResource` and registers it as a resource.\\n\\n## Resource Management\\n\\n**1. Resource Registration**\\n\\nThe resource for Seata TCC mode is called `TCCResource`, and its resource manager is called `TCCResourceManager`. As mentioned earlier, after parsing the TCC interface RPC resource, if it is the initiator, it will be registered as a resource:\\n\\nio.seata.rm.tcc.TCCResourceManager#registerResource\\n\\n```java\\npublic void registerResource(Resource resource){\\n    TCCResource tccResource=(TCCResource)resource;\\n    tccResourceCache.put(tccResource.getResourceId(),tccResource);\\n    super.registerResource(tccResource);\\n    }\\n```\\n\\n`TCCResource` contains the relevant information of the TCC interface and is cached locally. It continues to call the parent class `registerResource` method (which encapsulates communication methods) to register with the TC. The TCC resource\'s resourceId is the actionName, and the actionName is the name in the `@TwoParseBusinessAction` annotation.\\n\\n**2. Resource Commit/Rollback**\\n\\nio.seata.rm.tcc.TCCResourceManager#branchCommit\\n\\n```java\\npublic BranchStatus branchCommit(BranchType branchType,String xid,long branchId,String resourceId,\\n    String applicationData)throws TransactionException{\\n    TCCResource tccResource=(TCCResource)tccResourceCache.get(resourceId);\\n    if(tccResource==null){\\n    throw new ShouldNeverHappenException(String.format(\\"TCC resource is not exist, resourceId: %s\\",resourceId));\\n    }\\n    Object targetTCCBean=tccResource.getTargetBean();\\n    Method commitMethod=tccResource.getCommitMethod();\\n    if(targetTCCBean==null||commitMethod==null){\\n    throw new ShouldNeverHappenException(String.format(\\"TCC resource is not available, resourceId: %s\\",resourceId));\\n    }\\n    try{\\n    //BusinessActionContext\\n    BusinessActionContext businessActionContext=getBusinessActionContext(xid,branchId,resourceId,\\n    applicationData);\\n    // ... ...\\n    ret=commitMethod.invoke(targetTCCBean,args);\\n    // ... ...\\n    return result?BranchStatus.PhaseTwo_Committed:BranchStatus.PhaseTwo_CommitFailed_Retryable;\\n    }catch(Throwable t){\\n    String msg=String.format(\\"commit TCC resource error, resourceId: %s, xid: %s.\\",resourceId,xid);\\n    LOGGER.error(msg,t);\\n    return BranchStatus.PhaseTwo_CommitFailed_Retryable;\\n    }\\n    }\\n```\\n\\nWhen the TM resolves the phase two commit, the TC will callback to the corresponding participant (i.e., TCC interface initiator) service to execute the Confirm/Cancel method of the TCC Resource registered by the branch.\\n\\nIn the resource manager, the corresponding `TCCResource` will be found in the local cache based on the resourceId, and the corresponding `BusinessActionContext` will be found based on xid, branchId, resourceId, and applicationData, and the parameters to be executed are in the context. Finally, the commit method of the `TCCResource` is executed to perform the phase two commit.\\n\\nThe phase two rollback is similar.\\n\\n## Transaction Processing\\n\\nAs mentioned earlier, if the TCC interface is a caller, the Seata TCC proxy will be used to intercept the caller and register the branch before processing the actual RPC method call.\\n\\nThe method `io.seata.spring.util.TCCBeanParserUtils#isTccAutoProxy` not only parses the TCC interface resources, but also determines whether the TCC interface is a caller. If it is a caller, it returns true:\\n\\nio.seata.spring.annotation.GlobalTransactionScanner#wrapIfNecessary\\n\\n<img src=\\"/img/blog/20220116192544.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nAs shown in the figure, when `GlobalTransactionalScanner` scans the TCC interface caller (Reference), it will proxy and intercept it with `TccActionInterceptor`, which implements `MethodInterceptor`.\\n\\nIn `TccActionInterceptor`, it will also call `ActionInterceptorHandler` to execute the interception logic, and the transaction-related processing is in the `ActionInterceptorHandler#proceed` method:\\n\\n```java\\npublic Object proceed(Method method, Object[] arguments, String xid, TwoPhaseBusinessAction businessAction,\\n    Callback<Object> targetCallback) throws Throwable {\\n    //Get action context from arguments, or create a new one and then reset to arguments\\n    BusinessActionContext actionContext = getOrCreateActionContextAndResetToArguments(method.getParameterTypes(), arguments);\\n    //Creating Branch Record\\n    String branchId = doTccActionLogStore(method, arguments, businessAction, actionContext);\\n    // ... ...\\n    try {\\n    // ... ...\\n    return targetCallback.execute();\\n    } finally {\\n    try {\\n    //to report business action context finally if the actionContext.getUpdated() is true\\n    BusinessActionContextUtil.reportContext(actionContext);\\n    } finally {\\n    // ... ...\\n    }\\n    }\\n}\\n```\\n\\nIn the process of executing the first phase of the TCC interface, the `doTccActionLogStore` method is called for branch registration, and the TCC-related information such as parameters is placed in the context. This context will be used for resource submission/rollback as mentioned above.\\n\\n# How to control exceptions\\n\\nIn the process of executing the TCC model, various exceptions may occur, the most common of which are empty rollback, idempotence, and suspense. Here I will explain how Seata handles these three types of exceptions.\\n\\n## How to handle empty rollback\\n\\nWhat is an empty rollback?\\n\\nAn empty rollback refers to a situation in a distributed transaction where the TM drives the second-phase rollback of the participant\'s Cancel method without calling the participant\'s Try method.\\n\\nHow does an empty rollback occur?\\n\\n<img src=\\"/img/blog/20220116201900.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nAs shown in the above figure, after the global transaction is opened, participant A will execute the first-phase RPC method after completing branch registration. If the machine where participant A is located crashes or there is a network anomaly at this time, the RPC call will fail, meaning that participant A\'s first-phase method did not execute successfully. However, the global transaction has already been opened, so Seata must progress to the final state. When the global transaction is rolled back, participant A\'s Cancel method will be called, resulting in an empty rollback.\\n\\nTo prevent empty rollback, it is necessary to identify it in the Cancel method. How does Seata do this?\\n\\nSeata\'s approach is to add a TCC transaction control table, which contains the XID and BranchID information of the transaction. A record is inserted when the Try method is executed, indicating that phase one has been executed. When the Cancel method is executed, this record is read. If the record does not exist, it means that the Try method was not executed.\\n\\n## How to Handle Idempotent Operations\\n\\nIdempotent operation refers to TC repeating the two-phase commit, so the Confirm/Cancel interface needs to support idempotent processing, which means that it will not cause duplicate resource submission or release.\\n\\nSo how does idempotent operation arise?\\n\\n<img src=\\"/img/blog/20220116203816.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nAs shown in the above figure, after participant A completes the two phases, network jitter or machine failure may cause TC not to receive the return result of participant A\'s execution of the two phases. TC will continue to make repeated calls until the two-phase execution result is successful.\\n\\nHow does Seata handle idempotent operations?\\n\\nSimilarly, a status field is added to the TCC transaction control table. This field has 3 values:\\n\\n1. tried: 1\\n2. committed: 2\\n3. rollbacked: 3\\n\\nAfter the execution of the two-phase Confirm/Cancel method, the status is changed to committed or rollbacked. When the two-phase Confirm/Cancel method is called repeatedly, checking the transaction status can solve the idempotent problem.\\n\\n## How to Handle Suspend\\n\\nSuspension refers to the two-phase Cancel method being executed before the phase Try method, because empty rollback is allowed. After the execution of the two-phase Cancel method, directly returning success, the global transaction has ended. However, because the Try method is executed later, this will cause the resources reserved by the phase Try method to never be committed or released.\\n\\nSo how does suspension arise?\\n\\n<img src=\\"/img/blog/20220116205241.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nAs shown in the above figure, when participant A\'s phase Try method is executed, network congestion occurs, and due to Seata\'s global transaction timeout limit, after the Try method times out, TM resolves to roll back the global transaction. After the rollback is completed, if the RPC request arrives at participant A at this time and the Try method is executed to reserve resources, it will cause suspension.\\n\\nHow does Seata handle suspension?\\n\\nAdd a status to the TCC transaction control table:\\n\\n1. suspended: 4\\n\\nWhen the two-phase Cancel method is executed, if it is found that there is no related record in the TCC transaction control table, it means that the two-phase Cancel method is executed before the phase Try method. Therefore, a record with status=4 is inserted. Then, when the phase Try method is executed, if status=4 is encountered, it means that the two-phase Cancel has been executed, and false is returned to prevent the phase Try method from succeeding.\\n\\n# Author Introduction\\n\\nZhang Chenghui, currently working at Ant Group, loves to share technology. He is the author of the WeChat public account \\"Advanced Backend,\\" the author of the technical blog (https://objcoding.com/), and his GitHub ID is: objcoding."},{"id":"/seata-at-lock","metadata":{"permalink":"/blog/seata-at-lock","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-at-lock.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-at-lock.md","title":"In-Depth Analysis of Seata AT Mode Transaction Isolation Levels and Global Lock Design","description":"The transaction isolation in Seata AT mode is built on the basis of local isolation levels of supporting transactions. Assuming a database local isolation level of Read Committed or higher, Seata designs a global write-exclusive lock maintained by the transaction coordinator to ensure write isolation between transactions. Meanwhile, the default isolation level for global transactions is defined at Read Uncommitted.","date":"2022-01-12T00:00:00.000Z","formattedDate":"January 12, 2022","tags":[],"readingTime":7.015,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"In-Depth Analysis of Seata AT Mode Transaction Isolation Levels and Global Lock Design","author":"chenghui.zhang","keywords":["Seata","distributed transaction","AT mode","Transaction","GlobalLock"],"description":"The transaction isolation in Seata AT mode is built on the basis of local isolation levels of supporting transactions. Assuming a database local isolation level of Read Committed or higher, Seata designs a global write-exclusive lock maintained by the transaction coordinator to ensure write isolation between transactions. Meanwhile, the default isolation level for global transactions is defined at Read Uncommitted.","date":"2022/01/12"},"unlisted":false,"prevItem":{"title":"In-Depth Analysis of Seata TCC Mode (1)","permalink":"/blog/seata-tcc"},"nextItem":{"title":"Q&A on the New Version of Snowflake Algorithm","permalink":"/blog/seata-snowflake-explain"}},"content":"Seata AT mode is a non-intrusive distributed transaction solution. Seata internally implements a proxy layer for database operations. When using Seata AT mode, we actually use the built-in data source proxy DataSourceProxy provided by Seata. Seata adds a lot of logic in this proxy layer, such as inserting rollback undo_log records and checking global locks.\\n\\nWhy check global locks? This is because the transaction isolation of Seata AT mode is based on the local isolation level of supporting transactions. Under the premise of database local isolation level of read committed or above, Seata designs a global write exclusive lock maintained by the transaction coordinator to ensure write isolation between transactions. At the same time, global transactions are by default defined at the read uncommitted isolation level.\\n\\n# Understanding Seata Transaction Isolation Levels\\n\\nBefore discussing Seata transaction isolation levels, let\'s review the isolation levels of database transactions. Currently, there are four types of database transaction isolation levels, from lowest to highest:\\n\\n1. Read uncommitted\\n2. Read committed\\n3. Repeatable read\\n4. Serializable\\n\\nThe default isolation level for databases is usually read committed, such as Oracle, while some databases default to repeatable read, such as MySQL. Generally, the read committed isolation level of databases can satisfy the majority of business scenarios.\\n\\nWe know that a Seata transaction is a global transaction, which includes several local transaction branches. During the execution of a global transaction (before the global transaction is completed), if a local transaction commits and Seata does not take any measures, it may lead to reading of committed local transactions, causing dirty reads. If a local transaction that has been committed before the global transaction commits is modified, it may cause dirty writes.\\n\\nFrom this, we can see that traditional dirty reads involve reading uncommitted data, while Seata\'s dirty reads involve reading data that has not been committed under the global transaction, where the global transaction may include multiple local transactions. The fact that one local transaction commits does not mean that the global transaction commits.\\n\\nWorking under the read committed isolation level is fine for the vast majority of applications. In fact, the majority of scenarios that work under the read uncommitted isolation level also work fine.\\n\\nIn extreme scenarios, if an application needs to achieve global read committed, Seata also provides a global lock mechanism to implement global transaction read committed. However, by default, Seata\'s global transactions work under the read uncommitted isolation level to ensure efficiency in the majority of scenarios.\\n\\n# Implementation of Global Locks\\n\\nIn AT mode, Seata uses the internal data source proxy DataSourceProxy, and the implementation of global locks is hidden within this proxy. Let\'s see what happens during the execution and submission processes.\\n\\n## 1. Execution Process\\n\\nThe execution process is in the StatementProxy class. During execution, if the executed SQL is `select for update`, the SelectForUpdateExecutor class is used. If the executed method is annotated with `@GlobalTransactional` or `@GlobalLock`, it checks if there is a global lock. If a global lock exists, it rolls back the local transaction and continuously competes to obtain local and global locks through a while loop.\\n\\n\\nio.seata.rm.datasource.exec.SelectForUpdateExecutor#doExecute\\n\\n```java\\npublic T doExecute(Object... args) throws Throwable {\\n    Connection conn = statementProxy.getConnection();\\n    // ... ...\\n    try {\\n        // ... ...\\n        while (true) {\\n            try {\\n                // ... ...\\n                if (RootContext.inGlobalTransaction() || RootContext.requireGlobalLock()) {\\n                    // Do the same thing under either @GlobalTransactional or @GlobalLock, \\n                    // that only check the global lock  here.\\n                    statementProxy.getConnectionProxy().checkLock(lockKeys);\\n                } else {\\n                    throw new RuntimeException(\\"Unknown situation!\\");\\n                }\\n                break;\\n            } catch (LockConflictException lce) {\\n                if (sp != null) {\\n                    conn.rollback(sp);\\n                } else {\\n                    conn.rollback();\\n                }\\n                // trigger retry\\n                lockRetryController.sleep(lce);\\n            }\\n        }\\n    } finally {\\n        // ...\\n    }\\n```\\n\\n## 2. Submission Process\\n\\nThe submission process occurs in the doCommit method of ConnectionProxy.\\n\\n1) If the executed method is annotated with `@GlobalTransactional`, it will acquire the global lock during branch registration:\\n\\n- Requesting TC to register a branch\\n\\nio.seata.rm.datasource.ConnectionProxy#register\\n\\n```java\\nprivate void register() throws TransactionException {\\n    if (!context.hasUndoLog() || !context.hasLockKey()) {\\n        return;\\n    }\\n    Long branchId = DefaultResourceManager.get().branchRegister(BranchType.AT, getDataSourceProxy().getResourceId(),\\n                                                                null, context.getXid(), null, context.buildLockKeys());\\n    context.setBranchId(branchId);\\n}\\n```\\n\\n- When a TC registers a branch, it obtains a global lock\\n\\nio.seata.server.transaction.at.ATCore#branchSessionLock\\n\\n```java\\nprotected void branchSessionLock(GlobalSession globalSession, BranchSession branchSession) throws TransactionException {\\n    if (!branchSession.lock()) {\\n        throw new BranchTransactionException(LockKeyConflict, String\\n                                             .format(\\"Global lock acquire failed xid = %s branchId = %s\\", globalSession.getXid(),\\n                                                     branchSession.getBranchId()));\\n    }\\n}\\n```\\n\\n2\uff09If the execution method has a \'@GlobalLock\' annotation, the global lock is checked for existence before committing, and if it does, an exception is thrown:\\n\\nio.seata.rm.datasource.ConnectionProxy#processLocalCommitWithGlobalLocks\\n\\n```java\\nprivate void processLocalCommitWithGlobalLocks() throws SQLException {\\n    checkLock(context.buildLockKeys());\\n    try {\\n        targetConnection.commit();\\n    } catch (Throwable ex) {\\n        throw new SQLException(ex);\\n    }\\n    context.reset();\\n}\\n```\\n\\n## GlobalLock Annotation Explanation\\n\\nFrom the execution process and submission process, it can be seen that since opening a global transaction with the `@GlobalTransactional` annotation can check if the global lock exists before transaction submission, why does Seata still provide a `@GlobalLock` annotation?\\n\\nThis is because not all database operations require opening a global transaction, and opening a global transaction is a relatively heavy operation that involves initiating RPC processes to TC. The `@GlobalLock` annotation only checks the existence of the global lock during the execution process and does not initiate a global transaction. Therefore, when there is no need for a global transaction but the global lock needs to be checked to avoid dirty reads and writes, using the `@GlobalLock` annotation is a lighter operation.\\n\\n# How to Prevent Dirty Writes\\n\\nLet\'s first understand how dirty writes occur when using Seata AT mode:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20211226164628.png)\\n\\n*Note: Other processes in the branch transaction execution are omitted.*\\n\\nWhen Business One starts a global transaction containing branch transaction A (modifying A) and branch transaction B (modifying B), Business Two modifies A. Business One\'s branch transaction A obtains a local lock before Business Two, waiting for Business One to complete the execution of branch transaction A. Business Two then obtains the local lock, modifies A, and commits it to the database. However, Business One encounters an exception during the execution of branch transaction A. Since the data of branch transaction A has been modified by Business Two, Business One\'s global transaction cannot be rolled back.\\n\\nHow to prevent dirty writes?\\n\\n1. Business Two uses `@GlobalTransactional` annotation:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20211226210337.png)\\n\\n*Note: Other processes in the branch transaction execution are omitted.*\\n\\nDuring the execution of the global transaction by Business Two, when registering the branch transaction before the submission of branch transaction A and acquiring the global lock, it finds that Business One\'s global lock has not been released yet. Therefore, Business Two cannot commit and throws an exception to roll back, thus preventing dirty writes.\\n\\n2. Business Two uses `@GlobalLock` annotation:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20211226210502.png)\\n\\n*Note: Other processes in the branch transaction execution are omitted.*\\n\\nSimilar to the effect of `@GlobalTransactional` annotation, but without the need to open a global transaction, it only checks the existence of the global lock before local transaction submission.\\n\\n3. Business Two uses `@GlobalLock` annotation + `select for update` statement:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20211226172358.png)\\n\\nIf a `select for update` statement is added, it checks the existence of the global lock before the update operation. Business Two can only execute the updateA operation after the global lock is released.\\n\\nIf only `@Transactional` is used, there is a possibility of dirty writes. The fundamental reason is that without the GlobalLock annotation, the global lock is not checked, which may lead to another global transaction finding that a branch transaction has been modified when rolling back. Therefore, adding `select for update` also has a benefit, which is that it allows for retries.\\n\\n\\n# How to Prevent Dirty Reads\\n\\nDirty reads in Seata AT mode refer to the scenario where data from a branch transaction that has been committed is read by another business before the global transaction is committed. Essentially, this is because Seata\'s default global transaction isolation level is read uncommitted.\\n\\nSo how to prevent dirty reads?\\n\\nBusiness Two queries A with `@GlobalLock` annotation + `select for update` statement:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20211226210633.png)\\n\\nAdding the `select for update` statement checks the existence of the global lock before executing the SQL. The SQL can only be executed after the global lock is acquired, thus preventing dirty reads.\\n\\n# Author Bio:\\n\\nZhang Chenghui currently works at Ant Group and is passionate about sharing technology. He is the author of the WeChat public account \\"\u540e\u7aef\u8fdb\u9636\\" (Backend Advancement) and the owner of the technical blog (https://objcoding.com/). He is also a Seata Contributor with GitHub ID: objcoding."},{"id":"/seata-snowflake-explain","metadata":{"permalink":"/blog/seata-snowflake-explain","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-snowflake-explain.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-snowflake-explain.md","title":"Q&A on the New Version of Snowflake Algorithm","description":"In the previous analysis of the new version of the Snowflake algorithm, we mentioned two changes made in the new version:","date":"2021-06-21T00:00:00.000Z","formattedDate":"June 21, 2021","tags":[],"readingTime":7.315,"hasTruncateMarker":false,"authors":[{"name":"selfishlover"}],"frontMatter":{"title":"Q&A on the New Version of Snowflake Algorithm","author":"selfishlover","keywords":["Seata","snowflake","UUID","page split"],"date":"2021/06/21"},"unlisted":false,"prevItem":{"title":"In-Depth Analysis of Seata AT Mode Transaction Isolation Levels and Global Lock Design","permalink":"/blog/seata-at-lock"},"nextItem":{"title":"Analysis of Seata\'s Distributed UUID Generator Based on Improved Snowflake Algorithm","permalink":"/blog/seata-analysis-UUID-generator"}},"content":"In the previous analysis of the new version of the Snowflake algorithm, we mentioned two changes made in the new version:\\n1. The timestamp no longer constantly follows the system clock.\\n2. The exchange of positions between node ID and timestamp. From the original:\\n   ![Original Bit Allocation Strategy](/img/blog/seata/uuid/before.png)\\n   to:\\n   ![Improved Bit Allocation Strategy](/img/blog/seata/uuid/after.png)\\n\\nA careful student raised a question: In the new version, the algorithm is indeed monotonically increasing within a single node, but in a multi-instance deployment, it is no longer globally monotonically increasing! Because it is obvious that the node ID is in the high bits, so the generated ID with a larger node ID will definitely be greater than the ID with a smaller node ID, regardless of the chronological order. In contrast, the original algorithm, with the timestamp in the high bits and always following the system clock, can ensure that IDs generated earlier are smaller than those generated later. Only when two nodes happen to generate IDs at the same timestamp, the order of the two IDs is determined by the node ID. So, does it mean that the new version of the algorithm is wrong?\\n\\nThis is a great question! The fact that students can raise this question indicates a deep understanding of the essential differences between the standard Snowflake algorithm and the new version. This is commendable! Here, let\'s first state the conclusion: indeed, the new version of the algorithm does not possess global monotonicity, but this does not affect our original intention (to reduce database page splits). This conclusion may seem counterintuitive but can be proven.\\n\\nBefore providing the proof, let\'s briefly review some knowledge about page splits in databases. Taking the classic MySQL InnoDB as an example, InnoDB uses a B+ tree index where the leaf nodes of the primary key index also store the complete records of data rows. The leaf nodes are linked together in the form of a doubly linked list. The physical storage form of the leaf nodes is a data page, and each data page can store up to N rows of records (where N is inversely proportional to the size of each row). As shown in the diagram:\\n![Data Page](/img/blog/seata/uuid/page1.png)\\nThe characteristics of the B+ tree require that the left node should be smaller than the right node. What happens if we want to insert a record with an ID of 25 at this point (assuming each data page can only hold 4 records)? The answer is that it will cause a page split, as shown in the diagram:\\n![Page Split](/img/blog/seata/uuid/page2.png)\\nPage splits are unfriendly to I/O, requiring the creation of new data pages, copying and transferring part of the records from the old data page, etc., and should be avoided as much as possible.\\n\\nIdeally, the primary key ID should be sequentially increasing (for example, setting the primary key as auto_increment). This way, a new page will only be needed when the current data page is full, and the doubly linked list will always grow sequentially at the tail, avoiding any mid-node splits.\\n\\nIn the worst-case scenario, if the primary key ID is randomly generated and unordered (for example, a UUID string in Java), new records will be randomly assigned to any data page. If the page is already full, it will trigger a page split.\\n\\nIf the primary key ID is generated by the standard Snowflake algorithm, in the best-case scenario, only one node is generating IDs within each timestamp. In this case, the algorithm\'s effect is equivalent to the ideal situation of sequential incrementation, similar to auto_increment. In the worst-case scenario, all nodes within each timestamp are generating IDs, and the algorithm\'s effect is close to unordered (but still much better than completely unordered UUIDs, as the workerId with only 10 bits limits the nodes to a maximum of 1024). In actual production, the algorithm\'s effectiveness depends on business traffic, and the lower the concurrency, the closer the algorithm is to the ideal scenario.\\n\\nSo, how does it fare with the new version of the algorithm?  \\n\\n\\nThe new version of the algorithm, from a global perspective, produces IDs in an unordered manner. However, for each workerId, the generated IDs are strictly monotonically increasing. Additionally, since workerId is finite, it can divide into a maximum of 1024 subsequences, each of which is monotonically increasing.\\n\\nFor a database, initially, the received IDs may be unordered, coming from various subsequences, as illustrated here:\\n![Initial State](/img/blog/seata/uuid/page3.png)\\n\\nIf, at this point, a worker1-seq2 arrives, it will clearly cause a page split:\\n![First Split](/img/blog/seata/uuid/page4.png)\\n\\nHowever, after the split, interesting things happen. For worker1, subsequent seq3, seq4 will not cause page splits anymore (because there is still space), and seq5 only needs to link to a new page for sequential growth (the difference is that this new page is not at the tail of the doubly linked list). Note that the subsequent IDs of worker1 will not be placed after any nodes from worker2 or beyond (thus avoiding page splits for later nodes) because they are always smaller than the IDs of worker2; nor will they be placed before the current node of worker1 (thus avoiding page splits for previous nodes) because the subsequences of worker1 are always monotonically increasing. Here, we refer to such subsequences as reaching a steady state, meaning that the subsequence has \\"stabilized,\\" and its subsequent growth will only occur at the end of the subsequence without causing page splits for other nodes.\\n\\nThe same principle can be extended to all subsequences. Regardless of how chaotic the IDs received by the database are initially, after a finite number of page splits, the doubly linked list can always reach a stable state:\\n![Steady State](/img/blog/seata/uuid/page5.png)\\n\\nAfter reaching the steady state, subsequent IDs will only grow sequentially within their respective subsequences, without causing page splits. The difference between this sequential growth and the sequential growth of auto_increment is that the former has 1024 growth points (the ends of various subsequences), while the latter only has one at the end.\\n\\nAt this point, we can answer the question posed at the beginning: indeed, the new algorithm is not globally monotonically increasing, but the algorithm **converges**. After reaching a steady state, the new algorithm can achieve the same effect as global sequential incrementation.\\n\\n## Further Considerations\\n\\nThe discussion so far has focused on the continuous growth of sequences. However, in practical production, there is not only the insertion of new data but also the deletion of old data. Data deletion may lead to page merging (InnoDB, if it finds that the space utilization of two adjacent data pages is both less than 50%, it will merge them). How does this affect the new algorithm?\\n\\nAs we have seen in the above process, the essence of the new algorithm is to utilize early page splits to gradually separate different subsequences, allowing the algorithm to continuously converge to a steady state. Page merging, on the other hand, may reverse this process by merging different subsequences back into the same data page, hindering the convergence of the algorithm. Especially in the early stages of convergence, frequent page merging may even prevent the algorithm from converging forever (I just separated them, and now I\'m merging them back together, back to square one~)! However, after convergence, only page merging at the end nodes of each subsequence has the potential to disrupt the steady state (merging the end node of one subsequence with the head node of the next subsequence). Merging on the remaining nodes of the subsequence does not affect the steady state because the subsequence remains ordered, albeit with a shorter length.\\n\\nTaking Seata\'s server as an example, the data in the three tables of the server has a relatively short lifecycle. After a global transaction ends, the data is cleared. This is not friendly to the new algorithm, as it does not provide enough time for convergence. However, there is already a pull request (PR) for delayed deletion in the review process, and with this PR, the effect will be much better. For example, periodic weekly cleanup allows sufficient time for the algorithm to converge in the early stages, and for most of the time, the database can benefit from it. At the time of cleanup, the worst-case result is that the table is cleared, and the algorithm starts from scratch.\\n\\nIf you wish to apply the new algorithm to a business system, make sure to ensure that the algorithm has time to converge. For example, for user tables or similar, where data is intended to be stored for a long time, the algorithm can naturally converge. Alternatively, implement a mechanism for delayed deletion, providing enough time for the algorithm to converge.\\n\\nIf you have better opinions and suggestions, feel free to contact the Seata community!"},{"id":"/seata-analysis-UUID-generator","metadata":{"permalink":"/blog/seata-analysis-UUID-generator","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-UUID-generator.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-UUID-generator.md","title":"Analysis of Seata\'s Distributed UUID Generator Based on Improved Snowflake Algorithm","description":"Seata incorporates a distributed UUID generator to assist in generating global transaction IDs and branch transaction IDs. The desired characteristics for this generator include high performance, global uniqueness, and trend incrementation.","date":"2021-05-08T00:00:00.000Z","formattedDate":"May 8, 2021","tags":[],"readingTime":5.71,"hasTruncateMarker":false,"authors":[{"name":"selfishlover"}],"frontMatter":{"title":"Analysis of Seata\'s Distributed UUID Generator Based on Improved Snowflake Algorithm","author":"selfishlover","keywords":["Seata","snowflake","UUID"],"date":"2021/05/08"},"unlisted":false,"prevItem":{"title":"Q&A on the New Version of Snowflake Algorithm","permalink":"/blog/seata-snowflake-explain"},"nextItem":{"title":"Seata New Feature Support -- Undo_Log Compression","permalink":"/blog/seata-feature-undo-log-compress"}},"content":"Seata incorporates a distributed UUID generator to assist in generating global transaction IDs and branch transaction IDs. The desired characteristics for this generator include high performance, global uniqueness, and trend incrementation.\\n\\nHigh performance is self-explanatory, and global uniqueness is crucial to prevent confusion between different global transactions or branch transactions. Additionally, trend incrementation is valuable for users employing databases as the storage tool for TC clusters, as it can reduce the frequency of data page splits, thereby minimizing database IO pressure (the `branch_table` table uses the branch transaction ID as the primary key).\\n\\nIn the older version of Seata (prior to 1.4), the implementation of this generator was based on the standard version of the Snowflake algorithm. The standard Snowflake algorithm has been well-documented online, so we won\'t delve into it here. If you\'re unfamiliar with it, consider referring to existing resources before continuing with this article.\\n\\nHere, we discuss some drawbacks of the standard Snowflake algorithm:\\n1. **Clock Sensitivity:** Since ID generation is always tied to the current operating system\'s timestamp (leveraging the monotonicity of time), a clock rollback may result in repeated IDs. Seata\'s strategy to handle this is by recording the last timestamp and rejecting service if the current timestamp is less than the recorded value (indicating a clock rollback). The service waits until the timestamp catches up with the recorded value. However, this means the TC will be in an unavailable state during this period.\\n\\n2. **Burst Performance Limit:** The standard Snowflake algorithm claims a high QPS, approximately 4 million/s. However, strictly speaking, this is a bit misleading. The timestamp unit of the algorithm is milliseconds, and the bit length allocated to the sequence number is 12, allowing for 4096 sequence spaces per millisecond. So, a more accurate description would be 4096/ms. The distinction between 4 million/s and 4096/ms lies in the fact that the former doesn\'t require every millisecond\'s concurrency to be below 4096. Seata also adheres to this limitation. If the sequence space for the current timestamp is exhausted, it will spin-wait for the next timestamp.\\n\\nIn newer versions (1.4 and beyond), the generator has undergone optimizations and improvements to address these issues effectively. The core idea of the improvement is to decouple from the operating system\'s timestamp, with the generator obtaining the system\'s current timestamp only during initialization as the initial timestamp. Subsequently, it no longer synchronizes with the system timestamp. The incrementation is solely driven by the incrementation of the sequence number. For example, when the sequence number reaches its maximum value (4095), the next request causes an overflow of the 12-bit space. The sequence number resets to zero, and the overflow carry is added to the timestamp, incrementing it by 1. Thus, the timestamp and sequence number can be considered as a single entity. In practice, we adjusted the bit allocation strategy for the 64-bit ID, swapping the positions of the timestamp and node ID for easier handling of this overflow carry:\\n\\nOriginal Bit Allocation Strategy:\\n![Original Bit Allocation Strategy](/img/blog/seata/uuid/before.png)\\n\\nModified Bit Allocation Strategy (swapping timestamp and node ID):\\n![Modified Bit Allocation Strategy](/img/blog/seata/uuid/after.png)\\n\\nThis arrangement allows the timestamp and sequence number to be contiguous in memory, making it easy to use an `AtomicLong` to simultaneously store them.\\n\\n```\\n/**\\n * timestamp and sequence mix in one Long\\n * highest 11 bit: not used\\n * middle  41 bit: timestamp\\n * lowest  12 bit: sequence\\n */\\nprivate AtomicLong timestampAndSequence;\\n```\\nThe highest 11 bits can be determined during initialization and remain unchanged thereafter:\\n```\\n/**\\n * business meaning: machine ID (0 ~ 1023)\\n * actual layout in memory:\\n * highest 1 bit: 0\\n * middle 10 bit: workerId\\n * lowest 53 bit: all 0\\n */\\nprivate long workerId;\\n```\\nProducing an ID is then straightforward\uff1a\\n```\\npublic long nextId() {\\n   // Obtain the incremented timestamp and sequence number\\n   long next = timestampAndSequence.incrementAndGet();\\n   // Extract the lowest 53 bits\\n   long timestampWithSequence = next & timestampAndSequenceMask;\\n   // Perform a bitwise OR operation with the previously saved top 11 bits\\n   return workerId | timestampWithSequence;\\n}\\n```\\n\\nAt this point, we can observe the following:\\n\\n1. The generator no longer has a burst performance limit of 4096/ms. If the sequence number space for a timestamp is exhausted, it will directly advance to the next timestamp, \\"borrowing\\" the sequence number space of the next timestamp (there is no need to worry about serious consequences of this \\"advance consumption,\\" as the reasons will be explained below).\\n\\n2. The generator has a weak dependency on the operating system clock. During runtime, the generator is not affected by clock backtracking (whether it is manually backtracked or due to machine clock drift) because the generator only fetches the system clock once at startup, and thereafter, they no longer stay synchronized. The only possible scenario for duplicate IDs is a significant clock backtracking during restart (either deliberate human backtracking or modification of the operating system time zone, such as changing Beijing time to London time~ Machine clock drift is typically in the millisecond range and won\'t have such a large impact).\\n\\n3. Will continuous \\"advance consumption\\" cause the generator\'s timestamps to be significantly ahead of the system timestamps, resulting in ID duplicates upon restart? In theory, yes, but practically almost impossible. To achieve this effect, it would mean that the generator\'s QPS received must be consistently stable at over 400w/s~ To be honest, even TC can\'t handle such high traffic, so, the bottleneck is definitely not in the generator.\\n\\nIn addition, we also adjusted the strategy for generating node IDs. In the original version, when the user did not manually specify a node ID, it would take the low 10 bits of the local IPv4 address as the node ID. In practical production, it was found that there were occasional occurrences of duplicate node IDs (mostly users deploying with k8s). For example, the following IPs would result in duplicates:\\n- 192.168.4.10\\n- 192.168.8.10\\n\\nMeaning, as long as the low 2 bits of the fourth byte and the third byte of the IP are the same, duplicates would occur. The new version\'s strategy is to prioritize taking the low 10 bits from the MAC address of the local network card. If the local machine does not have a valid network card configuration, it randomly picks one from [0, 1023] as the node ID. After this adjustment, it seems that new version users are no longer reporting the same issue (of course, it remains to be tested over time, but in any case, it won\'t be worse than the IP extraction strategy).\\n\\nThe above is a brief analysis of Seata\'s distributed UUID generator. If you find this generator useful, you can directly use it in your project. Its class declaration is `public`, and the full class name is:\\n`io.seata.common.util.IdWorker`\\n\\nOf course, if you have better ideas, you are also welcome to discuss them with the Seata community."},{"id":"/seata-feature-undo-log-compress","metadata":{"permalink":"/blog/seata-feature-undo-log-compress","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-feature-undo-log-compress.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-feature-undo-log-compress.md","title":"Seata New Feature Support -- Undo_Log Compression","description":"Current Situation & Pain Points","date":"2021-05-07T00:00:00.000Z","formattedDate":"May 7, 2021","tags":[],"readingTime":3.785,"hasTruncateMarker":false,"authors":[{"name":"chd"}],"frontMatter":{"title":"Seata New Feature Support -- Undo_Log Compression","author":"chd","keywords":["Seata","undo_log","compress"],"date":"2021/05/07"},"unlisted":false,"prevItem":{"title":"Analysis of Seata\'s Distributed UUID Generator Based on Improved Snowflake Algorithm","permalink":"/blog/seata-analysis-UUID-generator"},"nextItem":{"title":"Seata Deadlock Issue Caused by ConcurrentHashMap","permalink":"/blog/seata-dsproxy-deadlock"}},"content":"## Current Situation & Pain Points\\n\\nFor Seata, it records the before and after data of DML operations to perform possible rollback operations, and stores this data in a blob field in the database. For batch operations such as insert, update, delete, etc., the number of affected rows may be significant, concatenated into a large field inserted into the database, which may lead to the following issues:\\n\\n1. Exceeding the maximum write limit for a single database operation (such as the `max_allowed_package` parameter in MySQL).\\n2. Significant network IO and database disk IO overhead due to a large amount of data.\\n\\n## Brainstorming\\n\\nFor the first issue, the `max_allowed_package` parameter limit can be increased based on the actual situation of the business to avoid the \\"query is too large\\" problem. For the second issue, increasing bandwidth and using high-performance SSD as the database storage medium can help.\\n\\nThe above solutions involve external or costly measures. Is there a framework-level solution to address the pain points mentioned above?\\n\\nConsidering the root cause of the pain points mentioned above, the problem lies in the generation of excessively large data fields. Therefore, if the corresponding data can be compressed at the business level before data transmission and storage, theoretically, it can solve the problems mentioned above.\\n\\n## Feasibility Analysis\\n\\nCombining the brainstorming above, in practical development, when large batch operations are required, they are often scheduled during periods of relatively low user activity and low concurrency. At such times, CPU and memory resources can be relatively more utilized to quickly complete the corresponding operations. Therefore, by consuming CPU and memory resources to compress rollback data, the size of data transmission and storage can be reduced.\\n\\nAt this point, two things need to be demonstrated:\\n\\n1. After compression, it can reduce the pressure on network IO and database disk IO. This can be measured by the total time taken for data compression + storage in the database.\\n2. After compression, the efficiency of compression compared to the original data size. This can be measured by the data size before and after compression.\\n\\nTesting the time spent on compressing network usage:\\n\\n![image](https://user-images.githubusercontent.com/22959373/95567752-f55ddf80-0a55-11eb-8092-1f1d99855bdd.png)\\n\\n## Compression Ratio Test:\\n\\n![image](https://user-images.githubusercontent.com/22959373/95567834-0ad30980-0a56-11eb-9d7e-48b74babbea4.png)\\n\\nThe test results clearly indicate that using gzip or zip compression can significantly reduce the pressure on the database and network transmission. At the same time, it can substantially decrease the size of the stored data.\\n\\n### Implementation\\n\\n#### Implementation Approach\\n\\n![Compression](https://user-images.githubusercontent.com/22959373/116281711-8f039900-a7bc-11eb-91f8-82afdbb9f932.png)\\n\\n#### Partial Code\\n\\n```properties\\n# Whether to enable undo_log compression, default is true\\nseata.client.undo.compress.enable=true\\n\\n# Compressor type, default is zip, generally recommended to be zip\\nseata.client.undo.compress.type=zip\\n\\n# Compression threshold for enabling compression, default is 64k\\nseata.client.undo.compress.threshold=64k\\n```\\n\\nDetermining Whether the Undo_Log Compression Feature is Enabled and if the Compression Threshold is Reached\\n\\n```java\\nprotected boolean needCompress(byte[] undoLogContent) {\\n// 1. Check whether undo_log compression is enabled (1.4.2 Enabled by Default).\\n// 2. Check whether the compression threshold has been reached (64k by default).\\n// If both return requirements are met, the corresponding undoLogContent is compressed\\n    return ROLLBACK_INFO_COMPRESS_ENABLE \\n        && undoLogContent.length > ROLLBACK_INFO_COMPRESS_THRESHOLD;\\n}\\n```\\n\\nInitiating Compression for Undo_Log After Determining the Need\\n\\n\\n```java\\n// If you need to compress, compress undo_log\\nif (needCompress(undoLogContent)) {\\n    // Gets the compression type, default zip\\n    compressorType = ROLLBACK_INFO_COMPRESS_TYPE;\\n    //Get the corresponding compressor and compress it\\n    undoLogContent = CompressorFactory.getCompressor(compressorType.getCode()).compress(undoLogContent);\\n}\\n// else does not need to compress and does not need to do anything\\n```\\n\\nSave the compression type synchronously to the database for use when rolling back:\\n\\n```java\\nprotected String buildContext(String serializer, CompressorType compressorType) {\\n    Map<String, String> map = new HashMap<>();\\n    map.put(UndoLogConstants.SERIALIZER_KEY, serializer);\\n    // Save the compression type to the database\\n    map.put(UndoLogConstants.COMPRESSOR_TYPE_KEY, compressorType.name());\\n    return CollectionUtils.encodeMap(map);\\n}\\n```\\n\\nDecompress the corresponding information when rolling back:\\n\\n```java\\nprotected byte[] getRollbackInfo(ResultSet rs) throws SQLException  {\\n    // Gets a byte array of rollback information saved to the database\\n    byte[] rollbackInfo = rs.getBytes(ClientTableColumnsName.UNDO_LOG_ROLLBACK_INFO);\\n    // Gets the compression type\\n    // getOrDefault uses the default value CompressorType.NONE to directly upgrade 1.4.2+ to compatible versions earlier than 1.4.2\\n    String rollbackInfoContext = rs.getString(ClientTableColumnsName.UNDO_LOG_CONTEXT);\\n    Map<String, String> context = CollectionUtils.decodeMap(rollbackInfoContext);\\n    CompressorType compressorType = CompressorType.getByName(context.getOrDefault(UndoLogConstants.COMPRESSOR_TYPE_KEY,\\n    CompressorType.NONE.name()));\\n    // Get the corresponding compressor and uncompress it\\n    return CompressorFactory.getCompressor(compressorType.getCode())\\n        .decompress(rollbackInfo);\\n}\\n```\\n\\n\\n\\n### peroration\\n\\nBy compressing undo_log, Seata can further improve its performance when processing large amounts of data at the framework level. At the same time, it also provides the corresponding switch and relatively reasonable default value, which is convenient for users to use out of the box, but also convenient for users to adjust according to actual needs, so that the corresponding function is more suitable for the actual use scenario."},{"id":"/seata-dsproxy-deadlock","metadata":{"permalink":"/blog/seata-dsproxy-deadlock","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-dsproxy-deadlock.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-dsproxy-deadlock.md","title":"Seata Deadlock Issue Caused by ConcurrentHashMap","description":"This article primarily discusses an online issue, a Seata dynamic data source proxy deadlock caused by a ConcurrentHashMap bug.","date":"2021-03-13T00:00:00.000Z","formattedDate":"March 13, 2021","tags":[],"readingTime":12.705,"hasTruncateMarker":false,"authors":[{"name":"xiaoyong.luo"}],"frontMatter":{"title":"Seata Deadlock Issue Caused by ConcurrentHashMap","keywords":["Seata","dynamic data source","DataSource","ConcurrentHashMap","computeIfAbsent"],"description":"This article primarily discusses an online issue, a Seata dynamic data source proxy deadlock caused by a ConcurrentHashMap bug.","author":"xiaoyong.luo","date":"2021/03/13"},"unlisted":false,"prevItem":{"title":"Seata New Feature Support -- Undo_Log Compression","permalink":"/blog/seata-feature-undo-log-compress"},"nextItem":{"title":"Seata Application-Side Startup Process Analysis \u2014 Registry and Configuration Module","permalink":"/blog/seata-client-start-analysis-02"}},"content":"1. seata version: 1.4.0, but all versions below 1.4 also have this problem.\\n2. Problem description: In a global transaction, a pure query operation on a branch transaction suddenly gets stuck without any feedback (logs/exceptions) until the RPC timeout on the consumer side\\n\\n! [image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/03a7f737b56e45b4b74e662033ec74f6~tplv-k3u1fbpfcp-watermark.image)\\n\\n# Problem Troubleshooting\\n1. The whole process is in a global transaction, the consumer and provider can be seen as two branches of the global transaction, consumer --\x3e provider.\\n2. the consumer first executes some local logic, and then sends an RPC request to the provider to make sure that the consumer has sent the request and the provider has received it.\\n3. the provider first prints a log, and then executes a pure query SQL, if the SQL is executed properly, it will print the log, but the current phenomenon is that only the log before the execution of the SQL is printed, and no SQL-related logs are printed. Find DBA to check the SQL log, and found that the SQL is not executed.\\n4. Determined that the operation is only a pure query operation under the global transaction, before the operation, the overall process of the global transaction is completely normal.\\n5. In fact, the phenomenon here has been very obvious, but at that time the idea did not change over, has been concerned about the query SQL, always thinking that even if the query timeout and other reasons should be thrown exceptions ah, should not be nothing. DBA can not find the query record, that is not to say that the SQL may not have been executed ah, but in the execution of the SQL before the problem, such as the agent?\\n6. With the help of arthas\'s watch command, there is no output. The first log output means that the method must have been executed, and the delay in outputting the result means that the current request is stuck, why is it stuck?\\n7. With arthas\'s thread command `thread -b`, `thread -n`, is to find out the current busiest thread. This works very well, there is a thread CPU usage `92%`, and because of this thread caused the other 20 or so Dubbo threads `BLOCKED`. The stack information is as follows\\n8. Analysing the stack information, we can clearly find the interface related to seata, which is probably related to seata\'s data source proxy; at the same time, we found that the thread with the highest CPU usage is stuck in the `ConcurrentHashMap#computeIfAbsent` method. Is there a bug in the `ConcurrentHashMap#computeIfAbsent` method?\\n9. By now, we don\'t know the exact cause of the problem, but it should have something to do with seata\'s data source proxy, and we need to analyse both the business code and the seata code to find out why.\\n\\n! [image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/faac0be0982e45a7a43b335e8f8b44bf~tplv-k3u1fbpfcp-watermark.image)\\n\\n## Problem analysis\\n\\n### ConcurrentHashMap##computeIfAbsent\\nThis method does have the potential for problems: if two keys have the same hascode, and the computeIfAbsent operation is performed in the corresponding mappingFunction, it will lead to a dead loop, refer to this article for specific analysis: https://juejin.cn/post/ 6844904191077384200\\n\\n### Seata data source autoproxy\\nRelated content has been analysed before, let\'s focus on the following core classes:\\n1. SeataDataSourceBeanPostProcessor\\n2. SeataAutoDataSourceProxyAdvice\\n3. DataSourceProxyHolder\\n\\n##### SeataDataSourceBeanPostProcessor\\nThe `SeataDataSourceBeanPostProcessor` is a `BeanPostProcessor` implementation class that creates a `seataAutoDataSourceProxyDataSource` for the data source configured by the business side in the `postProcessAfterInitialization` method (i.e., after the bean is initialised). proxy data source\\n\\n```java\\npublic class SeataDataSourceBeanPostProcessor implements BeanPostProcessor {\\n@Override\\npublic Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException {\\nif (bean instanceof DataSource) {\\n//When not in the excludes, put and init proxy. if (!excludes.contains.\\nif (!excludes.contains(bean.getClass().getName())) {\\n//Only put and init proxy, not return proxy.\\nDataSourceProxyHolder.get().putDataSource((DataSource) bean, dataSourceProxyMode);\\n}\\n//If is SeataDataSourceProxy, return the original data source.\\nif (bean instanceof SeataDataSourceProxy) {\\nLOGGER.info(\\"Unwrap the bean of the data source,\\" +\\n\\" and return the original data source to replace the data source proxy.\\"); return ((SeataDataSourceProxy); } }\\nreturn ((SeataDataSourceProxy) bean).getTargetDataSource();\\n}\\n}\\nreturn bean.\\n}\\n}\\n ```\\n\\n ##### SeataAutoDataSourceProxyAdvice\\n `SeataAutoDataSourceProxyAdvice` is a MethodInterceptor, `SeataAutoDataSourceProxyCreator` in seata creates a dynamic proxy object for `Bean` of type `DataSource`, the proxy logic is the `SeataAutoDataSourceProxyAdvice#invoke` logic. That is: when executing the relevant methods of the `DataSourceAOPProxyAdvice`, it will go through its `invoke` method, and in the `invoke` method, it will find the corresponding `SeataAutoDataSourceProxyAdvice` according to the native data source, which will ultimately execute the `SeataAutoDataSourceProxyAdvice` logic.\\n ```java\\n public class SeataAutoDataSourceProxyAdvice implements MethodInterceptor, IntroductionInfo {\\n    ......\\n    @Override\\n    public Object invoke(MethodInvocation invocation) throws Throwable {\\n        if (!RootContext.requireGlobalLock() && dataSourceProxyMode ! = RootContext.getBranchType()) {\\n            return invocation.proceed();\\n        }\\n        Method method = invocation.getMethod();\\n        Object[] args = invocation.getArguments(); } Method m = BeanUtils.getMethod(); }\\n        Method m = BeanUtils.findDeclaredMethod(dataSourceProxyClazz, method.getName(), method.getParameterTypes());\\n        if (m ! = null) {\\n            SeataDataSourceProxy dataSourceProxy = DataSourceProxyHolder.get().putDataSource((DataSource) invocation.getThis(), dataSourceProxyMode ); return m.invoke(dataSourceProxyHolder).\\n            return m.invoke(dataSourceProxy, args);\\n        } else {\\n            return invocation.proceed();\\n        }\\n    }\\n }\\n ```\\n\\n##### DataSourceProxyHolder\\nThe process is clear to us, now there is a question, how to maintain the relationship between `native data source` and `seata proxy data source`? It is maintained by `DataSourceProxyHolder`, which is a singleton object that maintains the relationship between the two through a ConcurrentHashMap: `native data source` as key --\x3e `seata proxy data source` as value.\\n\\n```java\\npublic class DataSourceProxyHolder {\\npublic SeataDataSourceProxy putDataSource(DataSource dataSource, BranchType dataSourceProxyMode) {\\nDataSource originalDataSource = dataSource;\\n......\\nreturn CollectionUtils.computeIfAbsent(this.dataSourceProxyMap, originalDataSource, BranchType.\\nBranchType.XA == dataSourceProxyMode ? DataSourceProxyXA::new : DataSourceProxy::new);\\n}\\n}\\n\\n\\n// CollectionUtils.java\\npublic static <K, V> V computeIfAbsent(Map<K, V> map, K key, Function<? super K, ? extends V> mappingFunction) {\\nV value = map.get(key);\\nif (value ! = null) {\\nreturn value; }\\n}\\nreturn map.computeIfAbsent(key, mappingFunction);\\n}\\n ```\\n\\n ### Client data source configuration\\n 1. Two data sources are configured: `DynamicDataSource`, `P6DataSource`, `P6DataSource`, `P6DataSource` and `P6DataSource`.\\n 2. `P6DataSource` can be seen as a wrapper for `DynamicDataSource`. 3.\\n 3. Let\'s not worry about whether this configuration makes sense or not, now we just analyse the problem purely based on this data source configuration.\\n\\n ```java\\n @Qualifier(\\"dsMaster\\")\\n @Bean(\\"dsMaster\\")\\n DynamicDataSource dsMaster() {\\n    return new DynamicDataSource(masterDsRoute);\\n }\\n\\n @Primary\\n @Qualifier(\\"p6DataSource\\")\\n @Bean(\\"p6DataSource\\")\\n P6DataSource p6DataSource(@Qualifier(\\"dsMaster\\") DataSource dataSource) {\\n    P6DataSource p6DataSource = new P6DataSource(dsMaster());\\n    return p6DataSource;\\n }\\n ```\\n\\n ### Analyse the process\\n\\n ``Assuming that by now everyone is aware of the problems that may arise from ConcurrentHashMap#computeIfAbsent``, it is known that this problem has now arisen, and in combination with the stack information, we can see roughly where this problem has arisen.\\n\\n 1, `ConcurrentHashMap#computeIfAbsent` will produce this problem precondition is: `two key hashcode is the same`; `mappingFunction corresponds to a put operation`. Combined with our seata usage scenario, the mappingFunction corresponds to `DataSourceProxy::new`, suggesting that the put operation may be triggered in the DataSourceProxy\'s constructor method\\n\\n ! [image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/00d8e13f71644c63b3bbb58c93b30e0c~tplv-k3u1fbpfcp-watermark.image)\\n ```java\\n Execute AOP proxy data source related methods =>\\n Enter SeataAutoDataSourceProxyAdvice cutover logic =>\\n Execute DataSourceProxyHolder#putDataSource method =>\\n Execute DataSourceProxy::new =>\\n AOP proxy data source\'s getConnection method =>\\n The getConnection method of the native data source =>\\n Enter SeataAutoDataSourceProxyAdvice cutover logic =>\\n Execute DataSourceProxyHolder#putDataSource method =>\\n Execute DataSourceProxy::new =>\\n DuridDataSource\'s getConnection method\\n ```\\n\\n2, What is the `AOP proxy data source` and `native data source` stated in step 1? Look at the following diagram\\n! [image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3579631f58df4d17bfcd6f28ccc3fd79~tplv-k3u1fbpfcp-watermark.image)\\n\\n3, the above also said to produce this problem there is a condition ` two key hashcode the same `, but I see that these two data source objects are not overriding the ` hashcode ` method, so by definition, these two objects must be different hashcode. After looking at the ConcurrentHashMap problem again, I feel that the statement `two keys have the same hashcode` is not correct, and `two keys will generate hash conflict` is more reasonable, which explains why two objects with different hashcodes will encounter this problem. To prove this, I\'ve given an example below.\\n```java\\npublic class Test {\\npublic static void main(String[] args) {\\nConcurrentHashMap map = new ConcurrentHashMap(8); Num n1 = new Num(8)\\nNum n1 = new Num(3);\\nNum n2 = new Num(19); Num n3 = new Num(19); Num n3 = new Num(19)\\nNum n3 = new Num(20);\\n\\n// map.computeIfAbsent(n1, k1 -> map.computeIfAbsent(n3, k2 -> 200)); // This line of code does not cause the program to loop.\\nmap.computeIfAbsent(n1, k1 -> map.computeIfAbsent(n2, k2 -> 200)); // this line of code will cause the program to die\\n}\\n\\n    static class Num{\\n        private int i; public Num(int i){\\n        public Num(int i){\\n            this.i = i; } static class Num{ private int i; public Num(int i){ this.\\n        }\\n\\n        public int hashCode() { this.i = i; this.i = i; }\\n        public int hashCode() {\\n            return i; } @Override public int hashCode() { this.i = i; }\\n        }\\n    }\\n}\\n ```\\n 1. To make it easier to reproduce the problem, we rewrite the `Num#hashCode` method to ensure that the constructor input is the hashcode value.\\n 2. create a ConcurrentHashMap object, initialCapacity is 8, sizeCtl calculated value is 16, that is, the default length of the array in the map is 16\\n 3. create object `n1`, the input parameter is 3, that is, the hashcode is 3, the calculation of its corresponding array subscript 3\\n 4. create object `n2`, the input parameter is 19, that is, the hashcode is 19, calculate its corresponding array subscript is 3, at this time we can think of `n1 and n2 hash conflict`.\\n 5. create object `n3` with input 20, i.e., hashcode 20, and its corresponding array subscript is 4.\\n 6. execute `map.computeIfAbsent(n1, k1 -> map.computeIfAbsent(n3, k2 -> 200))`, the programme exits normally: `Because there is no hash conflict between n1 and n3, the programme terminates normally`.\\n 7. Execute `map.computeIfAbsent(n1, k1 -> map.computeIfAbsent(n2, k2 -> 200))`, the programme exits normally: ` because n1 and n2 have a hash conflict, so it is in a dead loop`.\\n\\n\\n 4\u3001During the initialisation of the object, hasn\'t `SeataDataSourceBeanPostProcessor` already initialised the corresponding data source proxy of the object? Why the corresponding data source proxy is still created in `SeataAutoDataSourceProxyAdvice`?\\n 1. First of all, the `SeataDataSourceBeanPostProcessor` execution period is later than the creation of the AOP proxy object, so when executing the `SeataDataSourceBeanPostProcessor` related methods, the `SeataAutoDataSourceBeanPostProcessor` method is executed. SeataAutoDataSourceProxyAdvice` should actually take effect when the `SeataDataSourceBeanPostProcessor` related methods are executed.\\n 2. when adding elements to the map in `SeataDataSourceBeanPostProcessor`, the key is `AOP proxy datasource`; in `SeataAutoDataSourceProxyAdvice`, the key is `native datasource`, so the key is not the same as `invocation.getThis()`, so the key is not the same. `, so the key is not the same\\n\\n ! [image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/747b664a0b6c4f58843947576dd0856e~tplv-k3u1fbpfcp-watermark.image)\\n\\n 5, there is another problem, `SeataAutoDataSourceProxyAdvic#invoke` method does not filter `toString, hashCode` and other methods, the proxy object created by cglib will override these methods by default, if these methods of the proxy object are triggered when putting elements into the map. If these methods are triggered when putting elements into the map, the proxy object will re-enter the `SeataAutoDataSourceProxyAdvic#invoke` cut until the thread stack benefits.\\n\\n\\n\\n # Summary of the problem\\n 1. in two key will produce hash conflict, will trigger `ConcurrentHashMap#computeIfAbsent` BUG, the performance of this BUG is to make the current thread into a dead loop\\n 2. business feedback, the problem is occasional, occasional for two reasons: first, the application is a multi-node deployment, but only one node on the line triggered the BUG (hashcode conflict), so only when the request hits this node may trigger the BUG; second, because each restart object address (hashcode) are not sure, so not triggered after every app restart, but if once triggered, the node will always have this problem. Having a thread that keeps dying and blocking other threads that are trying to get the proxy data source from the map is business feedback that the request is stuck. If this happens to successive requests, the business side may restart the service, and then, ``because the hash conflict does not necessarily exist after the restart, the business may behave normally after the restart, but it is also possible that the bug will be triggered again on the next restart.\\n 3. when encountering this problem, from the perspective of the whole problem, it is indeed a deadlock, because the dead loop thread occupant lock has not been released, resulting in other threads operating on the map is BLOCKED!\\n 4. essentially because `ConcurrentHashMap#computeIfAbsent method may trigger a bug`, and seata\'s usage scenario just triggered the bug.\\n 5. The following demo is actually a complete simulation of what happens when something goes wrong online, as follows:\\n\\n ```java\\n public class Test {\\n    public static void main(String[] args) {\\n\\n        ConcurrentHashMap map = new ConcurrentHashMap(8);\\n\\n        Num n1 = new Num(3);\\n        Num n2 = new Num(19);\\n\\n        for(int i = 0; i< 20; i++){\\n            new Thread(()-> {\\n                try {\\n                    Thread.sleep(1000); } catch (InterruptedException e.g.\\n                } catch (InterruptedException e) {\\n                    e.printStackTrace();\\n                }\\n\\n                map.computeIfAbsent(n1, k-> 200); }).start(); }\\n            }).start();\\n        }\\n        map.computeIfAbsent(n1, k1 -> map.computeIfAbsent(n2, k2 -> 200));\\n    }\\n\\n\\n    static class Num{\\n        private int i; public Num(int i){\\n\\n        public Num(int i){\\n            this.i = i; }\\n        }\\n        public int hashCode() { this.i = i; this.i = i; }\\n        public int hashCode() {\\n            return i; } @Override public int hashCode() { this.i = i; }\\n        }\\n    }\\n }\\n ```\\n\\n! [image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d6134c6498fa49c4a68b2745ba0895e3~tplv-k3u1fbpfcp-watermark.image)\\n\\n\\n### Solving the problem\\nThe problem can be solved in two ways:\\n1. business changes: P6DataSource and DynamicDataSource do not need to be proxied, directly proxy P6DataSource can be, DynamicDataSource does not need to be declared as a bean; or through the excludes property excludes P6DataSource, so that there is no duplicate proxy problem. There will be no problem of duplicate proxies\\n2. Seata refinement: improve the logic related to data source proxy\\n\\n##### Business changes\\n1. Data source related configuration can be changed to the following:\\n ```java\\n @Primary\\n @Qualifier(\\"p6DataSource\\")\\n @Bean(\\"p6DataSource\\")\\n P6DataSource p6DataSource(@Qualifier(\\"dsMaster\\") DataSource dataSource) {\\n    P6DataSource p6DataSource = new P6DataSource(new TuYaDynamicDataSource(masterDsRoute));\\n    logger.warn(\\"dsMaster={}, hashcode={}\\",p6DataSource, p6DataSource.hashCode());\\n    return p6DataSource.\\n }\\n ```\\n\\n2. Or leave the current data source configuration unchanged and add the excludes property\\n ```java\\n @EnableAutoDataSourceProxy(excludes={\\"P6DataSource\\"})\\n ```\\n\\n##### Seata refinement\\n\\n1. `ConcurrentHashMap#computeIfAbsent` method is changed to double check as follows:\\n ``` java\\n SeataDataSourceProxy dsProxy = dataSourceProxyMap.get(originalDataSource);\\n if (dsProxy == null) {\\n    synchronized (dataSourceProxyMap) {\\n        dsProxy = dataSourceProxyMap.get(originalDataSource);\\n        if (dsProxy == null) {\\n            dsProxy = createDsProxyByMode(dataSourceProxyMode, originalDataSource);\\n            dataSourceProxyMap.put(originalDataSource, dsProxy);\\n        }\\n    }\\n }\\n return dsProxy;\\n ```\\n\\nI wanted to change the `CollectionUtils#computeIfAbsent` method directly, and the feedback from the group was that this might cause the data source to be created multiple times, which is indeed a problem: as follows\\n```java\\npublic static <K, V> V computeIfAbsent(Map<K, V> map, K key, Function<? super K, ? extends V> mappingFunction) {\\nV value = map.get(key);\\nif (value ! = null) {\\nreturn value; }\\n}\\nvalue = mappingFunction.apply(key);\\nreturn map.computeIfAbsent(key, value);\\n}\\n ```\\n\\n 2. Add some filtering to the SeataAutoDataSourceProxyAdvice cutout logic\\n ```java\\n Method m = BeanUtils.findDeclaredMethod(dataSourceProxyClazz, method.getName(), method.getParameterTypes());\\n if (m ! = null && DataSource.class.isAssignableFrom(method.getDeclaringClass())) {\\n    SeataDataSourceProxy dataSourceProxy = DataSourceProxyHolder.get().putDataSource((DataSource) invocation.getThis(), dataSourceProxyMode ); return m.invoke(dataSourceProxyHolder).\\n    return m.invoke(dataSourceProxy, args);\\n } else {\\n    return invocation.proceed();\\n }\\n ```\\n\\n### Legacy issues\\nIn the corresponding methods of `SeataDataSourceBeanPostProcessor` and `SeataAutoDataSourceProxyAdvice`, the keys corresponding to initialising the `seata datasource proxy` into the map are fundamentally different, the ` The key in `SeataDataSourceBeanPostProcessor` is the `AOP proxy data source`; the key in `SeataAutoDataSourceProxyAdvice` is the native object, which results in the unnecessary creation of the `seata data source proxy` object.\\n\\nWhat is the best suggestion for this problem? Is it possible to specify an order for `SeataDataSourceBeanPostProcessor` to take effect before the AOP proxy object is created?\\n\\n\\n# Link to original article\\n\\nhttps://juejin.cn/post/6939041336964153352/"},{"id":"/seata-client-start-analysis-02","metadata":{"permalink":"/blog/seata-client-start-analysis-02","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-client-start-analysis-02.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-client-start-analysis-02.md","title":"Seata Application-Side Startup Process Analysis \u2014 Registry and Configuration Module","description":"\\"Just getting started with Seata and don\'t know enough about its modules?","date":"2021-03-04T01:35:01.000Z","formattedDate":"March 4, 2021","tags":[{"label":"Seata","permalink":"/blog/tags/seata"}],"readingTime":14.515,"hasTruncateMarker":false,"authors":[{"name":"booogu"}],"frontMatter":{"layout":"post","comments":true,"title":"Seata Application-Side Startup Process Analysis \u2014 Registry and Configuration Module","date":"2021-03-04T01:35:01.000Z","author":"booogu","catalog":true,"tags":["Seata"]},"unlisted":false,"prevItem":{"title":"Seata Deadlock Issue Caused by ConcurrentHashMap","permalink":"/blog/seata-dsproxy-deadlock"},"nextItem":{"title":"Analysis of Seata Application-Side Startup Process - How RM & TM Establish Connections with TC","permalink":"/blog/seata-client-start-analysis-01"}},"content":"> \\"Just getting started with Seata and don\'t know enough about its modules? <br />\\nWant to dive into the Seata source code, but haven\'t done so yet? <br />\\nWant to find out what your application is doing \\"on the sly\\" during startup after integrating Seata? <br />\\nWant to learn the design concepts and best practices of Seata as a great open source framework? <br />\\nIf any of the above apply to you, then today\'s article is for you!\\n\\n## Preface\\nIn Seata\'s application-side (RM, TM) startup process, the first thing to do is to establish communication with the coordinator side (TC), which is a prerequisite for Seata to be able to complete the distributed transaction coordination, so Seata in the process of completing the initialisation of the application side and establishing a connection with the TC, it is **How to find the cluster and address of the TC Transaction Coordinator**? And how does it **get various configuration information** from the configuration module? That\'s what this article is going to explore.\\n\\n## Give a qualification\\nSeata as a middleware level of the underlying components, is very careful to introduce third-party frameworks for specific implementations, interested students can learn more about Seata\'s SPI mechanism, to see how Seata is through a large number of extension points (Extension), to invert the specific implementation of the dependent components out of the turn rely on abstract interfaces, and at the same time, Seata in order to better At the same time, Seata in order to better integrate into microservices, cloud native and other popular architectures derived from the ecosystem, but also based on the SPI mechanism on a number of mainstream microservice frameworks, registry, configuration centre and Java development frameworks, \\"the leader\\" - SpringBoot and so on. Do the active integration , in order to ensure that the microkernel architecture , loosely coupled , scalable at the same time , but also can be very good with all kinds of components \\"to play with\\", so that the environment using a variety of technology stacks can be more convenient to introduce Seata.\\n\\nIn this paper, in order to be close to everyone ** just introduced Seata trial ** scene , in the following introduction , select ** application side ** qualifications are as follows : the use of **File (file) as the configuration centre and registration centre **, and based on ** SpringBoot ** start.\\n\\nWith this qualification, let\'s dive into the Seata source code and find out what\'s going on.\\n\\n## RM/TM Initialisation Process with Alternating Multi-Module Collaboration\\nIn [ Seata Client Startup Process Dissection (I) ](http://booogu.top/2021/02/28/seata-client-start-analysis-01/), we analysed the initialization of TM and RM on the application side of Seata, and how the application side creates a Netty Channel and sends a registration request to the TC Server to send a registration request. In addition to this, during RM initialisation, several other Seata modules (Registration Centre, Configuration Centre, Load Balancing) come into play and collaborate with each other to complete the process of connecting to the TC Server.\\n\\nWhen executing the Client reconnect to TC Server method: NettyClientChannelManager.Channreconnect(), you first need to get the list of available TC Server addresses based on the current **transaction grouping**:\\n```js\\n/**\\n     * NettyClientChannelManager.reconnect()\\n     * Reconnect to remote server of current transaction service group.\\n*\\n     * @param transactionServiceGroup transaction service group\\n*/\\nvoid reconnect(String transactionServiceGroup) {\\nList<String> availList = null; }\\ntry {\\n// Get the available TC Server addresses from the registry\\navailList = getAvailServerList(transactionServiceGroup);\\n} catch (Exception e) {\\nLOGGER.error(\\"Failed to get available servers: {}\\", e.getMessage(), e); return; {// Get the available TC Server addresses from the registry.\\nreturn; }\\n}\\n// The following code is omitted\\n}\\n```\\n\\nFor a detailed introduction to the concept of transaction grouping, you can refer to the official document [Introduction to Transaction Grouping](/docs/user/txgroup/transaction-group/). Here is a brief introduction.\\n- Each Seata application-side RM, TM, has a **transaction grouping** name\\n- Each TC on the Seata coordinator side has a **cluster name** and **address**.\\n  The application side goes through the following two steps when connecting to the coordinator side:\\n- Through the name of the transaction grouping, the cluster name of the TC corresponding to this application side is obtained from the configuration\\n- By using the cluster name, the address list of the TC cluster can be obtained from the registry.\\n  The above concepts, relationships and processes are shown in the following figure:\\n  ! [Relationship between Seata transaction grouping and connection establishment](http://booogu.top/img/in-post/TXGroup_Group_Relation.jpg)\\n\\n### Getting TC Server cluster addresses from **Registry**\\nAfter understanding the main concepts and steps involved in connecting TCs from RM/TC, let\'s move on to explore the getAvailServerList method:\\n```js\\nprivate List<String> getAvailServerList(String transactionServiceGroup) throws Exception {\\n//\u2460 Use the registry factory to get a registry instance.\\n//\u2461 Call the registry\'s lookup method lookUp() to get a list of the addresses of the available Servers in the TC cluster based on the transaction group name.\\nList<InetSocketAddress> availInetSocketAddressList = RegistryFactory.getInstance().lookup(transactionServiceGroup);\\nif (CollectionUtils.isEmpty(availInetSocketAddressList)) {\\nreturn Collections.emptyList();\\n}\\n\\n        return availInetSocketAddressList.stream()\\n                                         .map(NetUtil::toStringAddress)\\n                                         .collect(Collectors.toList()); }\\n    }\\n ```\\n #### Which registry to use? The **Seata meta-configuration file** gives the answer\\n As mentioned above, Seata supports a variety of registry implementations, so Seata first needs to get the \\"type of registry\\" information from a place first.\\n\\n Seata has designed a \\"configuration file\\" to store some basic information about the components used in its framework. I prefer to call this configuration file **\\"meta-configuration file \\"**, because the information it contains is actually the \\"configuration of the configuration\\", i.e., the \\"configuration of the configuration\\", i.e., the \\"configuration of the configuration\\". This is because the information it contains is actually the \\"configuration of the configuration\\", i.e., the concept of \\"meta\\", which can be understood by comparing the information in the database table with the information in the structure of the database table itself (table data and table metadata).\\n\\n We can think of the information in the Registry and Configuration Centre as **configuration information itself**, and what is the configuration** of this **configuration information? This information, then, is contained in Seata\'s meta-configuration file. In fact, there are only **two types of information** contained in the \'meta-configuration file\':\\n - The first is the type of registry: registry.type, as well as some basic information about that type of registry, for example, when the registry type is a file, the meta configuration file stores the file\'s name information; when the registry type is Nacos, the meta configuration file stores Nacos addresses, namespaces, cluster names and other information.\\n - Second, the type of configuration centre: config.type, as well as some basic information about the type of configuration centre, such as when the configuration centre is a file, the meta-configuration file stores information about the name of the file; when the type of registry is Consul, the meta-configuration file stores information about the address of the Consul\\n\\n Seata\'s meta-configuration file supports Yaml, Properties and other formats , and can be integrated into the SpringBoot application.yaml file ( use seata-spring-boot-starter can be ) , easy to integrate with SpringBoot .\\n\\n The default meta-configuration file that comes with Seata is registry.conf, and when we use a file as the registration and configuration centre, the content in registry.conf is set as follows:\\n ```js\\n registry {\\n  # file , nacos , eureka, redis, zk, consul, etcd3, sofa\\n  type = \\"file\\"\\n  file {\\n    name = \\"file.conf\\"\\n  }\\n }\\n\\n config {\\n  # file, nacos, apollo, zk, consul, etcd3\\n  type = \\"file\\"\\n  file {\\n    name = \\"file.conf\\"\\n  }\\n }\\n ```\\nIn the following source code, we can find that the type of registry used by Seata is taken from ConfigurationFactory.CURRENT_FILE_INSTANCE, and this CURRENT_FILE_INSTANCE is what we call, an instance of the Seata **meta-configuration file **\\n```js\\n// In getInstance(), call buildRegistryService to build the specific registry instance\\npublic static RegistryService getInstance() {\\nif (instance == null) {\\nsynchronized (RegistryFactory.class) {\\nif (instance == null) {\\ninstance = buildRegistryService();\\n}\\n}\\n}\\nreturn instance; }\\n}\\n\\n    private static RegistryService buildRegistryService() {\\n        RegistryType registryType.\\n        // Get the registry type\\n        String registryTypeName = ConfigurationFactory.CURRENT_FILE_INSTANCE.getConfig(\\n            ConfigurationKeys.FILE_ROOT_REGISTRY + ConfigurationKeys.FILE_CONFIG_SPLIT_CHAR\\n                + ConfigurationKeys.FILE_ROOT_TYPE);\\n        try {\\n            registryType = RegistryType.getType(registryTypeName); } catch (Exception exx); exx = RegistryType.\\n        } catch (Exception exx) {\\n            throw new NotSupportYetException(\\"not support registry type: \\" + registryTypeName); }\\n        }\\n        if (RegistryType.File == registryType) {\\n            return FileRegistryServiceImpl.getInstance(); } else {\\n        } else {\\n            // Load the registry instance using the SPI method based on the registry type\\n            return EnhancedServiceLoader.load(RegistryProvider.class, Objects.requireNonNull(registryType).name()).provide();\\n        }\\n    }\\n ```\\n Let\'s look at the initialisation process of the meta-configuration file, which triggers the initialisation of the ConfigurationFactory class when the static field CURRENT_FILE_INSTANCE is fetched for the first time:\\n ```js\\n    // Static block of the ConfigurationFactory class\\n    static {\\n        load();\\n    }\\n\\n     /**\\n     * In the load() method, load Seata\'s meta configuration file\\n     */\\n    private static void load() {\\n        // The name of the meta configuration file, support through the system variable, environment variable expansion\\n        String seataConfigName = System.getProperty(SYSTEM_PROPERTY_SEATA_CONFIG_NAME);\\n        if (seataConfigName == null) {\\n            seataConfigName = System.getenv(ENV_SEATA_CONFIG_NAME);\\n        }\\n        if (seataConfigName == null) {\\n            seataConfigName = REGISTRY_CONF_DEFAULT;\\n        }\\n        String envValue = System.getProperty(ENV_PROPERTY_KEY);\\n        if (envValue == null) {\\n            envValue = System.getenv(ENV_SYSTEM_KEY); }\\n        }\\n        // Create a file configuration instance that implements the Configuration interface based on the meta-configuration file name\\n        Configuration configuration = (envValue == null) ? new FileConfiguration(seataConfigName,\\n                false) : new FileConfiguration(seataConfigName + \\"-\\" + envValue, false);\\n        Configuration extConfiguration = null;\\n        // Determine if an extended configuration provider exists by loading it through SPI\\n        //When the application side uses seata-spring-boot-starer, it will pass the SpringBootConfigurationProvider as the extended configuration provider, at this point, when getting the meta-configuration item, it will no longer get it from file.conf (the default), but from application. properties/application.yaml.\\n        try {\\n            // Replace the original Configuration instance with an instance of the extended configuration via the ExtConfigurationProvider\'s provide method\\n            extConfiguration = EnhancedServiceLoader.load(ExtConfigurationProvider.class).provide(configuration);\\n            if (LOGGER.isInfoEnabled()) {\\n                LOGGER.info(\\"load Configuration:{}\\", extConfiguration == null ? configuration.getClass().getSimpleName()\\n                        : extConfiguration.getClass().getSimpleName());\\n            }\\n        } catch (EnhancedServiceNotFoundException ignore) {\\n\\n        } catch (Exception e) {\\n            LOGGER.error(\\"failed to load extConfiguration:{}\\", e.getMessage(), e);\\n        }\\n        // Existence of an extended configuration returns an instance of the extended configuration, otherwise it returns an instance of the file configuration\\n        CURRENT_FILE_INSTANCE = extConfiguration == null ? configuration : extConfiguration;\\n    }\\n ```\\nThe call sequence diagram for the load() method is as follows:\\n! [Seata metaconfiguration file loading process](http://booogu.top/img/in-post/seata_config_initialization.png)\\n\\nIn the above sequence diagram, you can focus on the following points:\\n- Seata meta configuration file **Name support extension**\\n- Seata meta-configuration file suffixes** support 3 suffixes**, yaml/properties/conf, which will be attempted to match in turn when the meta-configuration file instance is created\\n- Seata ** configuration capabilities related to the top-level interface for the Configuration **, a variety of configuration centres are required to implement this interface, Seata\'s meta-configuration file is the use of FileConfiguration (file type configuration centre) to implement this interface\\n\\n ```js\\n /**\\n * Seata Configuration Capability Interface\\n * package: io.seata.config\\n */\\n\\n public interface Configuration {\\n    /**\\n     * Gets short.\\n     *\\n     * @param dataId the data id\\n     * @param defaultValue the default value\\n     * @param timeoutMills the timeout mills\\n     * @return the short\\n     */short getShort(String dataId)\\n    short getShort(String dataId, int defaultValue, long timeoutMills);; short getShort(String dataId, int defaultValue, long timeoutMills)\\n\\n    // The following content is omitted, the main ability to add, delete and retrieve configuration\\n }\\n ```\\n- Seata provides an extension point of type ExtConfigurationProvider, opening up the ability to extend the specific implementation of the configuration, which has a provide() method to receive the original Configuration, return a completely new Configuration, the form of the methods of this interface determines that the general The form of this interface method determines that, in general, static proxies, dynamic proxies, decorators and other design patterns can be used to implement this method to achieve the original Configuration enhancement.\\n ```js\\n /**\\n * Seata extends the Configuration Provider interface\\n * package: io.seata.configuration\\n */\\n public interface ExtConfigurationProvider {\\n    /**\\n     * provide a AbstractConfiguration implementation instance\\n     * @param originalConfiguration\\n     * @return configuration\\n     */\\n    Configuration provide(Configuration originalConfiguration); }\\n }\\n ```\\n- When the application side is started based on seata-seata-spring-boot-starter, it will ** use \\"SpringBootConfigurationProvider\\" as the extended configuration provider ** and in its provide method, it uses dynamic bytecode generation (CGLIB) to create a dynamic proxy class for the \\"FileConfiguration\\" instance. FileConfiguration\' instance using dynamic bytecode generation (CGLIB) to create a dynamic proxy class that intercepts all methods starting with \\"get\\" to get meta-configuration items from application.properties/application.yaml.\\n\\nSpringBootConfigurationProvider class, this article only explains the implementation of the idea , no longer unfolding the analysis of the source code, which is only an implementation of the ExtConfigurationProvider interface, from the point of view of the Configuration can be extended, can be replaced , Seata is precisely through the ExtConfigurationProvider such an extension point for the implementation of a variety of configurations provides a broad stage , allowing a variety of configuration implementation and access options.\\n\\nAfter going through the above loading process, if we **didn\'t extend the configuration provider**, we would get the registry type of file from the Seata meta-configuration file, and at the same time create a file registry instance: FileRegistryServiceImpl\\n#### Getting the TC Server address from the registry centre\\nAfter getting the registry instance, you need to execute the lookup() method (RegistryFactory.getInstance(). **lookup(transactionServiceGroup)**), FileRegistryServiceImpl.lookup() is implemented as follows:\\n```js\\n/**\\n* Get a list of available addresses for TC Server based on the transaction group name\\n* package: io.seata.discovery.registry\\n* class: FileRegistryServiceImpl\\n*/\\n@Override\\npublic List<InetSocketAddress> lookup(String key) throws Exception {\\n// Get TC Server cluster name\\nString clusterName = getServiceGroup(key);\\nif (clusterName == null) {\\nif (clusterName == null) { return null; }\\n}\\n//Get all available Server addresses in the TC cluster from the Configuration Centre\\nString endpointStr = CONFIG.getConfig(\\nPREFIX_SERVICE_ROOT + CONFIG_SPLIT_CHAR + clusterName + POSTFIX_GROUPLIST);\\nif (StringUtils.isNullOrEmpty(endpointStr)) {\\nthrow new IllegalArgumentException(clusterName + POSTFIX_GROUPLIST + \\" is required\\");\\n}\\n// Encapsulate the address as InetSocketAddress and return it\\nString[] endpoints = endpointStr.split(ENDPOINT_SPLIT_CHAR);\\nList<InetSocketAddress> inetSocketAddresses = new ArrayList<>();\\nfor (String endpoint : endpoints) {\\nString[] ipAndPort = endpoint.split(IP_PORT_SPLIT_CHAR);\\nif (ipAndPort.length ! = 2) {\\nthrow new IllegalArgumentException(\\"endpoint format should be like ip:port\\");;\\n}\\ninetSocketAddresses.add(new InetSocketAddress(ipAndPort[0], Integer.parseInt(ipAndPort[1]))); }\\n}\\nreturn inetSocketAddresses;\\n}\\n\\n    /**\\n     * default method in the registry interface\\n     * package: io.seata.discovery.registry\\n     * class: RegistryService\\n     */\\n    default String getServiceGroup(String key) {\\n        key = PREFIX_SERVICE_ROOT + CONFIG_SPLIT_CHAR + PREFIX_SERVICE_MAPPING + key;\\n        // In the configuration cache, add a transaction group name change listening event.\\n        if (!SERVICE_GROUP_NAME.contains(key)) {\\n            ConfigurationCache.addConfigListener(key);\\n            SERVICE_GROUP_NAME.add(key);\\n        }\\n        // Get the TC cluster name corresponding to the transaction grouping from the Configuration Centre\\n        return ConfigurationFactory.getInstance().getConfig(key);\\n    }\\n ```\\n As you can see, the code logic matches the flow in Figure **Seata Transaction Grouping in Relation to Establishing Connections** in Section I.\\n At this point, the registry will need assistance from the **Configuration Centre** to get the cluster name corresponding to the transaction grouping and to find the available service addresses in the cluster.\\n\\n ### Get TC cluster name from **Configuration Centre**\\n #### Configuration Centre initialisation\\n The initialisation of the configuration centre (in ConfigurationFactory.buildConfiguration()) is similar to the initialisation process of the registration centre, which is to get the type of the configuration centre and other information from the **meta-configuration file** first, and then initialise a specific instance of the configuration centre, which is no longer repeated here, with the foundation of the previous analysis.\\n\\n #### Getting the value of a configuration item\\n The two methods in the above snippet, *FileRegistryServiceImpl.lookup()* and *RegistryService.getServiceGroup()*, both get the values of the configuration items from the configuration centre:\\n - lookup() need to be implemented by the specific registry, the use of file as a registry, in fact, is a direct connection to the TC Server, the special point is that **TC Server\'s address is written to death in the configuration ** (normal should be stored in the registry), so FileRegistryServiceImpl.lookup() method, is the address information of the Server in the TC cluster obtained through the configuration centre.\\n - getServiceGroup() is the default method in the RegistryServer interface, which is the public implementation of all registries. Any kind of registry in Seata needs to be configured to get the TC cluster name based on the name of the transaction group.\\n\\n ### Load Balancing\\n After the above link configuration centre, registration centre collaboration, now we have obtained the current application side of all the available TC Server address, then before sending the real request, you also need to pass a specific load balancing policy, select a TC Server address, this part of the source code is relatively simple, will not take you to analyse.\\n\\n > About the load balancing source code, you can read AbstractNettyRemotingClient.doSelect(), because the code analysed in this article is the reconnection method of RMClient/TMClient, in this method, all the obtained Server addresses will be connected (reconnected) sequentially by traversing, so here There is no need to do load balancing.\\n\\n The above is the Seata application side in the startup process, the registration centre and configuration centre of the two key modules between the collaboration and workflow, welcome to discuss and learn together!\\n\\n > Postscript: This article and its predecessor [ Seata client startup process dissection (a)](http://booogu.top/2021/02/28/seata-client-start-analysis-01/), is the first batch of technical blogs written by me, will be on the hands of Seata, I personally believe that Seata in the more complex, need to study and figure out. When I started Seata, I have analysed and documented some of the more complex parts of Seata\'s source code that I think need to be researched and figured out.\\n I welcome any suggestions for improvement from readers, thank you!"},{"id":"/seata-client-start-analysis-01","metadata":{"permalink":"/blog/seata-client-start-analysis-01","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-client-start-analysis-01.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-client-start-analysis-01.md","title":"Analysis of Seata Application-Side Startup Process - How RM & TM Establish Connections with TC","description":"\\"Just started with Seata and don\'t have a deep understanding of its various modules?","date":"2021-02-28T21:08:00.000Z","formattedDate":"February 28, 2021","tags":[{"label":"Seata","permalink":"/blog/tags/seata"}],"readingTime":8.15,"hasTruncateMarker":false,"authors":[{"name":"booogu"}],"frontMatter":{"layout":"post","comments":true,"title":"Analysis of Seata Application-Side Startup Process - How RM & TM Establish Connections with TC","date":"2021-02-28T21:08:00.000Z","author":"booogu","catalog":true,"tags":["Seata"]},"unlisted":false,"prevItem":{"title":"Seata Application-Side Startup Process Analysis \u2014 Registry and Configuration Module","permalink":"/blog/seata-client-start-analysis-02"},"nextItem":{"title":"Integration of Spring Cloud with Seata for Distributed Transaction - TCC Mode","permalink":"/blog/integrate-seata-tcc-mode-with-spring-cloud"}},"content":"> \\"Just started with Seata and don\'t have a deep understanding of its various modules? <br />\\n> Want to delve into Seata\'s source code but haven\'t taken the plunge yet? <br />\\n> Curious about what your application does \'secretly\' during startup after integrating Seata? <br />\\n> Want to learn about the design principles and best practices embodied in Seata as an excellent open-source framework? <br />\\n> If you have any of the above thoughts, then this article is tailor-made for you~\\n\\n## Introduction\\n\\nThose who have seen the first picture in the official README should know that Seata coordinates distributed transactions through its **coordinator side** TC, which communicates and interacts with the **application side** TM and RM to ensure data consistency among multiple transaction participants in distributed transactions. So, how does Seata establish connections and communicate between the coordinator side and the application side?\\n\\nThat\'s right, the answer is Netty. Netty, as a high-performance RPC communication framework, ensures efficient communication between TC and RM. This article will not go into detail about Netty; instead, our focus today is on how the **application side, during startup, uses a series of Seata\'s key modules (such as RPC, Config/Registry Center, etc.) to establish communication with the coordinator side.\\n\\n## Starting with GlobalTransactionScanner\\n\\nWe know that Seata provides several development annotations, such as @GlobalTransactional for enabling distributed transactions, @TwoPhraseBusinessAction for declaring TCC two-phase services, and so on, which are based on the Spring AOP mechanism to enhance the annotations by assigning the corresponding bean methods to interceptors. Interceptors are enhanced to complete the corresponding processing logic. GlobalTransactionScanner, a Spring bean, carries the responsibility of assigning interceptors to annotations. From the name of its scanner, it is not difficult to deduce that it is designed for the startup of the Spring application, and the global transaction (GlobalTransactionScanner). GlobalTransactionScanner) during Spring application startup.\\n\\nIn addition, the process of initialising the application-side RPC clients (TMClient, RMClient) and establishing a connection with the TC is also initiated in GlobalTransactionScanner#afterPropertiesSet():\\n\\n```js\\n/**\\n* package: io.seata.spring.annotation\\n* class: GlobalTransactionScanner\\n*/\\n@Override\\npublic void afterPropertiesSet() {\\nif (disableGlobalTransaction) {\\nif (LOGGER.isInfoEnabled()) {\\nLOGGER.info(\\"Global transaction is disabled.\\");\\n}\\nreturn.\\n}\\n// Perform TM, RM initialisation after the bean properties are initialised\\ninitClient();\\n\\n    }\\n```\\n\\n## Initialisation and connection process of RM & TM\\n\\nHere, we take RMClient.init() as an example, and the initialisation process of TMClient is the same.\\n\\n### Design of class relationship\\n\\nLooking at the source code of RMClient#init(), we find that RMClient first **constructs** an RmNettyRemotingClient, and then executes its **initialisation** init() method. The **constructor** and **initialisation** methods of RmNettyRemotingClient call the constructor and initialisation methods of the parent class layer by layer\\n\\n```js\\n    /**\\n     * RMClient\'s initialisation logic\\n     * package: io.seata.rm\\n     * class: RMClient\\n     */\\n    public static void init(String applicationId, String transactionServiceGroup) {\\n        //\u2460 Start with the RmNettyRemotingClient class and call the constructor of the parent class in turn\\n\\n        rmNettyRemotingClient.setResourceManager(DefaultResourceManager.get());\\n        rmNettyRemotingClient.setTransactionMessageHandler(DefaultRMHandler.get()); rmNettyRemotingClient.setTransactionMessageHandler(DefaultRMHandler.get());\\n        //\u2461 Then, starting with the RmNettyRemotingClient class, call init() of the parent class in turn\\n        rmNettyRemotingClient.init();\\n    }\\n```\\n\\nThe relationship between the above RMClient family classes and the process of calling the constructor and init() initialisation method is illustrated in the following diagram:\\n![Relationship between the simplified version of the RMClient.init process and the main classes](http://booogu.top/img/in-post/rmclient_relation.jpg)\\n\\nSo why did you design RMClient with such a more complex inheritance relationship? In fact, it is in order to divide the responsibilities and boundaries of each layer clearly, so that each layer can focus on specific logic processing, to achieve better scalability, this part of the detailed design ideas, you can refer to the Seata RPC module refactoring PR of the operator by Hui brother\'s article! [The Road to Seata-RPC Refactoring](https://mp.weixin.qq.com/s/PCSZ4a8cgmyZNhbUrO-BZQ))\\n\\n### The complete flow of initialisation\\n\\nThe main logic in the constructor and initialisation methods of each class can be sorted out with the help of the following ideographic sequence diagram, which can also be skipped first, and then looked back to see when these classes debut and how they interact with each other after we have analysed a few key classes below.\\n![Initialisation flow of RMClient](http://booogu.top/img/in-post/rmclient_initialization.png)\\n\\n### Grabbing the core - Channel creation\\n\\nFirst of all, we need to know that the communication between the application side and the coordinator side is done with the help of Netty\'s Channel, so the key to the communication process lies in the creation of the Channel**, which is created and managed in Seata by means of pooling (with the help of the object pool in common-pool).\\n\\nHere we need to briefly introduce the simple concept of object pool and its implementation in Seata:\\nThe main classes in common-pool are involved:\\n\\n- **GenericKeydObjectPool\\\\<K, V>**: A KV generic object pool that provides access to all objects, while object creation is done by its internal factory class.\\n- **KeyedPoolableObjectFactory\\\\<K, V>**: KV generic object factory responsible for the creation of pooled objects, held by the object pool\\n\\nThe main classes involved are related to the implementation of object pooling in Seata:\\n\\n- First, the pooled objects are **Channel**, which corresponds to the generic V in common-pool.\\n- **NettyPoolKey**: Key for Channel, corresponding to generic K in common-pool, NettyPoolKey contains two main information:\\n  - _address_:Address of TC Server when the Channel is created.\\n  - _message_:The RPC message sent to TC Server when the Channel is created.\\n- **GenericKeydObjectPool\\\\<NettyPoolKey,Channel>**: Pool of Channel objects.\\n- **NettyPoolableFactory**: the factory class for creating Channel.\\n  Having recognised the main classes related to object pooling above, let\'s take a look at some of the main classes in Seata that are involved in channel management and are related to RPC:\\n\\n- NettyClientChannelManager:\\n  - Holds the pool of Channel objects.\\n  - Interacts with the channel object pool to manage application-side channels (acquisition, release, destruction, caching, etc.).\\n- RpcClientBootstrap: core bootstrap class for RPC clients, holds the Netty framework bootstrap object with start/stop capability; has the ability to get a new Channel based on the connection address for the Channel factory class to call.\\n- AbstractNettyRemotingClient:\\n  - Initialises and holds the RpcClientBootstrap.\\n  - Application-side Netty client top-level abstraction, abstracts the ability of application-side RM/TM to obtain the NettyPoolKey corresponding to their respective Channel, for NettyClientChannelManager to call.\\n  - Initialising the NettyPoolableFactory\\n\\nUnderstanding the above concepts, we can simplify the process of creating a channel in Seata as follows:\\n![Process of creating a Channel object](http://booogu.top/img/in-post/create_channel.jpg)\\n\\nWhen you see this, you can go back and take a look at the above **Initialisation Sequence Diagram for RMClient**, and you should have a clearer understanding of the responsibilities and relationships of the various categories in the diagram, as well as the intent of the entire initialisation process.\\n\\n### Timing and flow of establishing a connection\\n\\nSo, when does RMClient establish a connection with Server?\\n\\nDuring the initialisation of RMClient, you will find that many init() methods set up some timed tasks, and the mechanism of reconnecting (connecting) the Seata application side to the coordinator is achieved through timed tasks:\\n\\n```js\\n/**\\n* package: io.seata.core.rpcn.netty\\n* class: AbstractNettyRemotingClient\\n*/\\npublic void init() {\\n// Set the timer to reconnect to the TC Server at regular intervals.\\ntimerExecutor.scheduleAtFixedRate(new Runnable() {\\n@Override\\npublic void run() {\\nclientChannelManager.reconnect(getTransactionServiceGroup());\\n}\\n}, SCHEDULE_DELAY_MILLS, SCHEDULE_INTERVAL_MILLS, TimeUnit.MILLISECONDS);\\nif (NettyClientConfig.isEnableClientBatchSendRequest()) {\\nmergeSendExecutorService = new ThreadPoolExecutor(MAX_MERGE_SEND_THREAD,\\nMAX_MERGE_SEND_THREAD,\\nKEEP_ALIVE_TIME, TimeUnit.\\nnew LinkedBlockingQueue<>(),\\nnew NamedThreadFactory(getThreadPrefix(), MAX_MERGE_SEND_THREAD));\\nmergeSendExecutorService.submit(new MergedSendRunnable());\\n}\\nsuper.init();\\nclientBootstrap.start();\\n}\\n```\\n\\nLet\'s see how the classes we explored above work together to connect RMClient to TC by tracing the execution of a reconnect (the first connection may actually occur during registerResource, but the process is the same)\\n![RMClient and TC Server connection process](http://booogu.top/img/in-post/rmclient_connect_tcserver.png)\\n\\nIn this diagram, you can focus on these points:\\n\\n- NettyClientChannelManager executes the callback function (getPoolKeyFunction()) to get the NettyPoolKey in the concrete AbstractNettyRemotingClient: the different Clients (RMClient and TMClient) on the application side, when they create the NettyPoolKey, they create the NettyChannelManager. TMClient) on the application side, the Key used when creating the Channel is different, so that **they send different registration messages when reconnecting to the TC Server**, which is also determined by the different roles they play in Seata:\\n  - TMClient: plays the role of transaction manager, when creating a Channel, it only sends a TM registration request (RegisterTMRequest) to the TC.\\n  - RMClient: plays the role of resource manager, needs to manage all transaction resources on the application side, therefore, when creating a Channel, it needs to get all transaction resource information on the application side before sending RM registration request (RegisterRMRequest), and register it to TC Server.\\n- In the Channel object factory\'s `NettyPoolableFactory`\'s `makeObject` (create Channel) method, two tasks are completed using the two pieces of information in `NettyPoolKey`:\\n  - A new Channel is created using the address from `NettyPoolKey`.\\n  - A registration request is sent to the TC Server using the message from `NettyPoolKey` and the new Channel. This is the Client\'s initial connection (first execution) or reconnection (subsequent executions driven by scheduled tasks) request to the TC Server.\\n\\nThe above content covers the entire process of the Seata application\'s initialization and its connection establishment with the TC Server coordinator side.\\n\\nFor deeper details, it is recommended to thoroughly read the source code based on the outline and key points mentioned in this article. This will undoubtedly lead to a deeper understanding and new insights!\\n\\n> Postscript: Considering the length and to maintain a suitable amount of information for a source code analysis article, the **collaboration of configuration and registration modules** mentioned in the introduction was not expanded upon in this article. <br />\\n> In the next source code analysis, I will focus on the **configuration center** and **registration center**, analyzing how the Seata application side **discovers the TC Server through service discovery** and how it **obtains various information from the configuration module** before establishing connections between RMClient/TM Client and the TC Server."},{"id":"/integrate-seata-tcc-mode-with-spring-cloud","metadata":{"permalink":"/blog/integrate-seata-tcc-mode-with-spring-cloud","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/integrate-seata-tcc-mode-with-spring-cloud.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/integrate-seata-tcc-mode-with-spring-cloud.md","title":"Integration of Spring Cloud with Seata for Distributed Transaction - TCC Mode","description":"This article mainly introduces the integration of Spring Cloud with Seata for distributed transaction using the TCC mode.","date":"2021-01-23T00:00:00.000Z","formattedDate":"January 23, 2021","tags":[],"readingTime":5.76,"hasTruncateMarker":false,"authors":[{"name":"gongxing(zhijian.tan)"}],"frontMatter":{"title":"Integration of Spring Cloud with Seata for Distributed Transaction - TCC Mode","keywords":["TCC","Seata","Spring Cloud","Distributed","Transaction"],"description":"This article mainly introduces the integration of Spring Cloud with Seata for distributed transaction using the TCC mode.","author":"gongxing(zhijian.tan)","date":"2021-01-23T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Analysis of Seata Application-Side Startup Process - How RM & TM Establish Connections with TC","permalink":"/blog/seata-client-start-analysis-01"},"nextItem":{"title":"Analysis of Seata Configuration Management Principles","permalink":"/blog/seata-config-manager"}},"content":"This article will introduce how to integrate Seata (1.4.0) with Spring Cloud and Feign using the TCC mode. In practice, Seata\'s AT mode can meet about 80% of our distributed transaction needs. However, when dealing with operations on databases and middleware (such as Redis) that do not support transactions, or when using databases that are not currently supported by the AT mode (currently AT supports MySQL, Oracle, and PostgreSQL), cross-company service invocations, cross-language application invocations, or the need for manual control of the entire two-phase commit process, we need to combine the TCC mode. Moreover, the TCC mode also supports mixed usage with the AT mode.\\n\\n\\n# \u4e00\u3001The concept of TCC mode\\n\\nIn Seata, a distributed global transaction follows a two-phase commit model with a Try-[Confirm/Cancel] pattern. Both the AT (Automatic Transaction) mode and the TCC (Try-Confirm-Cancel) mode in Seata are implementations of the two-phase commit. The main differences between them are as follows:\\n\\nAT mode is based on relational databases that support local ACID transactions (currently supporting MySQL, Oracle, and PostgreSQL):\\n\\nThe first phase, prepare: In the local transaction, it combines the submission of business data updates and the recording of corresponding rollback logs.\\nThe second phase, commit: It immediately completes successfully and automatically asynchronously cleans up the rollback logs.\\nThe second phase, rollback: It automatically generates compensation operations through the rollback logs to complete data rollback.\\n\\nOn the other hand, TCC mode does not rely on transaction support from underlying data resources:\\n\\nThe first phase, prepare: It calls a custom-defined prepare logic.\\nThe second phase, commit: It calls a custom-defined commit logic.\\nThe second phase, rollback: It calls a custom-defined rollback logic.\\n\\nTCC mode refers to the ability to include custom-defined branch transactions in the management of global transactions.\\n\\nIn summary, Seata\'s TCC mode is a manual implementation of the AT mode that allows you to define the processing logic for the two phases without relying on the undo_log used in the AT mode.\\n\\n# \u4e8c\u3001prepare\\n\\n- regist center [nacos](https://nacos.io/zh-cn/ \\"nacos\\") \\n- [seata server(TC\uff09](/docs/ops/deploy-guide-beginner/ \\"seata\u670d\u52a1\u7aef(TC\uff09\\")\\n\\n\\n# \u4e09\u3001Building TM and TCC-RM\\n\\nThis chapter focuses on the implementation of TCC using Spring Cloud + Feign. For the project setup, please refer to the source code (this project provides demos for both AT mode and TCC mode).\\n\\n[DEMO](https://github.com/tanzzj/springcloud-seata-feign \\"\u670d\u52a1\u7aef\u642d\u5efa\u6587\u6863\\")\\n\\n## 3.1 build seata server \\n\\n[build server doc](/docs/ops/deploy-guide-beginner/ \\"\u670d\u52a1\u7aef\u642d\u5efa\u6587\u6863\\")\\n\\n## 3.2 build TM\\n\\n[service-tm](https://github.com/tanzzj/springcloud-seata-feign/tree/master/service-tm)\\n\\n## 3.3 build RM-TCC\\n\\n### 3.3.1 Defining TCC Interface\\n\\nSince we are using Spring Cloud + Feign, which relies on HTTP for communication, we can use @LocalTCC here. It is important to note that @LocalTCC must be annotated on the interface. This interface can be a regular business interface as long as it implements the corresponding methods for the two-phase commit in TCC. The TCC-related annotations are as follows:\\n\\n- @LocalTCC: Used for TCC in the Spring Cloud + Feign mode.\\n- @TwoPhaseBusinessAction: Annotates the try method. The name attribute represents the bean name of the current TCC method, which can be the method name (globally unique). The commitMethod attribute points to the commit method, and the rollbackMethod attribute points to the transaction rollback method. After specifying these three methods, Seata will automatically invoke the commit or rollback method based on the success or failure of the global transaction.\\n- @BusinessActionContextParameter: Annotates the parameters to be passed to the second phase (commitMethod/rollbackMethod) methods.\\n- BusinessActionContext: Represents the TCC transaction context.\\n\\nHere is an example:\\n\\n```java\\n/**\\n * Here we define the TCC interface.\\n * It must be defined on the interface.\\n * We are using Spring Cloud for remote invocation.\\n * Therefore, we can use LocalTCC here.\\n *\\n */\\n@LocalTCC\\npublic interface TccService {\\n \\n    /**\\n     * Define the two-phase commit.\\n     * name = The bean name of this TCC, globally unique.\\n     * commitMethod = The method for the second phase confirmation.\\n     * rollbackMethod = The method for the second phase cancellation.\\n     * Use the BusinessActionContextParameter annotation to pass parameters to the second phase.\\n     *\\n     * @param params  \\n     * @return String\\n     */\\n    @TwoPhaseBusinessAction(name = \\"insert\\", commitMethod = \\"commitTcc\\", rollbackMethod = \\"cancel\\")\\n    String insert(\\n            @BusinessActionContextParameter(paramName = \\"params\\") Map<String, String> params\\n    );\\n \\n    /**\\n     *  The confirmation method can be named differently, but it must be consistent with the commitMethod.\\n     *  The context can be used to pass the parameters from the try method.\\n     * @param context \\n     * @return boolean\\n     */\\n    boolean commitTcc(BusinessActionContext context);\\n \\n    /**\\n     * two phase cancel\\n     *\\n     * @param context \\n     * @return boolean\\n     */\\n    boolean cancel(BusinessActionContext context);\\n}\\n```\\n\\n### 3.3.2 Business Implementation of TCC Interface\\n\\nTo keep the code concise, we will combine the routing layer with the business layer for explanation here. However, in actual projects, this may not be the case.\\n\\n- Using @Transactional in the try method allows for direct rollback of operations in relational databases through Spring transactions. The rollback of operations in non-relational databases or other middleware can be handled in the rollbackMethod.\\n- By using context.getActionContext(\\"params\\"), you can retrieve the parameters defined in the try phase and perform business rollback operations on these parameters in the second phase.\\n- Note 1: It is not advisable to catch exceptions here (similarly, handle exceptions with aspects), as doing so would cause TCC to recognize the operation as successful, and the second phase would directly execute the commitMethod.\\n- Note 2: In TCC mode, it is the responsibility of the developer to ensure idempotence and transaction suspension prevention.\\n\\n```java\\n@Slf4j\\n@RestController\\npublic class TccServiceImpl implements  TccService {\\n \\n    @Autowired\\n    TccDAO tccDAO;\\n \\n    /**\\n     * tcc t\uff08try\uff09method\\n     * Choose the actual business execution logic or resource reservation logic based on the actual business scenario.\\n     *\\n     * @param params - name\\n     * @return String\\n     */\\n    @Override\\n    @PostMapping(\\"/tcc-insert\\")\\n    @Transactional(rollbackFor = Exception.class, propagation = Propagation.REQUIRED)\\n    public String insert(@RequestBody Map<String, String> params) {\\n        log.info(\\"xid = \\" + RootContext.getXID());\\n        //todo Perform actual operations or operations on MQ, Redis, etc.\\n        tccDAO.insert(params);\\n        //Remove the following annotations to throw an exception\\n        //throw new RuntimeException(\\"\u670d\u52a1tcc\u6d4b\u8bd5\u56de\u6eda\\");\\n        return \\"success\\";\\n    }\\n \\n    /**\\n     * TCC service confirm method\\n     * If resource reservation is used in the first phase, the reserved resources should be committed during the second phase confirmation\\n     * @param context \\n     * @return boolean\\n     */\\n    @Override\\n    public boolean commitTcc(BusinessActionContext context) {\\n        log.info(\\"xid = \\" + context.getXid() + \\"\u63d0\u4ea4\u6210\u529f\\");\\n        //todo If resource reservation is used in the first phase, resources should be committed here.\\n        return true;\\n    }\\n \\n    /**\\n     * tcc  cancel method\\n     *\\n     * @param context \\n     * @return boolean\\n     */\\n    @Override\\n    public boolean cancel(BusinessActionContext context) {\\n        //todo Here, write the rollback operations for middleware or non-relational databases.\\n        System.out.println(\\"please manually rollback this data:\\" + context.getActionContext(\\"params\\"));\\n        return true;\\n    }\\n}\\n```\\n\\n### 3.3.3 Starting a Global Transaction in TM and Invoking RM-TCC Interface\\n\\nPlease refer to the project source code in section 3.2.\\n\\nWith this, the integration of TCC mode with Spring Cloud is complete."},{"id":"/seata-config-manager","metadata":{"permalink":"/blog/seata-config-manager","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-config-manager.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-config-manager.md","title":"Analysis of Seata Configuration Management Principles","description":"This article primarily introduces the core implementation of Seata configuration management and the interaction process with Spring configuration.","date":"2021-01-10T00:00:00.000Z","formattedDate":"January 10, 2021","tags":[],"readingTime":9.945,"hasTruncateMarker":false,"authors":[{"name":"xiaoyong.luo"}],"frontMatter":{"title":"Analysis of Seata Configuration Management Principles","keywords":["Seata","configuration center","configuration management","Spring configuration"],"description":"This article primarily introduces the core implementation of Seata configuration management and the interaction process with Spring configuration.","author":"xiaoyong.luo","date":"2021/01/10"},"unlisted":false,"prevItem":{"title":"Integration of Spring Cloud with Seata for Distributed Transaction - TCC Mode","permalink":"/blog/integrate-seata-tcc-mode-with-spring-cloud"},"nextItem":{"title":"Detailed Explanation of seata-golang Communication Model","permalink":"/blog/seata-golang-communication-mode"}},"content":"When it comes to Seata configuration management, you may think of Seata in the adaptation of the various configuration centre, in fact, today to say that this is not the case, although it will also be a simple analysis of Seata and the process of adapting to the configuration centre, but the main still explain the core implementation of Seata configuration management\\n\\n# Server startup process\\nBefore talking about the configuration centre, first briefly introduce the startup process of the Server side, because this piece involves the initialisation of the configuration management, the core process is as follows:\\n1. The entry point of the process is in the `Server#main` method.\\n2. several forms of obtaining the port: from the container; from the command line; the default port\\n3. Parse the command line parameters: host, port, storeMode and other parameters, this process may be involved in the initialisation of the configuration management\\n4. Metric-related: irrelevant, skip\\n5. NettyServer initialisation\\n6. core controller initialisation: the core of the Server side, but also includes a few timed tasks\\n7. NettyServer startup\\n\\n\\nThe code is as follows, with non-core code removed\\n```java\\npublic static void main(String[] args) throws IOException {\\n// Get the port in several forms: from the container; from the command line; the default port, use to logback.xml\\nint port = PortHelper.getPort(args);\\nSystem.setProperty(ConfigurationKeys.SERVER_PORT, Integer.toString(port));\\n\\n    // Parsing startup parameters, container and non-container.\\n    ParameterParser parameterParser = new ParameterParser(args); // Parsing startup parameters, both container and non-container.\\n\\n    // Metric-related\\n    MetricsManager.get().init(); // MetricsManager.get().init(); // NettyServer initialisation.\\n\\n    // NettyServer initialisation\\n    NettyRemotingServer nettyRemotingServer = new NettyRemotingServer(workingThreads); // NettyServer initialisation.\\n\\n    // Core controller initialisation\\n    DefaultCoordinator coordinator = new DefaultCoordinator(nettyRemotingServer); // Core controller initialisation.\\n    coordinator.init(); // Initialise the core controller.\\n\\n    // NettyServer startup\\n    nettyRemotingServer.init(); // NettyServer starts.\\n}\\n``\\n\\nWhy does ``step 3`` involve the initialisation of the configuration management? The core code is as follows:\\n ```java\\n if (StringUtils.isBlank(storeMode)) {\\n    storeMode = ConfigurationFactory.getInstance().getConfig(ConfigurationKeys.STORE_MODE,\\n        SERVER_DEFAULT_STORE_MODE);\\n }\\n ```\\nIf `storeMode` is not specifically specified in the startup parameters, the configuration will be fetched through the `ConfigurationFactory` related API, which involves two parts in the `ConfigurationFactory.getInstance()` line of code: ` ConfigurationFactory` static code initialisation and `Configuration` initialisation. Let\'s focus on analysing this part\\n\\n## Configuration management initialisation\\n\\n## ConfigurationFactory initialisation\\nWe know that there are two key configuration files in Seata: `registry.conf`, which is the core configuration file and must be there, and `file.conf`, which is only needed if the configuration centre is `File`. The `ConfigurationFactory` static code block actually mainly loads the `registry.conf` file, roughly as follows:\\n\\n1, in the case of no manual configuration, the default read `registry.conf` file, encapsulated into a `FileConfiguration` object, the core code is as follows:\\n```java\\nConfiguration configuration = new FileConfiguration(seataConfigName,false);\\nFileConfigFactory.load(\\"registry.conf\\", \\"registry\\");\\nFileConfig fileConfig = EnhancedServiceLoader.load(FileConfig.class, \\"CONF\\", argsType, args);\\n ```\\n\\n2\u3001Configuration enhancement: similar to the proxy model, to get the configuration, do some other processing inside the proxy object, the following details\\n ```java\\n Configuration extConfiguration = EnhancedServiceLoader.load(ExtConfigurationProvider.class).provide(configuration);\\n ```\\n\\n3. Assign the proxy object in step 2 to the `CURRENT_FILE_INSTANCE` reference, which is used directly in many places as a static reference to `CURRENT_FILE_INSTANCE`.\\n```java\\n   CURRENT_FILE_INSTANCE = extConfiguration == null ? configuration : extConfiguration;\\n ```\\n\\nIt\'s easy to assume that `CURRENT_FILE_INSTANCE` corresponds to the contents of `registry.conf`. I don\'t think `registry.conf` is a good name for the file, it\'s too ambiguous, would it be better to call it `bootstrap.conf`?\\n\\n\\n## Configuration initialisation\\n`Configuration` actually corresponds to the configuration centre, Seata currently supports a lot of configuration centres, almost all the mainstream configuration centres are supported, such as: apollo, consul, etcd, nacos, zk, springCloud, local files. When using local files as a configuration centre, it involves the loading of `file.conf`, which of course is the default name and can be configured by yourself. By now, you basically know the relationship between `registry.conf` and `file.conf`.\\n\\n`Configuration` as a single instance in `ConfigurationFactory`, so the initialisation logic of `Configuration` is also in `ConfigurationFactory`, the approximate process is as follows:\\n1, first read the `config.type` attribute from the `registry.conf` file, which is `file` by default.\\n ```java\\n configTypeName = CURRENT_FILE_INSTANCE.getConfig(ConfigurationKeys.FILE_ROOT_CONFIG + ConfigurationKeys.FILE_CONFIG_SPLIT_CHAR+ ConfigurationKeys.FILE_ROOT_TYPE);\\n ```\\n2. Load the configuration centre based on the value of the `config.type` attribute, e.g., the default is: `file`, then first read the `registry.conf` file from `config.file.name` to read the corresponding file name of the local file configuration centre, and then create a `FileConfiguration` object based on the name of the file. This loads the configuration in `file.conf` into memory. Similarly, if the configuration is for another Configuration Centre, the other Configuration Centre will be initialised via SPI. Another thing to note is that at this stage, if the configuration centre is a local file, a proxy object is created for it; if it is not a local file, the corresponding configuration centre is loaded via SPI\\n ```java\\n if (ConfigType.File == configType) {\\n    String pathDataId = String.join(\\"config.file.name\\");\\n    String name = CURRENT_FILE_INSTANCE.getConfig(pathDataId);\\n    String name = CURRENT_FILE_INSTANCE.getConfig(pathDataId); configuration = new FileConfiguration(name);\\n    try {\\n        // Configure the Enhanced Proxy\\n        extConfiguration = EnhancedServiceLoader.load(ExtConfigurationProvider.class).provide(configuration); } catch (Exception e) { { new FileConfiguration(name); configuration = new FileConfiguration(name); }\\n    } catch (Exception e) {\\n        LOGGER.error(\\"failed to load extConfiguration:{}\\", e.getMessage(), e);\\n    }\\n } else {\\n    configuration = EnhancedServiceLoader\\n            .load(ConfigurationProvider.class, Objects.requireNonNull(configType).name()).provide();\\n }\\n ```\\n\\n3, based on the `Configuration` object created in step 2, create another layer of proxy for it, this proxy object has two roles: one is a local cache, you do not need to get the configuration from the configuration every time you get the configuration; the other is a listener, when the configuration changes will update the cache it maintains. The following:\\n ```java\\n if (null ! = extConfiguration) {\\n    configurationCache = ConfigurationCache.getInstance().proxy(extConfiguration);\\n } else {\\n    configurationCache = ConfigurationCache.getInstance().proxy(configuration);\\n }\\n ```\\n\\nAt this point, the initialisation of the configuration management is complete. **Seata initialises the configuration centre by first loading the `registry.conf` file to get the corresponding configuration centre information, the registry, and then initialising the configuration centre based on the corresponding information obtained. In the case of using a local file as the configuration centre, the default is to load the `file.conf` file. Then create a proxy object for the corresponding configuration centre to support local caching and configuration listening**.\\n\\nThe finishing process is still relatively simple, so I\'m going to throw out a few questions here:\\n1. what is configuration enhancement and how is it done in Seata?\\n2. if using a local file as a configuration centre, the configuration has to be defined in the `file.conf` file. If it is a Spring application, can the corresponding configuration items be defined directly in `application.yaml`?\\n3. In step 2 mentioned above, why is it necessary to create the corresponding configuration enhancement proxy object for `Configuration` first in the case of using a local file configuration centre, but not for other configuration centres?\\n\\nThese 3 questions are all closely related and are all related to Seata\'s configuration additions. Here are the details\\n\\n# Configuration Management Enhancements\\nConfiguration enhancement, in simple terms, is to create a proxy object for which proxy logic is executed when executing the target method of the target unique object, and from the perspective of the configuration centre, it is to execute the proxy logic when obtaining the corresponding configuration through the configuration centre.\\n\\n1. get the configuration through `ConfigurationFactory.CURRENT_FILE_INSTANCE.getgetConfig(key)`.\\n2. Load the `registry.conf` file to create the FileConfiguration object `configuration`.\\n3. Create a proxy object `configurationProxy` for `configuration` based on `SpringBootConfigurationProvider`.\\n4. Get the configuration centre connection information `file zk nacos etc` from `configurationProxy`.\\n5. Create a configuration centre configuration object `configuration2` based on the connection information.\\n6. Create a proxy object `configurationProxy2` for `configuration2` based on `SpringBootConfigurationProvider`.\\n7. Execute the proxy logic for `configurationProxy2`.\\n8. Find the corresponding SpringBean based on the key.\\n9. Execute the getXxx method of the SpringBean.\\n\\n! [](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0cb93fec40df40ba9e8ab9db06cc9b93~tplv-k3u1fbpfcp-watermark.image)\\n\\n## Configuration Enhancement Implementation\\nConfiguration enhancement was also briefly mentioned above and the related code is as follows:\\n ```java\\n EnhancedServiceLoader.load(ExtConfigurationProvider.class).provide(configuration);\\n ```\\n1. First get an `ExtConfigurationProvider` object through the SPI machine, there is only one implementation in Seata by default: `SpringBootConfigurationProvider`.\\n2. Through the `ExtConfigurationProvider#provider` method to create a proxy object for the `Configuration`.\\n\\nThe core code is as follows.\\n```java\\npublic Configuration provide(Configuration originalConfiguration) {\\nreturn (Configuration) Enhancer.create(originalConfiguration.getClass(), new MethodInterceptor() {\\n@Override\\npublic Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy)\\nthrows Throwable {\\nif (method.getName().startsWith(\\"get\\") && args.length > 0) {\\nObject result = null;\\nString rawDataId = (String) args[0];\\nif (args.length == 1) {\\nresult = get(convertDataId(rawDataId));\\n} else if (args.length == 2) {\\nresult = get(convertDataId(rawDataId), args[1]); } else if (args.length == 2) { result = get(convertDataId(rawDataId))\\n} else if (args.length == 3) {\\nresult = get(convertDataId(rawDataId), args[1], (Long) args[2]); } else if (Long) args.length == 3); }\\n}\\nif (result ! = null) {\\n//If the return type is String,need to convert the object to string\\nif (method.getReturnType().equals(String.class)) {\\nreturn String.valueOf(result); }\\n}\\nreturn result; }\\n}\\n}\\n\\n            return method.invoke(originalConfiguration, args); }\\n        }\\n    }); }\\n}\\n\\nprivate Object get(String dataId) throws IllegalAccessException, InstantiationException {\\nString propertyPrefix = getPropertyPrefix(dataId); }; private Object get(String dataId); }; }; }\\nString propertySuffix = getPropertySuffix(dataId);\\nApplicationContext applicationContext = (ApplicationContext) ObjectHolder.INSTANCE.getObject(OBJECT_KEY_SPRING_APPLICATION_CONTEXT);\\nClass<? > propertyClass = PROPERTY_BEAN_MAP.get(propertyPrefix);\\nObject valueObject = null;\\nif (propertyClass ! = null) {\\ntry {\\nObject propertyBean = applicationContext.getBean(propertyClass);\\nvalueObject = getFieldValue(propertyBean, propertySuffix, dataId);\\n} catch (NoSuchBeanDefinitionException ignore) {\\n\\n        }\\n    } else {\\n        throw new ShouldNeverHappenException(\\"PropertyClass for prefix: [\\" + propertyPrefix + \\"] should not be null.\\"); }\\n    }\\n    if (valueObject == null) {\\n        valueObject = getFieldValue(propertyClass.newInstance(), propertySuffix, dataId);\\n    }\\n\\n    return valueObject; }\\n}\\n ```\\n1, if the method starts with `get` and the number of arguments is 1/2/3, then perform the other logic of getting the configuration, otherwise perform the logic of the native `Configuration` object\\n2, we do not need to bother why this rule, this is a Seata agreement\\n3, `Other logic to get the configuration`, that is, through the Spring way to get the corresponding configuration value\\n\\nHere has been clear about the principle of configuration enhancement, at the same time, can also be guessed that the only `ExtConfigurationProvider` implementation of `SpringBootConfigurationProvider`, must be related to the Spring\\n\\n\\n## Configuration Enhancement and Spring\\nBefore we introduce this piece, let\'s briefly describe how Seata is used:\\n1. Non-Starter way: introduce dependency `seata-all`, then manually configure a few core beans.\\n2. Starter way: Introduce the dependency `seata-spring-boot-starter`, fully automated quasi-configuration, do not need to automatically inject the core bean\\n\\nThe `SpringBootConfigurationProvider` is in the `seata-spring-boot-starter` module, i.e. when we use Seata by introducing `seata-all`, the configuration enhancement doesn\'t really do anything, because at this point there is no `ExtConfigurationProvider` to be found. ExtConfigurationProvider` implementation class can\'t be found at this point, so naturally it won\'t be enhanced.\\n\\nSo how does `seata-spring-boot-starter` tie all this together?\\n\\n1. First, in the `resources/META-INF/services` directory of the `seata-spring-boot-starter` module, there exists a `spring.factors` file with the following contents\\n ```\\n org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\\\\n io.seata.spring.boot.autoconfigure.SeataAutoConfiguration,\\\\\\n\\n # Ignore for now\\n io.seata.spring.boot.autoconfigure.HttpAutoConfiguration\\n ```\\n\\n2, in the `SeataAutoConfiguration` file, the following beans will be created: GlobalTransactionScanner , SeataDataSourceBeanPostProcessor, SeataAutoDataSourceProxyCreator SpringApplicationContextProvider. The first three are not related to what we are going to talk about in this article, mainly focus on `SpringApplicationContextProvider`, the core code is very simple, is to save the `ApplicationContext`:\\n ```java\\n public class SpringApplicationContextProvider implements ApplicationContextAware {\\n    @Override\\n    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {\\n        ObjectHolder.INSTANCE.setObject(OBJECT_KEY_SPRING_APPLICATION_CONTEXT, applicationContext);\\n    }\\n }\\n ```\\n3. Then, in the `SeataAutoConfiguration` file, some `xxxProperties.Class` and the corresponding Key prefixes are also cached into `PROPERTY_BEAN_MAP`. The ``xxxProperties`` are simply understood as the various configuration items in `application.yaml`:\\n```java\\n   static {\\n   PROPERTY_BEAN_MAP.put(SEATA_PREFIX, SeataProperties.class);\\n   PROPERTY_BEAN_MAP.put(CLIENT_RM_PREFIX, RmProperties.class);\\n   PROPERTY_BEAN_MAP.put(SHUTDOWN_PREFIX, ShutdownProperties.class); ...\\n   ... Omit ...\\n   }\\n ```\\n\\nAt this point, the whole process is actually clear, when there is `SpringBootConfigurationProvider` configuration enhancement, we get a configuration item as follows:\\n1. first according to the `p Configuration Key` to get the corresponding `xxxProperties` object\\n2. get the corresponding `xxxProperties` SpringBean through the `ApplicationContext` in the `ObjectHolder`.\\n3. Get the value of the corresponding configuration based on the `xxxProperties` SpringBean.\\n4. At this point, we have successfully obtained the values in `application.yaml` through configuration enhancement."},{"id":"/seata-golang-communication-mode","metadata":{"permalink":"/blog/seata-golang-communication-mode","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-golang-communication-mode.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-golang-communication-mode.md","title":"Detailed Explanation of seata-golang Communication Model","description":"This article provides a detailed explanation of the underlying RPC communication implementation in seata-golang.","date":"2021-01-04T00:00:00.000Z","formattedDate":"January 4, 2021","tags":[],"readingTime":16.875,"hasTruncateMarker":false,"authors":[{"name":"xiaomin.liu"}],"frontMatter":{"title":"Detailed Explanation of seata-golang Communication Model","keywords":["seata","seata-golang","seata-go","getty","distributed transaction"],"description":"This article provides a detailed explanation of the underlying RPC communication implementation in seata-golang.","author":"xiaomin.liu","date":"2021/01/04"},"unlisted":false,"prevItem":{"title":"Analysis of Seata Configuration Management Principles","permalink":"/blog/seata-config-manager"},"nextItem":{"title":"Analysis of Seata Data Source Proxy","permalink":"/blog/seata-datasource-proxy"}},"content":"Author | Liu Xiaomin Yu Yu\\n\\n## I. Introduction\\n\\n\\nIn the Java world, netty is a widely used high-performance network communication framework, and many RPC frameworks are based on netty. In the golang world, [getty](https://github.com/AlexStocks/getty) is also a high-performance network communication library similar to netty. getty was originally developed by Yu Yu, the leader of the dubbogo project, and is available in [dubbo-go](https://) as an underlying communication library. github.com/apache/dubbo-go). With the donation of dubbo-go to the apache foundation, getty eventually made its way into the apache family and was renamed [dubbo-getty](https://github.com/apache/dubbo-getty), thanks to the efforts of the community.\\n\\n\\nIn \'18, I was practicing microservices in my company, and the biggest problem I encountered at that time was distributed transactions. In the same year, Ali open-sourced their distributed transaction solution in the community, and I quickly noticed this project, which was initially called fescar, but later renamed seata. Since I was very interested in open source technology, I added a lot of community groups, and at that time, I also paid attention to the dubbo-go project, and silently dived in it. As I learnt more about seata, the idea of making a go version of a distributed transaction framework gradually emerged.\\n\\n\\nTo make a golang version of distributed transaction framework, one of the first problems is how to achieve RPC communication. dubbo-go is a very good example in front of us, so we started to study the underlying getty of dubbo-go.\\n\\n\\n## How to implement RPC communication based on getty?\\n\\n\\nThe overall model of the getty framework is as follows:\\n\\n\\n! [image.png]( https://img.alicdn.com/imgextra/i1/O1CN011TIcL01jY4JaweOfV_! !6000000004559-2-tps-954-853.png)\\n\\n\\nThe following is a detailed description of the RPC communication process of seata-golang with related code.\\n\\n\\n### 1. Establish Connection\\n\\n\\nTo implement RPC communication, we need to establish a network connection first, let\'s start from [client.go](https://github.com/apache/dubbo-getty/blob/master/client.go).\\n\\n\\n ```go\\n func (c *client) connect() {\\n  var (\\n   err error\\n   ss Session\\n  )\\n\\n  for {\\n        // Create a session connection\\n   ss = c.dial()\\n   if ss == nil {\\n    if ss == nil { // client has been closed\\n    if ss == nil { // client has been closed\\n   }\\n   err = c.newSession(ss)\\n   if err == nil {\\n            // send and receive messages\\n    ss.(*session).run()\\n    // Omit some code here\\n\\n    break\\n   }\\n   // don\'t distinguish between tcp connection and websocket connection. because\\n   // gorilla/websocket/conn.go:(Conn)Close also invoke net.Conn.Close()\\n   ss.Conn().Close()\\n  Close()\\n }\\n ```\\n\\n\\n The `connect()` method gets a session connection via the `dial()` method into the dial() method:\\n\\n\\n ```\\nfunc (c *client) dial() Session {\\nswitch c.endPointType {\\ncase TCP_CLIENT.\\nreturn c.dialTCP()\\ncase UDP_CLIENT: return c.dialUDP()\\nreturn c.dialUDP()\\ncase WS_CLIENT: return c.dialWS()\\nreturn c.dialWS()\\ncase WSS_CLIENT: return c.dialWSS()\\nreturn c.dialWSS()\\n}\\n\\nreturn nil\\n}\\n```\\n\\n\\nWe\'re concerned with TCP connections, so we continue into the `c.dialTCP()` method:\\n\\n\\n ```go\\n func (c *client) dialTCP() Session {\\n  var (\\n   err error\\n   conn net.\\n  )\\n\\n  for {\\n   if c.IsClosed() {\\n    return nil\\n   }\\n   if c.sslEnabled {\\n    if sslConfig, err := c.tlsConfigBuilder.BuildTlsConfig(); err == nil && sslConfig ! = nil {\\n     d := &net.Dialer{Timeout: connectTimeout}\\n     // Establish an encrypted connection\\n     conn, err = tls.DialWithDialer(d, \\"tcp\\", c.addr, sslConfig)\\n    }\\n   } else {\\n            // Establish a tcp connection\\n    conn, err = net.DialTimeout(\\"tcp\\", c.addr, connectTimeout)\\n   }\\n   if err == nil && gxnet.IsSameAddr(conn.RemoteAddr(), conn.LocalAddr()) {\\n    conn.Close()\\n    err = errSelfConnect\\n   }\\n   if err == nil {\\n            // Return a TCPSession\\n    return newTCPSession(conn, c)\\n   }\\n\\n   log.Infof(\\"net.DialTimeout(addr:%s, timeout:%v) = error:%+v\\", c.addr, connectTimeout, perrors.WithStack(err))\\n   <-wheel.After(connectInterval)\\n  }\\n }\\n ```\\n\\n\\nAt this point, we know how getty establishes a TCP connection and returns a TCPSession.\\n\\n\\n### 2. Sending and Receiving Messages\\n\\n\\nHow does it send and receive messages? Let\'s go back to the connection method and look at the next line, which is `ss.(*session).run()`. After this line of code, the code is a very simple operation, so we guess that the logic of this line of code must include sending and receiving messages, and then go to the `run()` method:\\n\\n\\n```go\\nfunc (s *session) run() {\\n// Omit some of the code\\n\\ngo s.handleLoop()\\ngo s.handlePackage()\\n}\\n ```\\n\\n\\n There are two goroutines up here, `handleLoop` and `handlePackage`, which literally match our guesses into the `handleLoop()` method:\\n\\n\\n ```go\\n func (s *session) handleLoop() {\\n    // Omit some of the code\\n\\n  for {\\n   // A select blocks until one of its cases is ready to run.\\n   // It choose one at random if multiple are ready. Otherwise it choose default branch if none is ready.\\n   It choose one at random if multiple are ready.\\n   // Otherwise it choose default branch if none is ready.\\n\\n   case outPkg, ok = <-s.wQ.\\n    // Omit some of the code\\n\\n    iovec = iovec[:0]\\n    for idx := 0; idx < maxIovecNum; idx++ {\\n        // Encode interface{} type outPkg into binary bits via s.writer\\n     pkgBytes, err = s.writer.Write(s, outPkg)\\n     // Omit some of the code\\n\\n     iovec = append(iovec, pkgBytes)\\n\\n                // omit some code\\n    }\\n            // Send these binary bits out\\n    err = s.WriteBytesArray(iovec[:]...)\\n    if err ! = nil {\\n     log.Errorf(\\"%s, [session.handleLoop]s.WriteBytesArray(iovec len:%d) = error:%+v\\",\\n      s.sessionToken(), len(iovec), perrors.WithStack(err))\\n     s.stop()\\n     // break LOOP\\n     flag = false\\n    }\\n\\n   case <-wheel.After(s.period).\\n    if flag {\\n     if wsFlag {\\n      err := wsConn.writePing()\\n      if err ! = nil {\\n       log.Warnf(\\"wsConn.writePing() = error:%+v\\", perrors.WithStack(err))\\n      }\\n     }\\n                // Logic for timed execution, heartbeat, etc.\\n     s.listener.OnCron(s)\\n    }\\n   }\\n  }\\n }\\n ```\\n\\n\\nWith the above code, it is easy to see that the `handleLoop()` method handles the logic of sending the message, which is encoded into binary bits by `s.writer` and then sent over the established TCP connection. This `s.writer` corresponds to the Writer interface, which is an interface that must be implemented by the RPC framework.\\n\\n\\nMoving on to the `handlePackage()` method:\\n\\n\\n```go\\nfunc (s *session) handlePackage() {\\n// Omit some of the code\\n\\nif _, ok := s.Connection.(*gettyTCPConn); ok {\\nif s.reader == nil {\\nerrStr := fmt.Sprintf(\\"session{name:%s, conn:%#v, reader:%#v}\\", s.name, s.Connection, s.reader)\\nlog.Error(errStr)\\npanic(errStr)\\n}\\n\\nerr = s.handleTCPPackage()\\n} else if _, ok := s.Connection.(*gettyWSConn); ok {\\nerr = s.handleWSPackage()\\n} else if _, ok := s.Connection.(*gettyUDPConn); ok {\\nerr = s.handleUDPPackage()\\n} else {\\npanic(fmt.Sprintf(\\"unknown type session{%#v}\\", s))\\n}\\n}\\n```\\n\\n\\nGo to the `handleTCPPackage()` method:\\n\\n\\n ```go\\n func (s *session) handleTCPPackage() error {\\n    // Omit some of the code\\n\\n  conn = s.Connection.(*gettyTCPConn)\\n  for {\\n   // omit some code\\n\\n   bufLen = 0\\n   for {\\n    // for clause for the network timeout condition check\\n    // s.conn.SetReadTimeout(time.Now().Add(s.rTimeout))\\n            // Receive a message from the TCP connection\\n    bufLen, err = conn.recv(buf)\\n    // Omit some of the code\\n\\n    break\\n   }\\n   // Omit part of the code\\n\\n        // Write the binary bits of the received message to pkgBuf\\n   pktBuf.Write(buf[:bufLen])\\n   for {\\n    if pktBuf.Len() <= 0 {\\n     Write(buf[:bufLen]) for { if pktBuf.\\n    }\\n            // Decode the received message into an RPC message via s.reader\\n    pkg, pkgLen, err = s.reader.Read(s, pktBuf.Bytes())\\n    // Omit some of the code\\n\\n      s.UpdateActive()\\n            // Put the received message into a TaskQueue for consumption by the RPC consumer.\\n    s.addTask(pkg)\\n    pktBuf.Next(pkgLen)\\n    // continue to handle case 5\\n   If exit { pktBuf.Next(pkgLen) // continue to handle case 5\\n   if exit {\\n    pktBuf.Next(pkgLen) // continue to handle case 5 } if exit {\\n   }\\n  }\\n\\n  return perrors.WithStack(err)\\n }\\n ```\\n\\n\\nFrom the above code logic, we analyse that the RPC consumer needs to decode the binary bits received from the TCP connection into messages that can be consumed by RPC, and this work is implemented by s.reader, so we need to implement the Reader interface corresponding to s.reader in order to build the RPC communication layer.\\n\\n\\n### 3. How to decouple the underlying network message processing logic from the business logic\\n\\n\\nWe all know that netty decouples the underlying network logic from the business logic through the boss thread and the worker thread. So how does getty do it?\\n\\n\\nAt the end of the `handlePackage()` method, we see that the incoming message is put into the `s.addTask(pkg)` method, so let\'s move on:\\n\\n\\n```go\\nfunc (s *session) addTask(pkg interface{}) {\\nf := func() {\\ns.listener.OnMessage(s, pkg)\\ns.incReadPkgNum()\\n}\\nif taskPool := s.EndPoint().GetTaskPool(); taskPool ! = nil {\\ntaskPool.AddTaskAlways(f)\\nreturn\\n}\\nf()\\n}\\n ```\\n\\n\\n The `pkg` argument is passed to an anonymous method that ends up in `taskPool`. This method is critical, and I ran into a pitfall later on when I wrote the seata-golang code, which is analysed later.\\n\\n\\n Next we look at the definition of [taskPool](https://github.com/dubbogo/gost/blob/master/sync/task_pool.go):\\n\\n\\n ```go\\n // NewTaskPoolSimple builds a simple task pool.\\n func NewTaskPoolSimple(size int) GenericTaskPool {\\n  if size < 1 {\\n   size = runtime.NumCPU() * 100\\n  NumCPU() * 100 }\\n  return &taskPoolSimple{\\n   work: make(chan task), sem: make(chan struct{task\\n   sem:  make(chan struct{}, size),\\n   done: make(chan struct{}),\\n  }\\n }\\n ```\\n\\n\\nBuilds a channel `sem` with a buffer size of size (defaults to `runtime.NumCPU() * 100`). Then look at the method ``AddTaskAlways(t task)``:\\n\\n\\n```go\\nfunc (p *taskPoolSimple) AddTaskAlways(t task) {\\nselect {\\ncase <-p.done.\\nreturn\\ndefault.\\n}\\n\\nselect {\\ncase p.work <- t.\\nreturn\\ndefault: }\\n}\\nselect {\\ncase p.work <- t: return default: }\\ncase p.sem <- struct{}{}.\\np.wg.Add(1)\\ngo p.worker(t)\\ndefault.\\ngoSafely(t)\\n}\\n}\\n ```\\n\\n\\n When a task is added, it is consumed by len(p.sem) goroutines, and if no goroutine is free, a temporary goroutine is started to run t(). This is equivalent to having len(p.sem) goroutines to form a goroutine pool, and the goroutines in the pool process business logic instead of the goroutines that process network messages to run business logic, thus achieving decoupling. One of the pitfalls I encountered when writing seata-golang was that I forgot to set the taskPool, which resulted in the same goroutine handling the business logic and the underlying network message logic. When I blocked the business logic and waited for a task to complete, I blocked the entire goroutine, and I couldn\'t receive any messages during the blocking period.\\n\\n\\n ### 4. Implementation\\n\\n\\n The following code is available at [getty.go](https://github.com/apache/dubbo-getty/blob/master/getty.go):\\n\\n\\n ```go\\n // Reader is used to unmarshal a complete pkg from buffer\\n type Reader interface {\\n  Read(Session, []byte) (interface{}, int, error)\\n }\\n\\n // Writer is used to marshal a pkg and write to session.\\n type Writer interface {\\n  // If @Session is udpGettySession, the second parameter is UDPContext.\\n  Write(Session, interface{}) ([]byte, error)\\n Write(Session, interface{}) ([]byte, error) }\\n\\n // ReadWriter interface use for handle application packages.\\n type ReadWriter interface {\\n  Writer\\n  Writer\\n }\\n ```\\n\\n\\n ```go\\n // EventListener is used to process pkg that received from remote session\\n type EventListener interface {\\n  // invoked when session opened\\n  // If the return error is not nil, @Session will be closed.\\n  OnOpen(Session) error\\n\\n  OnOpen(Session) error // invoked when session closed.\\n  EventListener { // invoked when session opened // If the return error is not nil, @Session will be closed.)\\n\\n  OnOpen(Session) error // invoked when session closed.\\n  OnError(Session, error)\\n\\n  // invoked periodically, its period can be set by (Session)SetCronPeriod\\n  OnCron(Session)\\n\\n  // invoked when getty received a package. Pls attention that do not handle long time\\n  // logic processing in this func. You\'d better set the package\'s maximum length.\\n  // If the message\'s length is greater than it, u should should return err in\\n  If the message\'s length is greater than it, u should should return err in // Reader{Read} and getty will close this connection soon.\\n  // If ur logic processing in this func\\n  // If ur logic processing in this func will take a long time, u should start a goroutine\\n  // If ur logic processing in this func will take a long time, u should start a goroutine pool (like working thread pool in cpp) to handle the processing asynchronously.\\n  // can do the logic processing in other asynchronous way.\\n  Or u // can do the logic processing in other asynchronous way. !In short, ur OnMessage callback func should return asap.\\n  // In short, ur OnMessage callback func should return asap.\\n  // If this is a udp event listener, the second parameter type is UDPContext.\\n  OnMessage(Session, interface{})\\n }\\n ```\\n\\n\\nBy analysing the entire getty code, we only need to implement `ReadWriter` to encode and decode RPC messages, and then implement `EventListener` to handle the corresponding specific logic of RPC messages, and then inject the `ReadWriter` implementation and the `EventLister` implementation into the Client and Server sides of RPC, then we can implement RPC communication. Inject the `ReadWriter` implementation and `EventLister` implementation into the Client and Server side of RPC to achieve RPC communication.\\n\\n\\n#### 4.1 Codec Protocol Implementation\\n\\n\\nThe following is the definition of the seata protocol:\\n! [image-20201205214556457.png](https://cdn.nlark.com/yuque/0/2020/png/737378/1607180799872-5f96afb6-680d-4e69-8c95-b8fd1ac4c3a7.png #align=left&display=inline&height=209&margin=%5Bobject%20Object%5D&name=image-20201205214556457.png& originHeight=209&originWidth=690&size=18407&status=done&style=none&width=690)\\n\\n\\nIn the ReadWriter interface implementation [`RpcPackageHandler`](https://github.com/opentrx/seata-golang), call the Codec method to codec the message body in the above format:\\n\\n\\n```go\\n// Encode the message into binary bits\\nfunc MessageEncoder(codecType byte, in interface{}) []byte {\\nswitch codecType {\\ncase SEATA.\\nreturn SeataEncoder(in)\\ndefault.\\nlog.Errorf(\\"not support codecType, %s\\", codecType)\\nreturn nil\\n}\\n}\\n\\n// Decode the binary bits into the message body\\nfunc MessageDecoder(codecType byte, in []byte) (interface{}, int) {\\nswitch codecType {\\ncase SEATA.\\nreturn SeataDecoder(in)\\ndefault.\\nlog.Errorf(\\"not support codecType, %s\\", codecType)\\nreturn nil, 0\\n}\\n}\\n ```\\n\\n\\n #### 4.2 Client Side Implementation\\n\\n\\n Let\'s look at the client-side implementation of `EventListener` [`RpcRemotingClient`](https://github.com/opentrx/seata-golang/blob/dev/pkg/client/rpc_remoting_client. go):\\n\\n\\n ```go\\n func (client *RpcRemoteClient) OnOpen(session getty.Session) error {\\n  go func()\\n   request := protocal.RegisterTMRequest{AbstractIdentifyRequest: protocal.\\n    ApplicationId: client.conf.\\n    TransactionServiceGroup: client.conf.\\n   }}\\n    // Once the connection is established, make a request to the Transaction Coordinator to register the TransactionManager.\\n   _, err := client.sendAsyncRequestWithResponse(session, request, RPC_REQUEST_TIMEOUT)\\n   if err == nil {\\n      // Save the connection to the Transaction Coordinator in the connection pool for future use.\\n    clientSessionManager.RegisterGettySession(session)\\n    client.GettySessionOnOpenChannel <- session.RemoteAddr()\\n   }\\n  }()\\n\\n  return nil\\n }\\n\\n // OnError ...\\n func (client *RpcRemoteClient) OnError(session getty.Session, err error) {\\n  clientSessionManager.ReleaseGettySession(session)\\n }\\n\\n // OnClose ...\\n func (client *RpcRemoteClient) OnClose(session getty.Session) {\\n  clientSessionManager.ReleaseGettySession(session)\\n }\\n\\n // OnMessage ...\\n func (client *RpcRemoteClient) OnMessage(session getty.Session, pkg interface{}) {\\n  log.Info(\\"received message:{%v}\\", pkg)\\n  rpcMessage, ok := pkg.(clientRpcRemoteClient.Session, pkg interface{}) { log.Info(\\"received message:{%v}\\", pkg)\\n  if ok {\\n   heartBeat, isHeartBeat := rpcMessage.Body.(protocal.HeartBeatMessage)\\n   if isHeartBeat && heartBeat == protocal.HeartBeatMessagePong {\\n    log.Debugf(\\"received PONG from %s\\", session.RemoteAddr())\\n   }\\n  }\\n\\n  if rpcMessage.MessageType == protocal.MSGTYPE_RESQUEST ||\\n   rpcMessage.MessageType == protocal.MSGTYPE_RESQUEST_ONEWAY {\\n   log.Debugf(\\"msgId:%s, body:%v\\", rpcMessage.Id, rpcMessage.Body)\\n\\n   // Process the transaction message, commit or rollback\\n   client.onMessage(rpcMessage, session.RemoteAddr())\\n  } else {\\n   resp, loaded := client.futures.Load(rpcMessage.Id)\\n   if loaded {\\n    response := resp.(*getty2.MessageFuture)\\n    response.Response = rpcMessage.Body\\n    response.Done <- true\\n    client.futures.Delete(rpcMessage.Id)\\n   }\\n  }\\n }\\n\\n // OnCron ...\\n func (client *RpcRemoteClient) OnCron(session getty.Session) {\\n  // Send a heartbeat\\n  client.defaultSendRequest(session, protocal.HeartBeatMessagePing)\\n }\\n ```\\n\\n\\n The logic of `clientSessionManager.RegisterGettySession(session)` is analysed in subsection 4.4.\\n\\n\\n #### 4.3 Server-side Transaction Coordinator Implementation\\n\\n\\n See [``DefaultCoordinator``](https://github.com/opentrx/seata-golang/blob/dev/tc/server/default_coordinator_event_listener.go) for code:\\n\\n\\n ```go\\n func (coordinator *DefaultCoordinator) OnOpen(session getty.Session) error {\\n  log.Infof(\\"got getty_session:%s\\", session.Stat())\\n  error { log.Infof(\\"got getty_session:%s\\", session.Stat())\\n }\\n\\n func (coordinator *DefaultCoordinator) OnError(session getty.Session, err error) {\\n  // Release the TCP connection\\n  SessionManager.ReleaseGettySession(session)\\n  session.Close()\\n  log.Errorf(\\"getty_session{%s} got error{%v}, will be closed.\\", session.Stat(), err)\\n }\\n\\n func (coordinator *DefaultCoordinator) OnClose(session getty.Session) {\\n  log.Info(\\"getty_session{%s} is closing......\\" , session.Stat())\\n }\\n\\n func (coordinator *DefaultCoordinator) OnMessage(session getty.Session, pkg interface{}) {\\n  log.Debugf(\\"received message:{%v}\\", pkg)\\n  rpcMessage, ok := pkg.(protocal.)\\n  RpcMessage) if ok {\\n   _, isRegTM := rpcMessage.Body.(protocal.RegisterTMRequest)\\n   if isRegTM {\\n      // Map the TransactionManager information to the TCP connection.\\n    coordinator.OnRegTmMessage(rpcMessage, session)\\n    OnRegTmMessage(rpcMessage, session)\\n   }\\n\\n   heartBeat, isHeartBeat := rpcMessage.Body.(protocal.HeartBeatMessage)\\n   if isHeartBeat && heartBeat == protocal.HeartBeatMessagePing {\\n    coordinator.OnCheckMessage(rpcMessage, session)\\n    OnCheckMessage(rpcMessage, session)\\n   }\\n\\n   if rpcMessage.MessageType == protocal.MSGTYPE_RESQUEST ||\\n    rpcMessage.MessageType == protocal.MSGTYPE_RESQUEST_ONEWAY {\\n    log.Debugf(\\"msgId:%s, body:%v\\", rpcMessage.Id, rpcMessage.Body)\\n    _, isRegRM := rpcMessage.Body.(protocal.RegisterRMRequest)\\n    if isRegRM {\\n        // Map the ResourceManager information to the TCP connection.\\n     coordinator.OnRegRmMessage(rpcMessage, session)\\n    } else {\\n     if SessionManager.IsRegistered(session) {\\n      if err := recover(); } else { if SessionManager.IsRegistered(session) {\\n       if err := recover(); err ! = nil { log.Errorf(); err !\\n        log.Errorf(\\"Catch Exception while do RPC, request: %v,err: %w\\", rpcMessage, err)\\n       }\\n      }()\\n          // Handle transaction messages, global transaction registration, branch transaction registration, branch transaction commit, global transaction rollback, etc.\\n      coordinator.OnTrxMessage(rpcMessage, session)\\n     } else {\\n      session.Close()\\n      log.Infof(\\"Close an unhandled connection! [%v]\\", session)\\n     }\\n    }\\n   } else {\\n    resp, loaded := coordinator.futures.Load(rpcMessage.Id)\\n    if loaded {\\n     response := resp.(*getty2.MessageFuture)\\n     response.Response = rpcMessage.Body\\n     response.Done <- true\\n     coordinator.futures.Delete(rpcMessage.Id)\\n    }\\n   }\\n  }\\n }\\n\\n func (coordinator *DefaultCoordinator) OnCron(session getty.Session) {\\n\\n }\\n ```\\n\\n\\n`coordinator.OnRegTmMessage(rpcMessage, session)` registers the Transaction Manager, `coordinator.OnRegRmMessage(rpcMessage, session)` registers the Resource The logic is analysed in Section 4.4.\\nThe message enters the `coordinator.OnTrxMessage(rpcMessage, session)` method and is routed to the specific logic according to the message type code:\\n\\n\\n```go\\nswitch msg.GetTypeCode() {\\ncase protocal.TypeGlobalBegin:\\nreq := msg.(protocal.GlobalBeginRequest)\\nresp := coordinator.doGlobalBegin(req, ctx)\\nreturn resp\\ncase protocal.TypeGlobalStatus.\\nTypeGlobalStatus. req := msg.(protocal.GlobalStatusRequest)\\nresp := coordinator.doGlobalStatus(req, ctx)\\nreturn resp\\ncase protocal.TypeGlobalReport.\\nreq := msg.(protocal.GlobalReportRequest)\\nresp := coordinator.doGlobalReport(req, ctx)\\nreturn resp\\ncase protocal.TypeGlobalCommit.\\nreq := msg.(protocal.GlobalCommitRequest)\\nresp := coordinator.doGlobalCommit(req, ctx)\\nreturn resp\\ncase protocal.TypeGlobalRollback.\\nreq := msg.(protocal.GlobalRollbackRequest)\\nresp := coordinator.doGlobalRollback(req, ctx)\\nreturn resp\\ncase protocal.TypeBranchRegister.\\nTypeBranchRegister. req := msg.(protocal.BranchRegisterRequest)\\nresp := coordinator.doBranchRegister(req, ctx)\\nreturn resp\\ncase protocal.TypeBranchStatusReport.\\nTypeBranchStatusReport: req := msg.(protocal.BranchReportRequest)\\nresp := coordinator.doBranchReport(req, ctx)\\nreturn resp\\ndefault.\\nreturn nil\\n}\\n ```\\n\\n\\n #### 4.4 Session Manager Analysis\\n After the Client establishes a connection with the Transaction Coordinator, it saves the connection in the map `serverSessions = sync.Map{}` by using `clientSessionManager.RegisterGettySession(session)`. The key of the map is the RemoteAddress of the Transaction Coordinator obtained from the session, and the value is the session. This allows the Client to register the Transaction Manager and Resource Manager with the Transaction Coordinator through a session in the map. See [`getty_client_session_manager.go`]. (https://github.com/opentrx/seata-golang/blob/dev/pkg/client/getty_client_session_manager.go)\\n After the Transaction Manager and Resource Manager are registered with the Transaction Coordinator, a connection can be used to send either TM messages or RM messages. We identify a connection with an RpcContext:\\n ```go\\n type RpcContext struct {\\n  Version string\\n  TransactionServiceGroup string\\n  ClientRole meta.TransactionRole\\n  ApplicationId string\\n  ClientId string\\n  ResourceSets *model.\\n  Session getty.\\n Session }\\n ```\\nWhen a transaction message is received, we need to construct such an RpcContext to be used by the subsequent transaction logic. So, we will construct the following map to cache the mapping relationships:\\n ```go\\n var (\\n  // session -> transactionRole\\n  // TM will register before RM, if a session is not the TM registered, // it will be the RM registered.\\n  // it will be the RM registered\\n  session_transactionroles = sync.Map{}\\n\\n  // session -> applicationId\\n  identified_sessions = sync.Map{}\\n\\n  // applicationId -> ip -> port -> session\\n  client_sessions = sync.Map{}\\n\\n  // applicationId -> resourceIds\\n  client_resources = sync.Map{}\\n )\\n ```\\nIn this way, the Transaction Manager and Resource Manager are registered to the Transaction Coordinator via `coordinator.OnRegTmMessage(rpcMessage, session)` and `coordinator.OnRegRmMessage(rpcMessage, session)` respectively. session)` are registered with the Transaction Coordinator, the relationship between applicationId, ip, port and session is cached in the above client_sessions map, and the relationship between applicationId, ip, port and resourceIds (an application may be able to register with the Transaction Coordinator) is cached in the client_resources map. and resourceIds (there may be multiple Resource Managers for an application) in the client_resources map. When needed, we can construct an RpcContext from these mappings, which is very different from the java version of seata, so if you\'re interested, you can dig a little deeper. See [`getty_session_manager.go`]. (https://github.com/opentrx/seata-golang/blob/dev/tc/server/getty_session_manager.go)\\nAt this point, we have analysed [seata-golang](https://github.com/opentrx/seata-golang) the entire mechanism of the RPC communication model.\\n\\n## III. The Future of seata-golang\\n\\nThe development of [seata-golang](https://github.com/opentrx/seata-golang) started in April this year, and in August it basically realised the interoperability with the java version of [seata 1.2](https://github.com/apache/incubator-) protocol. seata) protocol, implemented AT mode for mysql database (automatically coordinating the commit rollback of distributed transactions), implemented TCC mode, and used mysql to store data on the TC side, which turned TC into a stateless application to support high-availability deployment. The following figure shows the principle of AT mode: ! [image20201205-232516.png]( https://img.alicdn.com/imgextra/i3/O1CN01alqsQS1G2oQecFYIs_! !6000000000565-2-tps-1025-573.png)\\n\\n\\nThere is still a lot of work to be done, such as support for the registry, support for the configuration centre, protocol interoperability with the java version of seata 1.4, support for other databases, implementation of the craft transaction coordinator, etc. We hope that developers interested in the distributed transaction problem can join in to build a perfect golang\'s distributed transaction framework.\\n\\nIf you have any questions, please feel free to join the group [group number 33069364]:\\n<img src=\\"https://img.alicdn.com/imgextra/i2/O1CN01IjOVG425erjuzqcOi_! !6000000007552-2-tps-600-621.png\\" width=\\"200px\\" />\\n\\n### **Author Bio**\\n\\nXiaomin Liu (GitHubID dk-lockdown), currently working at h3c Chengdu, is good at using Java/Go language, and has dabbled in cloud-native and microservices related technologies, currently specialising in distributed transactions.\\nYu Yu (github @AlexStocks), dubbo-go project and community leader, a programmer with more than 10 years of frontline experience in server-side infrastructure R&D, has participated in the improvement of Muduo/Pika/Dubbo/Sentinel-go and other well-known projects, and is currently engaged in container orchestration and service mesh work in the Trusted Native Department of ants. Currently, he is working on container orchestration and service mesh in the Trusted Native Department of AntGold.\\n\\n#### References\\n\\n\\nseata official: [https://seata.apache.org](https://seata.apache.org)\\n\\n\\njava version seata\uff1a[https://github.com/apache/incubator-seata](https://github.com/apache/incubator-seata)\\n\\n\\nseata-golang project address: [https://github.com/apache/incubator-seata-go](https://github.com/apache/incubator-seata-go)\\n\\n\\nseata-golang go night reading b\u7ad9\u5206\u4eab\uff1a[https://www.bilibili.com/video/BV1oz411e72T](https://www.bilibili.com/video/BV1oz411e72T)"},{"id":"/seata-datasource-proxy","metadata":{"permalink":"/blog/seata-datasource-proxy","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-datasource-proxy.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-datasource-proxy.md","title":"Analysis of Seata Data Source Proxy","description":"This article primarily introduces the implementation principles of Seata data source proxy and potential issues that may arise during usage.","date":"2020-10-16T00:00:00.000Z","formattedDate":"October 16, 2020","tags":[],"readingTime":29.455,"hasTruncateMarker":false,"authors":[{"name":"xiaoyong.luo"}],"frontMatter":{"title":"Analysis of Seata Data Source Proxy","keywords":["Seata","data source","data source proxy","multiple data sources"],"description":"This article primarily introduces the implementation principles of Seata data source proxy and potential issues that may arise during usage.","author":"xiaoyong.luo","date":"2020/10/16"},"unlisted":false,"prevItem":{"title":"Detailed Explanation of seata-golang Communication Model","permalink":"/blog/seata-golang-communication-mode"},"nextItem":{"title":"Seata Source Code - Client Startup Process in Distributed Transactions","permalink":"/blog/seata-sourcecode-client-bootstrap"}},"content":"In Seata version 1.3.0, data source auto-proxy and manual proxy must not be mixed, otherwise it will lead to multi-layer proxy, which will lead to the following problems:\\n1. single data source case: cause branch transaction commit, undo_log itself is also proxied, i.e. `generated undo_log for undo_log, assumed to be undo_log2`, at this time, undo_log will be treated as a branch transaction; branch transaction rollback, because of the `undo_log2` generated by the faulty in `undo_log corresponding transaction branch` rollback. When the branch transaction is rolled back, because there is a problem with the generation of `undo_log2`, when the transaction branch corresponding to the `undo_log` is rolled back, it will delete the `undo_log` associated with the `business table`, which will lead to the discovery that the `business table corresponding to the `business table` is rolled back and the `undo_log` doesn\'t exist, and thus generate an additional status of 1 for the `undo_log.\' This time, the overall logic is already messed up, which is a very serious problem!\\n2. multiple data sources and `logical data sources are proxied` case: in addition to the problems that will occur in the case of a single data source, may also cause deadlock problems. The reason for the deadlock is that for the undo_log operation, the `select for update` and `delete` operations that should have been performed in one transaction are spread out over multiple transactions, resulting in one transaction not committing after executing the `select for update`, and one transaction waiting for a lock when executing the `delete` until the timeout expires, and then the lock will not lock until the timeout expires. until it times out.\\n\\n\\n## Proxy description\\nThis is a layer of DataSource proxying that overrides some methods. For example, the `getConnection` method does not return a `Connection`, but a `ConnectionProxy`, and so on.\\n```java\\n// DataSourceProxy\\n\\npublic DataSourceProxy(DataSource targetDataSource) {\\nthis(targetDataSource, DEFAULT_RESOURCE_GROUP_ID);\\n}\\n\\nprivate void init(DataSource dataSource, String resourceGroupId) {\\nDefaultResourceManager.get().registerResource(this); }\\n}\\n\\npublic Connection getPlainConnection() throws SQLException {\\nreturn targetDataSource.getConnection(); } public Connection getPlainConnection(); return targetDataSource.\\n}\\n\\n@Override\\npublic ConnectionProxy getConnection() throws SQLException {\\nConnection targetConnection = targetDataSource.getConnection(); } @Override public ConnectionProxy getConnection(); }\\nreturn new ConnectionProxy(this, targetConnection);\\n}\\n ```\\n\\n ## Manual Proxy\\n That is, manually inject a `DataSourceProxy` as follows\\n ```\\n@Bean\\npublic DataSource druidDataSource() {\\nreturn new DruidDataSource()\\n}\\n\\n@Primary\\n@Bean(\\"dataSource\\")\\npublic DataSourceProxy dataSource(DataSource druidDataSource) {\\nreturn new DataSourceProxy(druidDataSource); }\\n}\\n```\\n\\n## AutoProxy\\nCreate a proxy class for `DataSource`, get `DataSourceProxy (create it if it doesn\'t exist)` based on `DataSource` inside the proxy class, and then call the relevant methods of `DataSourceProxy`. The core logic is in `SeataAutoDataSourceProxyCreator`.\\n```java\\npublic class SeataAutoDataSourceProxyCreator extends AbstractAutoProxyCreator {\\nprivate static final Logger LOGGER = LoggerFactory.getLogger(SeataAutoDataSourceProxyCreator.class);\\nprivate final String[] excludes; private final Advisor advisor = new SeataAutoDataSourceProxyCreator.class\\nprivate final Advisor advisor = new DefaultIntroductionAdvisor(new SeataAutoDataSourceProxyAdvice());\\n\\n    public SeataAutoDataSourceProxyCreator(boolean useJdkProxy, String[] excludes) {\\n        this.excludes = excludes;\\n        setProxyTargetClass(!useJdkProxy);\\n    }\\n\\n    @Override\\n    protected Object[] getAdvicesAndAdvisorsForBean(Class<? > beanClass, String beanName, TargetSource customTargetSource) throws BeansException {\\n        if (LOGGER.isInfoEnabled()) {\\n            LOGGER.info(\\"Auto proxy of [{}]\\", beanName);\\n        }\\n        return new Object[]{advisor};\\n    }\\n\\n    @Override\\n    protected boolean shouldSkip(Class<? > beanClass, String beanName) {\\n        return SeataProxy.class.isAssignableFrom(beanClass) ||\\n                DataSourceProxy.class.isAssignableFrom(beanClass) ||\\n                !DataSource.class.isAssignableFrom(beanClass) ||\\n                Arrays.asList(excludes).contains(beanClass.getName());\\n    }\\n}\\n\\npublic class SeataAutoDataSourceProxyAdvice implements MethodInterceptor, IntroductionInfo {\\n}\\npublic Object invoke(MethodInvocation invocation) throws Throwable {\\nDataSourceProxy dataSourceProxy = DataSourceProxyHolder.get().putDataSource((DataSource) invocation.getThis());\\nMethod method = invocation.getMethod();\\nObject[] args = invocation.getArguments();\\nMethod m = BeanUtils.findDeclaredMethod(DataSourceProxy.class, method.getName(), method.getParameterTypes());\\nif (m ! = null) {\\nreturn m.invoke(dataSourceProxy, args); } else { m.invoke(DataSourceProxy.class, method.getName(), method.getParameterTypes())\\n} else {\\nreturn invocation.proceed();\\n}\\n}\\n\\n    @Override\\n    public Class<? >[] getInterfaces() {\\n        return new Class[]{SeataProxy.class};\\n    }\\n}\\n ```\\n\\n\\n ## Data Source Multi-Level Proxy\\n ```\\n@Bean.\\n@DependsOn(\\"strangeAdapter\\")\\npublic DataSource druidDataSource(StrangeAdapter strangeAdapter) {\\ndruidDataSource(StrangeAdapter strangeAdapter) { doxx\\nreturn new DruidDataSource()\\n}\\n\\n@Primary\\n@Bean(\\"dataSource\\")\\npublic DataSourceProxy dataSource(DataSource druidDataSource) {\\nreturn new DataSourceProxy(druidDataSource); }\\n}\\n ```\\n 1. First we inject two `DataSources` into the configuration class: `DruidDataSource` and `DataSourceProxy`, where `DruidDataSource` is used as the targetDataSource attribute of `DataSourceProxy` and `DataSourceProxy` is used as the targetDataSource attribute of `DruidDataSource`. DataSourceProxy` is declared using the `@Primary` annotation.\\n 2. The application has automatic data source proxying enabled by default, so when calling methods related to `DruidDataSource`, a corresponding data source proxy `DataSourceProxy2` will be created for `DruidDataSource`.\\n 3. What happens when we want to get a Connection in our application?\\n   1. first get a `DataSource`, because the `DataSourceProxy` is `Primary`, so we get a `DataSourceProxy`. 2. based on the `DataSource`, we create a corresponding `DataSourceProxy2`.\\n   2. get a `Connection` based on the `DataSource`, i.e. get the `Connection` through the `DataSourceProxy`. At this time, we will first call the getConnection method of `targetDataSource`, i.e. DruidDataSource`, but since the cutover will intercept `DruidDataSource`, according to the interception logic in step 2, we can know that a `DataSourceProxy2` will be created automatically, and then call the `DataSourceProxy2`. Then call `DataSourceProxy2#getConnection`, and then call `DruidDataSource#getConnection`. This results in a two-tier proxy, and the returned `Connection` is also a two-tier `ConnectionProxy`.\\n\\n ! [](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9ac0b91bd8fc4c48aa68afd5c58a42d5~tplv-k3u1fbpfcp-watermark.image)\\n\\n The above is actually the modified proxy logic, Seata\'s default autoproxy will proxy the `DataSourceProxy` again, the consequence is that there is one more layer of proxy at this time the corresponding diagram is as follows\\n\\n ! [](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/837aa0462d994e9a8614707c6a50b5ae~tplv-k3u1fbpfcp-watermark.image)\\n\\n The two problems that can result from multiple layers of proxies for a data source are summarised at the beginning of the article, with case studies below.\\n\\n\\n\\n # Branching Transaction Commits\\n What happens when the corresponding method is executed through the `ConnectionProxy`? Let\'s take an example of a branching transaction commit involving an update operation:\\n 1. Execute `ConnectionProxy#prepareStatement`, which returns a `PreparedStatementProxy`.\\n 2. Execute `PreparedStatementProxy#executeUpdate`, `PreparedStatementProxy#executeUpdate` will probably do two things: execute the business SQL and commit the undo_log.\\n\\n ## Commit business SQL\\n ```java\\n // ExecuteTemplate#execute\\n if (sqlRecognizers.size() == 1) {\\n    SQLRecognizer sqlRecognizer = sqlRecognizers.get(0);\\n    switch (sqlRecognizer.getSQLType()) {\\n        case INSERT.\\n            executor = EnhancedServiceLoader.load(InsertExecutor.class, dbType, new Class[]{StatementLoader.load(InsertExecutor.class, dbType)) { case INSERT.\\n                    new Class[]{StatementProxy.class, StatementCallback.class, SQLRecognizer.class}, new\\n                    new Object[]{statementProxy, statementCallback, sqlRecognizer});\\n            statementProxy, statementCallback, sqlRecognizer}); break;\\n        case UPDATE: executor = new UpdateExecutor\\n            executor = new UpdateExecutor<>(statementProxy, statementCallback, sqlRecognizer); break; case UPDATE.\\n            break;\\n        case DELETE.\\n            executor = new DeleteExecutor<>(statementProxy, statementCallback, sqlRecognizer); break; case DELETE.\\n            break; case SELECT_FOR_UPDATE.\\n        case SELECT_FOR_UPDATE: executor = new SelectForUpdate.\\n            executor = new SelectForUpdateExecutor<>(statementProxy, statementCallback, sqlRecognizer); break; case SELECT_FOR_UPDATE.\\n            break; break\\n        default: executor = new PlainExecutor\\n            executor = new PlainExecutor<>(statementProxy, statementCallback); break; default.\\n            break;\\n    }\\n } else {\\n    executor = new MultiExecutor<>(statementProxy, statementCallback, sqlRecognizers); } else { executor = new MultiExecutor<>(statementProxy, statementCallback, sqlRecognizers); }\\n }\\n ```\\n\\nThe main process is: first execute the business SQL, then execute the commit method of the ConnectionProxy, in which the corresponding undo_log SQL will be executed for us, and then commit the transaction.\\n ```\\n PreparedStatementProxy#executeUpdate =>\\n ExecuteTemplate#execute =>\\n BaseTransactionalExecutor#execute =>\\n AbstractDMLBaseExecutor#doExecute =>\\n AbstractDMLBaseExecutor#executeAutoCommitTrue =>\\n AbstractDMLBaseExecutor#executeAutoCommitFalse => In this step, the statementCallback#execute method will be triggered, i.e. the native PreparedStatement#executeUpdate method will be called.\\n ConnectionProxy#commit\\n ConnectionProxy#processGlobalTransactionCommit\\n ```\\n\\n## UNDO_LOG insert\\n ```java\\n // ConnectionProxy#processGlobalTransactionCommit\\n private void processGlobalTransactionCommit() throws SQLException {\\n    try {\\n        // Register for a branch transaction, simply understand that a request is sent to the server, and then the server inserts a record into the branch_table table.\\n        register();\\n    } catch (TransactionException e) {\\n        // If there is no for update sql, it will register directly before commit, then not only insert a branch record, but also lock information for the competition, the following exception is generally thrown in the registration did not get the lock, generally is pure update statement concurrency will trigger the competition lock failure exception @FUNKYE\\n        recognizeLockKeyConflictException(e, context.buildLockKeys());\\n    }\\n    try {\\n        // undo_log handling, expect targetConnection handling @1\\n        UndoLogManagerFactory.getUndoLogManager(this.getDbType()).flushUndoLogs(this); // Commit local transaction, expect targetConnection.\\n\\n        // Commit the local transaction, expecting it to be handled by targetConnection @2\\n        targetConnection.commit(); } catch (Throwable ex)\\n    } catch (Throwable ex) {\\n        LOGGER.error(\\"process connectionProxy commit error: {}\\", ex.getMessage(), ex); report(false); }\\n        report(false); } catch (Throwable ex); }\\n        throw new SQLException(ex);\\n    }\\n    if (IS_REPORT_SUCCESS_ENABLE) {\\n        report(true); }\\n    }\\n    context.reset();\\n }\\n ```\\n1. undo_log processing @1, parses the `undo_log` involved in the current transaction branch and writes it to the database using `TargetConnection`.\\n``` java\\n   public void flushUndoLogs(ConnectionProxy cp) throws SQLException {\\n   ConnectionContext connectionContext = cp.getContext();\\n   if (!connectionContext.hasUndoLog()) {\\n   return;\\n   }\\n\\n   String xid = connectionContext.getXid(); long branchId = connectionContext.hasUndoLog(); { return; }\\n   long branchId = connectionContext.getBranchId(); }\\n\\n   BranchUndoLog branchUndoLog = new BranchUndoLog(); branchUndoLog.setBranchId = connectionContext.getBranchId(); }\\n   branchUndoLog.setXid(xid); branchUndoLog.\\n   branchUndoLog.setBranchId(branchId); branchUndoLog.\\n   branchUndoLog.setSqlUndoLogs(connectionContext.getUndoItems());\\n\\n   UndoLogParser parser = UndoLogParserFactory.getInstance();\\n   byte[] undoLogContent = parser.encode(branchUndoLog);\\n\\n   if (LOGGER.isDebugEnabled()) {\\n   LOGGER.debug(\\"Flushing UNDO LOG: {}\\", new String(undoLogContent, Constants.DEFAULT_CHARSET));\\n   }\\n\\n   insertUndoLogWithNormal(xid, branchId, buildContext(parser.getName()), undoLogContent,cp.getTargetConnection());\\n   }\\n ```\\n 2. Commit local transaction @2, i.e., commit the transaction via `TargetConnection`. That is, the same `TargetConnection` is used for `service SQL execution`, `undo_log write`, and `i.e. transaction commit`.\\n\\n >lcn\'s built-in database solution, lcn is to write undolog to his embedded h2 (I forget if it\'s this one) database, at this time it will become two local transactions, one is h2\'s undolog insertion transaction, one is the transaction of the business database, if the business database is abnormal after the insertion of the h2, lcn\'s solution will be data redundancy, roll back the data. data is the same, delete undolog and rollback business data is not a local transaction.\\n But the advantage of lcn is the invasion of small, do not need to add another undolog table. Thanks to @FUNKYE for the advice, I don\'t know much about lcn, I\'ll look into it when I get a chance!\\n\\n\\n # Branch Transaction Rollback\\n 1. Server sends a rollback request to Client. 2.\\n 2. Client receives the request from Server, and after a series of processing, it ends up in the `DataSourceManager#branchRollback` method. 3.\\n 3. first according to the resourceId from the `DataSourceManager.dataSourceCache` to get the corresponding `DataSourceProxy`, at this time for the `masterSlaveProxy` (rollback stage we do not test the proxy data source, simple and direct, anyway, the final get all the `TragetConnection`)\\n 4. According to the xid and branchId sent from the Server side to find the corresponding undo_log and parse its `rollback_info` attribute, each undo_log may be parsed out of more than one `SQLUndoLog`, each `SQLUndoLog` can be interpreted as an operation. For example, if a branch transaction updates table A and then table B, the undo_log generated for the branch transaction contains two `SQLUndoLog`s: the first `SQLUndoLog` corresponds to the snapshot before and after the update of table A; the second `SQLUndoLog` corresponds to the snapshot before and after the update of table B.\\n 5. for each `SQLUndoLog` execute the corresponding rollback operation, for example, a `SQLUndoLog` corresponds to the operation `INSERT`, then its corresponding rollback operation is `DELETE`.\\n 6. Delete the undo_log based on the xid and branchId.\\n\\n\\n ```java\\n // AbstractUndoLogManager#undo removes some non-critical code\\n\\n public void undo(DataSourceProxy dataSourceProxy, String xid, long branchId) throws TransactionException {\\n    Connection conn = null;\\n    ResultSet rs = null;\\n    PreparedStatement selectPST = null;\\n    boolean originalAutoCommit = true; for (; ; ) {\\n\\n    for (; ; ) {\\n        try {\\n            // Get the connection to the original data source, we don\'t care about the proxy data source in the rollback phase, we\'ll end up with the TargetConnection.\\n            conn = dataSourceProxy.getPlainConnection(); // Get the connection to the native data source.\\n\\n            // Put the rollback operation in a local transaction and commit it manually, making sure that the final business SQL operation is committed along with the undo_log delete operation.\\n            if (originalAutoCommit = conn.getAutoCommit()) {\\n                conn.setAutoCommit(false);\\n            }\\n\\n            // Query undo_log based on xid and branchId, note the SQL statement SELECT * FROM undo_log WHERE branch_id = ? AND xid = ? FOR UPDATE\\n            selectPST = conn.prepareStatement(SELECT_UNDO_LOG_SQL);\\n            selectPST.setLong(1, branchId); selectPST.setString(1, branchId); selectPST.setString(1, branchId)\\n            selectPST.setString(2, xid);\\n            rs = selectPST.executeQuery(); boolean exists = false; rs = selectPST.\\n\\n            boolean exists = false; while (rs.next())\\n            while (rs.next()) {\\n                exists = true; boolean exists = false; while (rs.next()) {\\n                // status == 1 undo_log is not processed, related to anti-suspension\\n                if (!canUndo(state)) {\\n                    return; }\\n                }\\n\\n                // Parsing the undo_log\\n                byte[] rollbackInfo = getRollbackInfo(rs); // Parsing the undo_log.\\n                BranchUndoLog branchUndoLog = UndoLogParserFactory.getInstance(serialiser).decode(rollbackInfo);\\n                try {\\n                    setCurrentSerializer(parser.getName());\\n                    List<SQLUndoLog> sqlUndoLogs = branchUndoLog.getSqlUndoLogs(); if (sqlUndoLog.getSqlUndoLogs(parser.getName()); } }\\n                    if (sqlUndoLogs.size() > 1) {\\n                        Collections.reverse(sqlUndoLogs);\\n                    }\\n                    for (SQLUndoLog sqlUndoLog : sqlUndoLogs) {\\n                        AbstractUndoExecutor undoExecutor = UndoExecutorFactory.getUndoExecutor(dataSourceProxy.getDbType(), sqlUndoLog);\\n                        // Execute the corresponding rollback operation\\n                        undoExecutor.executeOn(conn);\\n                    }\\n                }\\n            }\\n\\n            // If (exists) { undoExecutor.executeOn(conn); }\\n            if (exists) {\\n                LOGGER.error(\\"\\\\n delete from undo_log where xid={} AND branchId={} \\\\n\\", xid, branchId);\\n                deleteUndoLog(xid, branchId, conn);\\n                conn.commit();\\n            // and anti-suspension related If no undo_log is found based on xid and branchId, it means that there is an exception in the branch transaction: for example, the business process timed out, resulting in a global transaction rollback, but the business undo_log was not inserted at that time.\\n            } else {\\n                LOGGER.error(\\"\\\\n insert into undo_log xid={},branchId={} \\\\n\\", xid, branchId);\\n                insertUndoLogWithGlobalFinished(xid, branchId, UndoLogParserFactory.getInstance(), conn);\\n                conn.commit();\\n            }\\n            return; }\\n        } catch (Throwable e) {\\n            throw new BranchTransactionException(BranchRollbackFailed_Retriable, String\\n                .format(\\"Branch session rollback failed and try again later xid = %s branchId = %s %s\\", xid,branchId, e.getMessage()), e); }\\n        }\\n    }\\n }\\n ```\\nThere are several notes:\\n1. rollback does not take into account data source proxying, and ends up using ``TargetConnection``.\\n2. set atuoCommit to false, i.e. you need to commit the transaction manually\\n3. `for update` is added when querying the undo_log based on xid and branchId, i.e. the transaction will hold the lock for this undo_log until all rollbacks are complete, as it is not until they are done that the\\n\\n\\n# Multi-Tier Proxy Issues\\nSeveral issues that can be caused by multi-tier proxying of data sources have been mentioned at the beginning of the article, focusing on analysing why the above issues are caused:\\n\\n\\n## Impact on branch transaction commits\\nLet\'s start by analysing what happens if we use a two-tier proxy. Let\'s analyse it from two aspects: `business SQL` and `undo_log`\\n1. business SQL\\n```\\n   PreparedStatementProxy1.executeUpdate =>\\n   statementCallback#executeUpdate(PreparedStatementProxy2#executeUpdate) =>\\n   PreparedStatement#executeUpdate\\n ```\\n It doesn\'t seem to matter, it\'s just an extra loop, and it\'s still executed through `PreparedStatement` in the end.\\n\\n\\n 2. undo_log\\n ```\\nConnectionProxy1#getTargetConnection ->\\nConnectionProxy2#prepareStatement ->\\nPreparedStatementProxy2#executeUpdate ->\\nPreparedStatement#executeUpdate (native undo_log write, before generating undo_log2 (the undo_log of undo_log) for that undo_log) ->\\nConnectionProxy2#commit ->\\nConnectionProxy2#processGlobalTransactionCommit(write undo_log2) ->\\nConnectionProxy2#getTargetConnection ->\\nTargetConnection#prepareStatement ->\\nPreparedStatement#executeUpdate\\n ```\\n\\n\\n ## Impact on branch transaction rollback\\n > Why isn\'t the undo_log deleted after a transaction rollback?\\n\\n It is not actually not deleted. As I said before, the two-tier proxy causes the `undo_log` to be treated as a branch transaction, so it generates an undo_log for that `undo_log` (assuming it\'s `undo_log2`), and `undo_log2` is generated wrongly (which is fine, it should be generated this way), which results in the `business-table-associated undo_log` being deleted when rolling back. This leads to a rollback that deletes the `undo_log` associated with the `business table`, which ultimately leads to the `business table corresponding to the transaction branch` rolling back to find that the undo_log does not exist, thus generating one more undo_log with a status of 1.\\n\\n **Before the rollback**\\n ```\\n // undo_log\\n 84 59734070967644161 172.16.120.59:23004:59734061438185472 serializer=jackson 1.1KB 0\\n 85 59734075254222849 172.16.120.59:23004:59734061438185472 serializer=jackson 4.0KB 0\\n\\n // branch_table\\n 59734070967644161 172.16.120.59:23004:59734061438185472 jdbc:mysql://172.16.248.10:3306/tuya_middleware\\n 59734075254222849 172.16.120.59:23004:59734061438185472 jdbc:mysql://172.16.248.10:3306/tuya_middleware\\n\\n // lock_table\\n jdbc:mysql://xx^^^seata_storage^^^1 59734070967644161 jdbc:mysql://172.16.248.10:3306/tuya_middleware seata_storage 1\\n jdbc:mysql://xx^^^^undo_log^^^^84 59734075254222849 jdbc:mysql://172.16.248.10:3306/tuya_middleware undo_log 84\\n ```\\n\\n**After the rollback**\\n ```\\n // An undo_log with status 1 was generated, corresponding to the log: undo_log added with GlobalFinished\\n 86 59734070967644161 172.16.120.59:23004:59734061438185472 serializer=jackson 1.0Byte 1\\n ```\\n\\n\\n### Problem analysis\\n1. find the corresponding undo_log log based on xid and branchId\\n2. parse the undo_log, mainly its `rollback_info` field, `rollback_info` is a `SQLUndoLog collection`, each `SQLUndoLog` corresponds to an operation, which contains a snapshot before and after the operation, and then perform a corresponding rollback\\n3. Delete undo_log logs based on xid and branchId.\\n\\nBecause of the two-tier proxy problem, an undo_log becomes a branch transaction, so when a rollback occurs, we also need to rollback the undo_log branch transaction:\\n1, first according to the xid and branchId to find the corresponding `undo_log` and parse its `rollback_info` attribute, here the parsed rollback_info contains two `SQLUndoLog`. Why are there two?\\n>If you think about it, you can understand that the first layer of proxy operations on `seata_storage` are put into the cache, which should be cleared after execution, but because of the two-tier proxy, the process is not finished at this time. When it\'s the second tier proxy\'s turn to operate on `undo_log`, it puts that operation into the cache, and at that point there are two operations in the cache, `UPDATE` for `seata_storage` and `INSERT` for `undo_log`. So it\'s easy to see why the `undo_log operation` is extra large (4KB) because it has two operations in its `rollback_info`.\\n\\nOne thing to note is that the first `SQLUndoLog` corresponds to the after snapshot, which has branchId=`59734070967644161` pk=`84`, i.e., branchId` corresponding to the `seata_storage branch and `undo_log corresponding to the `seata_storage PK`. In other words, the undo_log rollback deletes the `seata_storage corresponding undo_log`.\\nHow to delete the undo_log itself? In the next logic, it will be deleted according to xid and branchId.\\n\\n2. Parsing the first `SQLUndoLog`, it corresponds to the INSERT` operation of `undo_log`, so its corresponding rollback operation is `DELETE`. Because `undo_log` is treated as a business table at this point. So this step will delete the `59734075254222849` corresponding to the `undo_log`, **but this is actually the corresponding business table corresponding to the corresponding `undo_log`**.\\n\\n3, parse the second `SQLUndoLog`, at this time corresponds to the `seata_storage UPDATE` operation, this time will be through the snapshot of the `seata_storage` corresponding to the recovery of records\\n\\n4\u3001Delete the undo_log log according to xid and branchId, here the deletion is the undo_log of `undo_log , i.e. undo_log2`. So, by this point, both undo_logs have been deleted.\\n\\n5. Next, roll back `seata_storage`, because at this time its corresponding undo_log has been deleted in step 2, so at this time can not check the undo_log, and then regenerate a `status == 1 undo_log`.\\n\\n\\n# Case Study\\n\\n## Background\\n1. Three data sources are configured: two physical data sources and one logical data source, but the corresponding connection addresses of the two physical data sources are the same. Is this interesting?\\n ```\\n @Bean(\\"dsMaster\\")\\n DynamicDataSource dsMaster() {\\n    return new DynamicDataSource(masterDsRoute);\\n }\\n\\n @Bean(\\"dsSlave\\")\\n DynamicDataSource dsSlave() {\\n    return new DynamicDataSource(slaveDsRoute); }\\n }\\n\\n @Primary\\n @Bean(\\"masterSlave\\")\\n DataSource masterSlave(@Qualifier(\\"dsMaster\\") DataSource dataSourceMaster,\\n                        @Qualifier(\\"dsSlave\\") DataSource dataSourceSlave) throws SQLException {\\n    Map<String, DataSource> dataSourceMap = new HashMap<>(2);\\n    // Master database\\n    dataSourceMap.put(\\"dsMaster\\", dataSourceMaster);\\n    //slave\\n    dataSourceMap.put(\\"dsSlave\\", dataSourceSlave); // Configure read/write separation rules.\\n    // Configure read/write separation rules\\n    MasterSlaveRuleConfiguration masterSlaveRuleConfig = new MasterSlaveRuleConfiguration(\\n            \\"masterSlave\\", \\"dsMaster\\", Lists.newArrayList(\\"dsSlave\\")\\n    );\\n    Properties shardingProperties = new Properties();\\n    shardingProperties.setProperty(\\"sql.show\\", \\"true\\");\\n    shardingProperties.setProperty(\\"sql.simple\\", \\"true\\");\\n    // Get the data source object\\n    DataSource dataSource = MasterSlaveDataSourceFactory.createDataSource(dataSourceMap, masterSlaveRuleConfig, shardingProperties);\\n    log.info(\\"datasource initialised!\\");\\n    return dataSource;\u02da\\n }\\n ```\\n! [](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d3e05c8fc0294a8caf4d0883a4676750~tplv-k3u1fbpfcp-watermark.image)\\n\\n2, open seata\'s data source dynamic proxy, according to seata\'s data source proxy logic can be known, will eventually generate three proxy data sources, the relationship between the native data source and the proxy data source is cached in the `DataSourceProxyHolder.dataSourceProxyMap`, if the native data source and the proxy data source corresponds to the relationship is as follows:\\n```\\ndsMaster(DynamicDataSource) => dsMasterProxy(DataSourceProxy)\\ndsSlave(DynamicDataSource) => dsSlaveProxy(DataSourceProxy)\\nmasterSlave(MasterSlaveDataSource) => masterSlaveProxy(DataSourceProxy)\\n```\\n So, ultimately, the three data sources that exist in the IOC container are: dsMasterProxy, dsSlaveProxy, masterSlaveProxy. According to the @Primary feature, when we get a DataSource from the container, the default data source returned is the proxy masterSlaveProxy.\\n\\n\\n >I haven\'t studied shardingjdbc specifically, but just guessed its working mechanism based on the code I saw during the debug.\\n\\n `masterSlaveProxy` can be seen as `MasterSlaveDataSource` wrapped by DataSourceProxy. We can venture to guess that `MasterSlaveDataSource` is not a physical data source, but a logical data source, which can simply be thought of as containing routing logic. When we get a connection, we will use the routing rules inside to select a specific physical data source, and then get a real connection through that physical data source.\\n The routing rules should be able to be defined by yourself. According to the phenomenon observed when debugging, the default routing rules should be:\\n  1. for select read operations, will be routed to the slave library, that is, our dsSlave\\n  2. for update write operations, will be routed to the master library, that is, our dsMaster\\n\\n 3. When each DataSourceProxy is initialised, it will parse the connection address of that real DataSource, and then maintain that `connection address and the DataSourceProxy itself` in `DataSourceManager.dataSourceCache`. The `DataSourceManager.dataSourceCache` is used for rollback: when rolling back, it finds the corresponding `DataSourceProxy` based on the connection address, and then does the rollback operation based on that `DataSourceProxy`.\\n But we can find this problem, these three data sources are resolved to the same connection address, that is, the key is duplicated, so in the `DataSourceManager.dataSourceCache`, when the connection place is the same, after the registration of the data source will overwrite the existing one. That is: `DataSourceManager.dataSourceCache` ultimately exists `masterSlaveProxy`, that is to say, will ultimately be rolled back through the `masterSlaveProxy`, this point is very important.\\n\\n 4, the table involved: very simple, we expect a business table `seata_account`, but because of the duplicate proxy problem, resulting in seata will undo_log also as a business table\\n  1. seata_account\\n  2. undo_log\\n\\n OK, here\'s a brief background, moving on to the Seata session\\n\\n\\n ## Requirements\\n We have a simple requirement to perform a simple update operation inside a branch transaction to update the count value of `seata_account`. After the update, manually throw an exception that triggers a rollback of the global transaction.\\n To make it easier to troubleshoot and reduce interference, we use one branch transaction in the global transaction and no other branch transactions.SQL is as follows.\\n ```\\n update seata_account set count = count - 1 where id = 100;\\n ```\\n\\n## Problems\\n\\nClient: In the console log, the following logs are printed over and over again\\n1. the above logs are printed at 20s intervals, and I checked the value of the `innodb_lock_wait_timeout` property of the database, and it happens to be 20, which means that every time a rollback request comes through, the rollback fails because of the timeout for acquiring the lock (20).\\n2. Why is it not printed once after 20s? Because the server side will have a timer to process the rollback request.\\n\\n ```\\n // Branch rollback starts\\n Branch rollback start: 172.16.120.59:23004:59991911632711680 59991915571163137 jdbc:mysql://172.16.248.10:3306/tuya_middleware\\n\\n // undo_log transaction branch The original action corresponds to insert, so it rolls back to delete.\\n undoSQL undoSQL=DELETE FROM undo_log WHERE id = ?  and PK=[[id,139]]\\n // Since the corresponding operation of the first-level agent is also in the context, when the undo_log branch transaction commits, the corresponding undo_log contains two actions\\n undoSQL undoSQL=UPDATE seata_account SET money = ? WHERE id = ?  and PK=[[id,1]].\\n\\n // After the branch transaction has been rolled back, delete the corresponding undo_log for that branch transaction\\n delete from undo_log where xid=172.16.120.59:23004:59991911632711680 AND branchId=59991915571163137\\n\\n // Threw an exception indicating that the rollback failed because `Lock wait timeout exceeded`, and failed when deleting the undo_log based on the xid and branchId because a lock acquisition timeout occurred, indicating that there was another operation that held a lock on the record that was not released.\\n branchRollback failed. branchType:[AT], xid:[172.16.120.59:23004:59991911632711680], branchId:[59991915571163137], resourceId:[jdbc. mysql://172.16.248.10:3306/tuya_middleware], applicationData:[null]. reason:[Branch session rollback failed and try again later xid = 172.16.120.59:23004:59991911632711680 branchId = 59991915571163137 Lock wait timeout exceeded; try restarting transaction]\\n ```\\n\\n Server: the following log is printed every 20s, indicating that the server is constantly retrying to send a rollback request\\n ```\\nRollback branch transaction failed and will retry, xid = 172.16.120.59:23004:59991911632711680 branchId = 59991915571163137\\n ```\\n\\n The SQL involved in the process is roughly as follows:\\n ```sql\\n 1. SELECT * FROM undo_log WHERE branch_id = ? AND xid = ? FOR UPDATE slaveDS\\n 2. SELECT * FROM undo_log WHERE (id ) in ( (?)  ) slaveDS\\n 3. DELETE FROM undo_log WHERE id = ?                                masterDS\\n 4. SELECT * FROM seata_account WHERE (id ) in ( (?)  ) masterDS\\n 5. UPDATE seata_account SET money = ? WHERE id = ?                      masterDS\\n 6. DELETE FROM undo_log WHERE branch_id = ? AND xid = ?                 masterDS\\n ```\\n\\n\\nAt this point, check the database transaction status, lock status, lock wait relationship\\n1, check the current transaction being executed\\n ```sql\\n SELECT * FROM information_schema.INNODB_TRX.\\n ```\\n! [](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d9852b91a9949f781e1f90bffe95fbf~tplv-k3u1fbpfcp-watermark.image)\\n\\n2. Check the current lock status\\n ```sql\\n SELECT * FROM information_schema.INNODB_LOCKs;\\n ```\\n! [](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1a29748a3af34e7c90e3aa7cb78564bc~tplv-k3u1fbpfcp-watermark.image)\\n\\n3. Check the current lock wait relationship\\n ```sql\\n SELECT * FROM information_schema.INNODB_LOCK_waits;\\n\\n SELECT\\n  block_trx.trx_mysql_thread_id AS sessionID that already holds a lock, request_trx.\\n  request_trx.trx_mysql_thread_id AS the sessionID that is requesting the lock,\\n  block_trx.trx_query AS the SQL statement that already holds the lock, request_trx.\\n  request_trx.trx_query AS the SQL statement for which the lock is being requested,\\n  waits.blocking_trx_id AS Transaction ID that already holds the lock, waits.requesting_trx.trx_query\\n  waits.requesting_trx_id AS \u6b63\u5728\u7533\u8bf7\u9501\u7684\u4e8b\u52a1ID,\\n  waits.requested_lock_id AS the ID of the lock object, waits.\\n  locks.lock_table AS lock_table, -- table locked by the lock object\\n  locks.lock_type AS lock_type, -- lock type\\n  locks.lock_mode AS lock_mode -- lock mode\\n FROM\\n  information_schema.innodb_lock_waits AS waits\\n  INNER JOIN information_schema.innodb_trx AS block_trx ON waits.blocking_trx_id = block_trx.trx_id\\n  INNER JOIN information_schema.innodb_trx AS request_trx ON waits.requesting_trx_id = request_trx.trx_id\\n  INNER JOIN information_schema.innodb_locks AS locks ON waits.requested_lock_id = locks.lock_id;\\n ```\\n! [](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4ca5b50cab534a69a49c3e470518e3b6~tplv-k3u1fbpfcp-watermark.image)\\n\\n\\n\\n1. the record involved is `branch_id = 59991915571163137 AND xid = 172.16.120.59:23004:59991911632711680`.\\n2. transaction ID `1539483284` holds the lock for this record, but its corresponding SQL is empty, so it should be waiting for a commit.\\n3. transaction ID `1539483286` is trying to acquire a lock on this record, but the logs show that it is waiting for a lock timeout.\\n\\nProbably a good guess is that `select for update` and `delete from undo ... ` are in conflict. According to the logic in the code, these two operations should have been committed in a single transaction, so why have they been separated into two transactions?\\n\\n\\n## Problem Analysis\\nIn conjunction with the rollback process described above, let\'s look at what happens during the rollback of our example.\\n1. first get the data source, at this time dataSourceProxy.getPlainConnection() to get the `MasterSlaveDataSource` data source\\n2. during the `select for update` operation, get a `Connection` from the `MasterSlaveDataSource`, as I said before, the `MasterSlaveDataSource` is a logical datasource, which has a routing logic, according to the above, this time we get the `dsSlave`\'s `Connection`, and then we get the `ddsSlave`\'s `Connection`. dsSlave`\'s `Connection`.\\n3. When executing the `delete from undo ... 3. When performing the `delete from undo ...\' operation, you get the `Connection` from the `dsMaster\'.\\n4. Although `dsSlave` and `dsMaster` correspond to the same address, they must be getting different connections, so the two operations must be spread across two transactions.\\n5. the transaction that executes `select for update` will wait until the deletion of the undo_log is complete before committing.\\n6. the transaction that executes `delete from undo ... The transaction executing `delete from undo ...\' waits for the `select for update` transaction to release the lock.\\n7. Typical deadlock problem\\n\\n## Verify the conjecture\\nI tried to verify this problem in two ways:\\n1. change the Seata code from `select for update` to `select`, then the query to undo_log does not need to hold a lock on the record, and will not cause a deadlock.\\n\\n\\n2. change the data source proxy logic, this is the key to the problem, the main cause of the problem is not `select for update`. The main cause of the problem is not `select for update`. The multi-layer proxy problem has already been created before that, and then it will cause the deadlock problem. We should never have proxied the `masterSlave` datasource in the first place. It\'s just a logical data source, so why proxy it? If we proxy the `masterSlave`, we won\'t cause multiple layers of proxies, and we won\'t cause the deadlock problem when deleting the undo_log!\\n\\n\\n## Final implementation\\n`masterSlave` is also a `DataSource` type, how to proxy just `dsMaster` and `dsSlave` but not `masterSlave`? Observing the `SeataAutoDataSourceProxyCreator#shouldSkip` method, we can solve this problem with the `excludes` attribute of the EnableAutoDataSourceProxy annotation\\n```\\n@Override\\nprotected boolean shouldSkip(Class<? > beanClass, String beanName) {\\nreturn SeataProxy.class.isAssignableFrom(beanClass) ||\\nDataSourceProxy.class.isAssignableFrom(beanClass) ||\\n!DataSource.class.isAssignableFrom(beanClass) ||\\nArrays.asList(excludes).contains(beanClass.getName());\\n}\\n ```\\n i.e.: turn off the data source autoproxy, then add this annotation to the startup class\\n ````\\n@EnableAutoDataSourceProxy(excludes = {\\"org.apache.shardingsphere.shardingjdbc.jdbc.core.datasource.MasterSlaveDataSource\\"})\\n ````\\n\\n\\n # Autoproxy optimisation in new releases\\n Since `Seata 1.4.0` has not been officially released yet, I\'m currently looking at the `1.4.0-SNAPSHOT` version of the code, which is the latest code in the `ddevelop` branch at the current time\\n\\n ## Code changes\\n The main changes are as follows, but I won\'t go into too much detail on the minor ones:\\n 1. `DataSourceProxyHolder` adjustment\\n 2. `DataSourceProxy` adjustment\\n 3. `SeataDataSourceBeanPostProcessor` is added.\\n\\n ### DataSourceProxyHolder\\n The most significant of the changes to this class are to its `putDataSource` method\\n ```java\\n public SeataDataSourceProxy putDataSource(DataSource dataSource, BranchType dataSourceProxyMode) {\\n    DataSource originalDataSource; if (dataSource instanceof SeataDataSource)\\n    if (dataSource instanceof SeataDataSourceProxy) {\\n        SeataDataSourceProxy dataSourceProxy = (SeataDataSourceProxy) dataSource;\\n        // If this is a proxy data source and it is the same as the current application\'s configured data source proxy mode (AT/XA), then return it directly\\n        if (dataSourceProxyMode == dataSourceProxy.getBranchType()) {\\n            return (SeataDataSourceProxy)dataSource; }\\n        }\\n\\n        // If it\'s a proxy data source, and the data source proxy mode (AT/XA) is different from the one configured by the current application, then you need to get its TargetDataSource and create a proxy data source for it.\\n        originalDataSource = dataSourceProxy.getTargetDataSource(); } else { dataSourceProxy.getTargetDataSource()\\n    } else {\\n        originalDataSource = dataSource; } else { originalDataSource = dataSource.\\n    }\\n\\n    // If necessary, create a proxy data source based on the TargetDataSource.\\n    return this.dataSourceProxyMap.computeIfAbsent(originalDataSource, originalDataSource, BranchType.\\n            BranchType.XA == dataSourceProxyMode ? DataSourceProxyXA::new : DataSourceProxy::new); }\\n }\\n ```\\n The `DataSourceProxyHolder#putDataSource` method is used in two main places: in the `SeataAutoDataSourceProxyAdvice` cutout; and in the `SeataDataSourceBeanPostProcessor`.\\n What problem does this judgement solve for us? The problem of multi-tier proxying of data sources. Think about the following scenarios with automatic data source proxying turned on:\\n 1. If we manually injected a `DataSourceProxy` into our project, a call to the `DataSourceProxyHolder#putDataSource` method in a cutover would return the `DataSourceProxy` itself directly, without creating another ` DataSourceProxy\\n 2. if we manually inject a `DruidSource` into the project, then the `DataSourceProxyHolder#putDataSource` method will create another `DataSourceProxy` for it and return it when it is called from the facet.\\n\\n It looks like the problem is solved, but is it possible that there are other problems? Take a look at the following code\\n ```java\\n @Bean\\n public DataSourceProxy dsA(){\\n    return new DataSourceProxy(druidA)\\n }\\n\\n @Bean\\n public DataSourceProxy dsB(DataSourceProxy dsA){\\n    return new DataSourceProxy(dsA)\\n }\\n ```\\n 1. this is definitely wrong, but you can\'t help it if he wants to write it this way\\n 2. there\'s nothing wrong with `dsA`, but `dsB` still has a double proxy problem, because the TargetDataSource of `dsB` is `dsA`.\\n 3. This brings us to the `DataSourceProxy` change.\\n\\n ### DataSourceProxy\\n ```java\\n public DataSourceProxy(DataSource targetDataSource, String resourceGroupId) {\\n    // The following judgement ensures that we don\'t have a two-tier proxy problem even when we pass in a DataSourceProxy\\n    if (targetDataSource instanceof SeataDataSourceProxy) {\\n        LOGGER.info(\\"Unwrap the target data source, because the type is: {}\\", targetDataSource.getClass().getName());\\n        targetDataSource = ((SeataDataSourceProxy) targetDataSource).getTargetDataSource();\\n    }\\n    this.targetDataSource = targetDataSource;\\n    init(targetDataSource, resourceGroupId);\\n }\\n ```\\n\\n ### SeataDataSourceBeanPostProcessor\\n ```java\\n public class SeataDataSourceBeanPostProcessor implements BeanPostProcessor {\\n    private static final Logger LOGGER = LoggerFactory.getLogger(SeataDataSourceBeanPostProcessor.class);\\n\\n    ......\\n\\n    @Override\\n    public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException {\\n        if (bean instanceof DataSource) {\\n            //When not in the excludes, put and init proxy. if (!excludes.contains.\\n            if (!excludes.contains(bean.getClass().getName())) {\\n                //Only put and init proxy, not return proxy.\\n                DataSourceProxyHolder.get().putDataSource((DataSource) bean, dataSourceProxyMode);\\n            }\\n\\n            //If is SeataDataSourceProxy, return the original data source.\\n            if (bean instanceof SeataDataSourceProxy) {\\n                LOGGER.info(\\"Unwrap the bean of the data source,\\" +\\n                    \\" and return the original data source to replace the data source proxy.\\"); return ((SeataDataSourceProxy); } }\\n                return ((SeataDataSourceProxy) bean).getTargetDataSource();\\n            }\\n        }\\n        return bean.\\n    }\\n }\\n ```\\n 1. `SeataDataSourceBeanPostProcessor` implements the `BeanPostProcessor` interface, which executes the `BeanPostProcessor#postProcessAfterInitialization` method after a bean is initialised. That is, in the `postProcessAfterInitialization` method, the bean is already available at this point.\\n 2. Why provide such a class? From its code, it is just to initialise the corresponding `DataSourceProxy` for the data source after the bean has been initialised, but why is this necessary?\\n > Because some data sources may not be initialised (i.e. the relevant methods of the data source will not be called) after the application is started. If the `SeataDataSourceBeanPostProcessor` class is not provided, then the `DataSourceProxyHolder#putDataSource` method will only be triggered in the `SeataAutoDataSourceProxyAdvice` cutout. If a client goes down during the rollback, after restarting, the Server sends it a rollback request via a timed task, at which point the client needs to first find the corresponding `DatasourceProxy` based on the `rsourceId` (connection address). However, if the client hasn\'t triggered the data source\'s related methods before then, it won\'t enter the `SeataAutoDataSourceProxyAdvice` cutover logic, and won\'t initialise the corresponding `DataSourceProxy` for the data source, which will result in the failure of the rollback.\\n\\n ## Multi-Layer Proxy Summary\\n Through the above analysis, we probably already know some optimisations of seata in avoiding multi-layer proxies, but there is actually one more issue to pay attention to:** Logical data source proxies**\\n ! [](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6910095aadab436eaffe03a752e44240~tplv-k3u1fbpfcp-watermark.image)\\n\\n The calling relationship at this point is: `masterSlaveProxy -> masterSlave -> masterproxy/slaveProxy -> master/slave`\\n\\n At this point you can exclude the logical datasource via the `excludes` attribute so that no datasource proxy is created for it.\\n\\n\\n To summarise:\\n 1. when initialising the corresponding `DataSourceProxy` for a `DataSource`, determine whether it is necessary to create a corresponding `DataSourceProxy` for it, and if it is a `DataSourceProxy` itself, return it directly.\\n 2. For the case of manual injection of some `DataSource`, in order to avoid the problem of multi-layer proxy caused by human error, we add a judgement in the constructor of `DataSourceProxy`, `If the input parameter TragetDatasource is a DataSourceProxy itself, then we get the target attribute of TragetDatasource as the target attribute of the new DataSourceProxy. TragetDatasource` of the new DataSourceProxy.\\n 3. for other cases, such as **logical data source proxy issues**, add exclusions to the `excludes` attribute to avoid creating a `DataSourceProxy` for the logical data source.\\n\\n\\n # Suggestions for using global and local transactions\\n There is a question, if there are multiple DB operations involved in a method, say 3 update operations are involved, do we need to use `@Transactional` annotation in spring for this method? We consider this question from two perspectives: without `@Transactional` annotation and with `@Transactional` annotation.\\n\\n\\n ## Not using the `@Transactional` annotation\\n 1. in the commit phase, since the branch transaction has 3 update operations, each time the update operation is executed, a branch transaction will be registered with the TC through the data broker and a corresponding undo_log will be generated for it, so that the 3 update operations will be treated as 3 branch transactions\\n 2. In the rollback phase, the three branch transactions need to be rolled back.\\n 3. data consistency is ensured by the seata global transaction.\\n\\n ## Use the `@Transactional` annotation.\\n 1. in the commit phase, the three update operations are committed as one branch transaction, so only one branch transaction will be registered in the end\\n 2. in the rollback phase, 1 branch transaction needs to be rolled back.\\n 3. data consistency: the 3 update operations are guaranteed by the consistency of the local transaction; global consistency is guaranteed by the seata global transaction. At this point, the 3 updates are just a branch transaction.\\n\\n ## Conclusion\\n Through the above comparison, the answer is obvious, the reasonable use of local transactions can greatly improve the processing speed of global transactions. The above is just 3 DB operations, what if there are more DB operations involved in a method, then the difference between the two ways is not greater?\\n\\n\\n\\n Finally, thanks to @FUNKYE for answering a lot of questions and providing valuable suggestions!"},{"id":"/seata-sourcecode-client-bootstrap","metadata":{"permalink":"/blog/seata-sourcecode-client-bootstrap","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-sourcecode-client-bootstrap.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-sourcecode-client-bootstrap.md","title":"Seata Source Code - Client Startup Process in Distributed Transactions","description":"\u3010Distributed Transaction Seata source code interpretation II\u3011 Client-side startup process","date":"2020-08-25T00:00:00.000Z","formattedDate":"August 25, 2020","tags":[],"readingTime":10.1,"hasTruncateMarker":false,"authors":[{"name":"xiaobin.yang"}],"frontMatter":{"title":"Seata Source Code - Client Startup Process in Distributed Transactions","author":"xiaobin.yang","date":"2020/08/25","keywords":["fescar","seata","distributed transaction"]},"unlisted":false,"prevItem":{"title":"Analysis of Seata Data Source Proxy","permalink":"/blog/seata-datasource-proxy"},"nextItem":{"title":"Setting Up Seata Demo Environment on Mac (AT Mode)","permalink":"/blog/seata-at-demo-in-mac"}},"content":"## \u3010Distributed Transaction Seata source code interpretation II\u3011 Client-side startup process\\n\\nIn this paper, we analyse the Client-side startup process in AT mode from the source code point of view, the so-called Client-side, i.e. the business application side. Distributed transactions are divided into three modules: TC, TM, RM, where TC is located in the seata-server side, while TM, RM through the SDK way to run in the client side.\\n\\nThe following figure shows a distributed transaction scenario of Seata\'s official demo, divided into the following several microservices, which together implement a distributed transaction of placing an order, deducting inventory, and deducting balance.\\n* **BusinessService: ** business service, the entrance to the order placing service\\n* **StorageService\uff1a** Inventory microservice, used to deduct the inventory of goods\\n* **OrderService:** Order microservice, to create orders\\n* **AccountService:** Account microservice, debits the balance of the user\'s account\\n\\n! [Insert image description here](https://img-blog.csdnimg.cn/20200820184156758.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10, text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTE0NTg0OA==,size_16,colour_FFFFFF,t_70#pic_center)\\n\\nIt can also be seen from the above figure that in AT mode Seata Client side implements distributed transactions mainly through the following three modules:\\n* **GlobalTransactionScanner:** GlobalTransactionScanner is responsible for initialising the TM, RM module and adding interceptors for methods that add distributed transaction annotations, the interceptors are responsible for the opening, committing or rolling back of the global transaction\\n* **DatasourceProxy:** DatasourceProxy for DataSource to add interception , the interceptor will intercept all SQL execution , and as RM transaction participant role in the distributed transaction execution .\\n* **Rpc Interceptor:** In the previous article [Distributed Transaction Seata Source Code Interpretation I](https://blog.csdn.net/weixin_45145848/article/details/106930538) there are a few core points of distributed transaction mentioned, one of which is Cross-Service Instance Propagation of Distributed Transactions The Rpc Interceptor is responsible for propagating transactions across multiple microservices.\\n\\n## seata-spring-boot-starter\\nThere are two ways to refer to the seata Distributed Transaction SDK, relying on seata-all or seata-spring-boot-starter. It is recommended to use the seata-spring-boot-starter because the starter has automatically injected the three modules mentioned above, and the user only needs to add the corresponding configuration in the business code to add a global distributed transaction annotation can be. Here\'s how to start with the code in the seata-spring-boot-starter project:\\n\\nThe following figure shows the project structure of seata-spring-boot-starter:\\n! [Insert image description here](https://img-blog.csdnimg.cn/20200810204518853.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10, text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTE0NTg0OA==,size_16,colour_FFFFFF,t_70)\\nIt is mainly divided into the following modules:\\n* **properties:** The properties directory contains the configuration classes that Springboot adapts to seata, i.e., you can use SpringBoot\'s configuration to configure the parameters of seata.\\n* **provider:** The classes in the provider directory are responsible for adapting Springboot and SpringCloud configurations to the Seata configuration.\\n* **resources:** There are two main files in the resources directory, spring.facts for registering Springboot auto-assembly classes and ExtConfigurationProvider for registering the SpringbootConfigurationProvider class, the Provider class is responsible for adapting SpringBoot related configuration classes to Seata.\\n\\nFor the springboot-starter project, let\'s first look at the resources/META-INF/spring.factors file:\\n```properties.\\n# Auto Configure\\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\nio.seata.spring.boot.autoconfigure.SeataAutoConfiguration\\n```\\nYou can see that the autoconfiguration class is configured in spring.facts: SeataAutoConfiguration, in which two instances of GlobalTransactionScanner and seataAutoDataSourceProxyCreator are injected. The code is as follows:\\n```java.\\n@ComponentScan(basePackages = \\"io.seata.spring.boot.autoconfigure.properties\\")\\n@ConditionalOnProperty(prefix = StarterConstants.SEATA_PREFIX, name = \\"enabled\\",\\n        havingValue = \\"true\\",\\n        matchIfMissing = true)\\n@Configuration\\n@EnableConfigurationProperties({SeataProperties.class})\\npublic class SeataAutoConfiguration {\\n\\n  ...\\n\\n  // GlobalTransactionScanner is responsible for adding interceptors to methods that add the GlobalTransaction annotation.\\n  // and is responsible for initialising the RM, TM\\n  @Bean\\n  @DependsOn({BEAN_NAME_SPRING_APPLICATION_CONTEXT_PROVIDER, BEAN_NAME_FAILURE_HANDLER})\\n  @ConditionalOnMissingBean(GlobalTransactionScanner.class)\\n  public GlobalTransactionScanner globalTransactionScanner(SeataProperties seataProperties,\\n                                                           FailureHandler failureHandler) {\\n    if (LOGGER.isInfoEnabled()) {\\n      LOGGER.info(\\"Automatically configure Seata\\");\\n    }\\n    return new GlobalTransactionScanner(seataProperties.getApplicationId(),\\n            seataProperties.getApplicationId(), seataProperties.getTxServiceGroup(), failureHandler); }\\n            failureHandler); }\\n  }\\n\\n  // The SeataAutoDataSourceProxyCreator is responsible for generating proxies for all DataSources in Spring.\\n  // This enables the interception of all SQL execution.\\n  @Bean(BEAN_NAME_SEATA_AUTO_DATA_SOURCE_PROXY_CREATOR)\\n  @ConditionalOnProperty(prefix = StarterConstants.SEATA_PREFIX, name = {\\n          \\"enableAutoDataSourceProxy\\", \\"enable-auto\\" +\\n          \\"-data-source-proxy\\"}, havingValue = \\"true\\", matchIfMissing = true)\\n  @ConditionalOnMissingBean(SeataAutoDataSourceProxyCreator.class)\\n  public SeataAutoDataSourceProxyCreator seataAutoDataSourceProxyCreator(SeataProperties seataProperties) {\\n    return new SeataAutoDataSourceProxyCreator(seataProperties.isUseJdkProxy(), seataProperties.getExpressionCreator(seataProperties.getExpressionCreator))\\n            seataProperties.getExcludesForAutoProxying());\\n  }\\n}\\n```\\n\\n### GlobalTransactionScanner\\nGlobalTransactionScanner inherits from AutoProxyCreator, which is a way to implement AOP in Spring to intercept all instances in Spring and determine whether they need to be proxied. Below is a list of some of the more important fields in GlobalTransactionScanner and the core methods for intercepting proxies:\\n```java\\npublic class GlobalTransactionScanner extends AbstractAutoProxyCreator\\n        implements InitialisingBean, ApplicationContextAware,\\n        DisposableBean {\\n  ...\\n  // The interceptor field is the interceptor corresponding to a proxy object.\\n  // It can be thought of as a temporary variable with an expiration date of a proxied object.\\n  private MethodInterceptor interceptor; // globalTransactionalInterceptor.\\n\\n  // globalTransactionalInterceptor is the generic Interceptor.\\n  // It is used by all non-TCC transactional methods.\\n  private MethodInterceptor globalTransactionalInterceptor; // PROXYED_SETTING_OBJECT\\n\\n  // PROXYED_SET stores instances that have already been proxied to prevent duplicate processing.\\n  private static final Set<String> PROXYED_SET = new HashSet<>(); // applicationId is the name of a service.\\n\\n  // applicationId is a unique identifier for a service.\\n  // corresponds to spring.application.name in the springcloud project\\n  private final String applicationId; // The group identifier of the transaction.\\n  // Grouping identifier for the transaction, refer to the wiki article: https://seata.apache.org/zh-cn/docs/user/txgroup/transaction-group/\\n  private final String txServiceGroup; // The group identifier of the transaction.\\n\\n  ...\\n\\n // Determine whether the target object needs to be proxied, and if so, generate an interceptor and assign it to the class variable interceptor.\\n  @Override\\n  protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) {\\n  \\t// Determine if distributed transactions are disabled\\n    if (disableGlobalTransaction) {\\n      return bean; }\\n    }\\n    try {\\n      synchronized (PROXYED_SET) {\\n        if (PROXYED_SET.contains(beanName)) {\\n          return bean; }\\n        }\\n\\n        // Each time a proxied object is processed, the intermediary is set to null, so the intermediary\'s // lifecycle is that of a proxied object.\\n        // lifecycle is a proxied object, and since the intermediary is used in a separate method, getAdvicesAndAdvisorsForBean\\n        // Since the interceptor is used in a separate method getAdvicesAndAdvisorsForBean, the interceptor is defined as a class variable\\n        interceptor = null; // Determine if this is a TCC transaction.\\n\\n        // Determine whether this is TCC transaction mode, primarily based on the presence of the TwoPhaseBusinessAction annotation on the method\\n        if (TCCBeanParserUtils.isTccAutoProxy(bean, beanName, applicationContext)) { if (TCCBeanParserUtils.isTccAutoProxy(bean, beanName,\\n                applicationContext)) {\\n          // Create an interceptor for the TCC transaction\\n          interceptor =\\n                  new TccActionInterceptor(TCCBeanParserUtils.getRemotingDesc(beanName));\\n        } else {\\n          // Get the class type of the object to be processed\\n          Class<? > serviceInterface = SpringProxyUtils.findTargetClass(bean); } else { // Get the class type of the object to be processed.\\n          // Get all interfaces inherited by the object to be processed\\n          Class<? >[] interfacesIfJdk = SpringProxyUtils.findInterfaces(bean); // Get all interfaces inherited by the pending object.\\n\\n          // If there is a GlobalTransactional annotation on the class of the pending object or on the inherited interfaces.\\n          // or any of the methods of the class of the object to be handled have a GlobalTransactional or\\n          // GlobalLock annotation on any of the methods of the class of the object to be handled returns true, i.e., it needs to be proxied.\\n          if (!existsAnnotation(new Class[]{serviceInterface})\\n                  && !existsAnnotation(interfacesIfJdk)) {\\n            return bean;\\n          }\\n\\n          // If the interceptor is null, i.e. not in TCC mode.\\n          // then use globalTransactionalInterceptor as the interceptor\\n          if (interceptor == null) {\\n            // globalTransactionalInterceptor will only be created once\\n            if (globalTransactionalInterceptor == null) {\\n              globalTransactionalInterceptor =\\n                      new GlobalTransactionalInterceptor(failureHandlerHook);\\n              ConfigurationCache.addConfigListener(\\n                      ConfigurationKeys.DISABLE_GLOBAL_TRANSACTION, (ConfigurationChangeListener.addConfigListener(\\n                      (ConfigurationChangeListener) globalTransactionalInterceptor);\\n            }\\n            interceptor = globalTransactionalInterceptor;\\n          }\\n        }\\n\\n        if (!AopUtils.isAopProxy(bean)) {\\n          // If the bean itself is not a Proxy object, then the parent class wrapIfNecessary is called to generate the proxy object\\n          // In the parent class, getAdvicesAndAdvisorsForBean is called to get the interceptor defined above.\\n          bean = super.wrapIfNecessary(bean, beanName, cacheKey); } else { getAdvicesAndAdvisorsForBean(bean, beanName, cacheKey); }\\n        } else {\\n          // If the bean is already a proxy, add a new interceptor directly to the proxy\'s interceptor call chain, AdvisedSupport\\n          // and add the new interceptor directly to the proxy\'s interception invocation chain.\\n          AdvisedSupport advised = SpringProxyUtils.getAdvisedSupport(bean);\\n          Advisor[] advisor = buildAdvisors(beanName,\\n                  getAdvicesAndAdvisorsForBean(null, null, null));\\n          for (Advisor avr : advisor) {\\n            advised.addAdvisor(0, avr);\\n          }\\n        }\\n        // Mark that the beanName has been processed\\n        PROXYED_SET.add(beanName);\\n        return bean; }\\n      }\\n    } catch (Exception exx) {\\n      throw new RuntimeException(exx); }\\n    }\\n  }\\n\\n  // Return the interceptor object computed in the wrapIfNecessary method.\\n  @Override\\n  protected Object[] getAdvicesAndAdvisorsForBean(Class beanClass, String beanName,\\n                                                  TargetSource customTargetSource)\\n          throws BeansException {\\n    return new Object[]{interceptor};\\n  }\\n}\\n```\\n\\nThe above describes how GlobalTransactionScanner intercepts global transactions via annotations, the specific interceptor implementations are TccActionInterceptor and GlobalTransactionalInterceptor, for the AT pattern we are mainly concerned with the GlobalTransactionalInterceptor, in subsequent articles will introduce the specific implementation of GlobalTransactionalInterceptor.\\n\\nIn addition GloabalTransactionScanner is also responsible for the initialisation of TM, RM, which is implemented in the initClient method:\\n```java\\nprivate void initClient() {\\n...\\n\\n    // Initialise the TM\\n    TMClient.init(applicationId, txServiceGroup); ...\\n    ...\\n\\n    //Initialise RM\\n    RMClient.init(applicationId, txServiceGroup); ...\\n\\t...\\n\\n    // Register the Spring shutdown callback to free up resources.\\n    registerSpringShutdownHook(); ... // Register the Spring shutdown callback for releasing resources.\\n\\n}\\n```\\n\\nTMClient, RMClient are Seata based on Netty implementation of the Rpc framework of the client class, just business logic is different, due to TMClient is relatively more simple, we take RMClient as an example to see the source code:\\n```java\\npublic class RMClient {\\n  // RMClient\'s init is a static method that creates an instance of RmNettyRemotingClient and calls the init method.\\n  public static void init(String applicationId, String transactionServiceGroup) {\\n    RmNettyRemotingClient rmNettyRemotingClient =\\n            RmNettyRemotingClient.getInstance(applicationId, transactionServiceGroup);\\n    rmNettyRemotingClient.setResourceManager(DefaultResourceManager.get());\\n    rmNettyRemotingClient.setTransactionMessageHandler(DefaultRMHandler.get()); rmNettyRemotingClient.setTransactionMessageHandler(DefaultRMHandler.get());\\n    rmNettyRemotingClient.init();\\n  }\\n}\\n```\\n\\nRmNettyRemotingClient is implemented as follows:\\n\\n```java\\n@Sharable\\npublic final class RmNettyRemotingClient extends AbstractNettyRemotingClient {\\n  // ResourceManager is responsible for handling transaction participants, supports AT, TCC and Saga modes.\\n\\n  // RmNettyRemotingClient singleton.\\n  private static volatile RmNettyRemotingClient instance; // RmNettyRemotingClient instance; // RmNettyRemotingClient instance.\\n  private final AtomicBoolean initialised = new AtomicBoolean(false); // The unique identifier of the microservice.\\n  // Unique identifier of the microservice\\n  private String applicationId; // Distributed transaction group name.\\n  // Distributed transaction group name\\n  private String transactionServiceGroup; // The name of the distributed transaction group.\\n\\n  // The init method is called by the init method in RMClient.\\n  public void init() {\\n    // Register the Processor for Seata\'s custom Rpc.\\n    registerProcessor(); // If (initialised.compareAndAndroid)\\n    if (initialised.compareAndSet(false, true)) {\\n      // Call the init method of the parent class, which is responsible for initialising Netty and establishing a connection to the Seata-Server in the parent class\\n      super.init();\\n    }\\n  }\\n\\n  // Register the Processor for the Seata custom Rpc.\\n  private void registerProcessor() {\\n    // 1. Register the Processor for the Seata-Server initiating the branchCommit.\\n    RmBranchCommitProcessor rmBranchCommitProcessor =\\n            new RmBranchCommitProcessor(getTransactionMessageHandler(), this);\\n    super.registerProcessor(MessageType.TYPE_BRANCH_COMMIT, rmBranchCommitProcessor,\\n            messageExecutor); messageExecutor\\n\\n    // 2. Register the Processor for the Seata-Server initiating the branchRollback.\\n    RmBranchRollbackProcessor rmBranchRollbackProcessor =\\n            new RmBranchRollbackProcessor(getTransactionMessageHandler(), this);\\n    super.registerProcessor(MessageType.TYPE_BRANCH_ROLLBACK, rmBranchRollbackProcessor\\n            , messageExecutor);\\n\\n    // 3. Register the Processor for the Seata-Server initiating the deletion of the undoLog.\\n    RmUndoLogProcessor rmUndoLogProcessor =\\n            new RmUndoLogProcessor(getTransactionMessageHandler());\\n    super.registerProcessor(MessageType.TYPE_RM_DELETE_UNDOLOG, rmUndoLogProcessor,\\n            rmUndoLogProcessor, rmUndoLogProcessor); messageExecutor);\\n\\n    // 4. Register the Processor for the response returned by Seata-Server, ClientOnResponseProcessor.\\n    // Used to process the Request initiated by the Client and the Response returned by the Seata-Server.\\n    The ClientOnResponseProcessor // is responsible for processing the Request sent by the Client and the Response returned by the Seata-Server.\\n    // Response returned by the Seata-Server, thus implementing Rpc.\\n    ClientOnResponseProcessor onResponseProcessor =\\n            new ClientOnResponseProcessor(mergeMsgMap, super.getFutures(),\\n                    getTransactionMessageHandler());\\n    super.registerProcessor(MessageType.TYPE_SEATA_MERGE_RESULT, onResponseProcessor,\\n            null); super.registerProcessor(MessageType.TYPE_SEATA_MERGE_RESULT, onResponseProcessor, null)\\n    super.registerProcessor(MessageType.TYPE_BRANCH_REGISTER_RESULT, onResponseProcessor, null); super.registerProcessor(MessageType.\\n            onResponseProcessor, null); super.registerProcessor(MessageType.\\n    super.registerProcessor(MessageType.TYPE_BRANCH_STATUS_REPORT_RESULT, onResponseProcessor, null); super.registerProcessor(MessageType.\\n            onResponseProcessor, null);\\n    super.registerProcessor(MessageType.TYPE_GLOBAL_LOCK_QUERY_RESULT,\\n            onResponseProcessor, null); super.registerProcessor(MessageType.\\n    super.registerProcessor(MessageType.TYPE_REG_RM_RESULT, onResponseProcessor, null);\\n\\n    // 5. Processing Pong messages returned by Seata-Server\\n    ClientHeartbeatProcessor clientHeartbeatProcessor = new ClientHeartbeatProcessor();\\n    super.registerProcessor(MessageType.TYPE_HEARTBEAT_MSG, clientHeartbeatProcessor,\\n            null);\\n  }\\n}\\n```\\n\\nThe above logic seems to be quite complex, and there are many related classes, such as Processor, MessageType, TransactionMessageHandler, ResourceManager, etc. In fact, it\'s essentially an Rpc call, which can be divided into Rm-initiated and Seata-initiated calls.\\n* **Rm active call methods:** such as: registering branches, reporting branch status, applying global locks, etc. Rm active call methods need to be in the ClientOnResponseProcessor to handle the Response returned by Seata-Server.\\n* **Seata-Server active call methods:** such as: commit branch transactions, rollback branch transactions, delete undolog log. Seata-Server active call methods, the Client side corresponds to a different Processor to deal with, and after the end of processing to return to the Seata-Server processing results. Response. The core implementation logic of transaction commit and rollback are in TransactionMessageHandler and ResourceManager.\\n\\nAbout TransactionMessageHandler, ResourceManager implementation will also be described in detail in subsequent chapters.\\n\\nThe next article will introduce the SeataAutoDataSourceProxyCreator, Rpc Interceptor is how to initialise and intercept."},{"id":"/seata-at-demo-in-mac","metadata":{"permalink":"/blog/seata-at-demo-in-mac","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-at-demo-in-mac.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-at-demo-in-mac.md","title":"Setting Up Seata Demo Environment on Mac (AT Mode)","description":"Seata Demo environment build under Mac (AT mode)","date":"2020-07-20T00:00:00.000Z","formattedDate":"July 20, 2020","tags":[],"readingTime":7.2,"hasTruncateMarker":false,"authors":[{"name":"portman xu"}],"frontMatter":{"title":"Setting Up Seata Demo Environment on Mac (AT Mode)","author":"portman xu","date":"2020/07/20","keywords":["seata","distributed transaction","demo","mac","at"]},"unlisted":false,"prevItem":{"title":"Seata Source Code - Client Startup Process in Distributed Transactions","permalink":"/blog/seata-sourcecode-client-bootstrap"},"nextItem":{"title":"The Refactoring Journey of Seata RPC Module","permalink":"/blog/seata-rpc-refactor"}},"content":"## Seata Demo environment build under Mac (AT mode)\\n\\n## Preface\\n\\nRecently, because of work needs, research and study Seata distributed transaction framework, this article to learn the knowledge of their own record!\\n\\n## Seata overview\\n\\n## cloc code statistics\\n\\nFirst look at the seata project cloc code statistics (as of 2020-07-20)\\n\\n! [cloc-seata](https://github.com/iportman/p/blob/master/blog/seata-at-demo-in-mac/cloc-seata.png?raw=true)\\n\\nThe number of Java code lines is about 97K\\n\\n### Code quality\\n\\nUnit test coverage 50%\\n\\n! [cloc-seata](https://github.com/iportman/p/blob/master/blog/seata-at-demo-in-mac/coverage.png?raw=true)\\n\\n### Demo code\\n\\nThe demo code in this article is the seata-samples-dubbo module under the seata-samples project at the following address:\\n\\nhttps://github.com/apache/incubator-seata-samples/tree/master/dubbo\\n\\n### Core problem solved\\n\\nThe AT pattern Demo example gives a typical distributed transaction scenario:\\n\\n- In a purchase transaction, it is necessary to:\\n\\n1. deduct the inventory of a product\\n2. deduct the user account balance\\n3. generate a purchase order\\n\\n- Obviously, all three steps must either succeed or fail, otherwise the system\'s data will be messed up.\\n- With the popular microservices architecture, generally speaking, inventory, account balance, and purchase order are three separate systems.\\n- Each microservice has its own database and is independent of each other.\\n\\nHere is the scenario for distributed transactions.\\n\\n! [Design diagram](/img/architecture.png)\\n\\n## Solution\\n\\nThe idea of the AT pattern to solve this problem is actually quite simple and is summarised in one sentence:\\n\\nIn the distributed transaction process, record the data to be modified before and after the modification of the value to the undo_log table, in case of abnormalities in the transaction, through the data in this to do a rollback!\\n\\nOf course, the specific code to implement, I believe that many details are far from being so simple.\\n\\n## Demo code structure\\n\\nClone the latest code from github.\\n\\n```sh\\ngit clone git@github.com:apache/incubator-seata-samples.git\\n``\\n\\nRead the Demo code structure\\n\\n```sh\\n$ cd seata-samples/dubbo/\\n$ tree -C -I \'target\' .\\n.\\n\u251c\u2500\u2500 README.md\\n\u251c\u2500 pom.xml\\n\u251c\u2500\u2500 seata-samples-dubbo.iml\\n\u2514\u2500\u2500 src\\n\u2514\u2500\u2500 main\\n\u251c\u2500 java\\n\u2502 \u2514\u2500\u2500 io\\n\u2502 \u2514\u2500\u2500 seata\\n\u2502 \u2514\u2500\u2500 samples\\n\u2502 \u2514\u2500 dubbo\\n\u2502 \u251c\u2500\u2500 ApplicationKeeper.java\\n\u2502 \u251c\u2500\u2500 Order.java\\n\u2502 \u251c\u2500\u2500 service\\n\u2502 \u2502 \u251c\u2500\u2500 AccountService.java\\n\u2502 \u2502 \u251c\u2500\u2500 BusinessService.java\\n\u2502 \u251c\u2500\u2500 OrderService.java \u2502 \u251c\u2500\u2500 OrderService.java\\n\u2502 \u2502 \u251c\u2500\u2500 StorageService.java\\n\u2502 \u2502 \u2514\u2500\u2500 impl\\n\u2502 \u2502 \u251c\u2500\u2500 AccountServiceImpl.java\\n\u2502 \u2502 \u251c\u2500\u2500 BusinessServiceImpl.java\\n\u2502 \u2502 \u251c\u2500\u2500 OrderServiceImpl.java\\n\u2502 \u2502 \u2514\u2500\u2500 StorageServiceImpl.java\\n\u2502 \u2514\u2500\u2500 starter\\n\u2502 \u251c\u2500\u2500 DubboAccountServiceStarter.java \u2502 \u251c\u2500\u2500 DubboAccountServiceStarter.java\\n\u2502 \u251c\u2500\u2500 DubboBusinessTester.java\\n\u2502 \u251c\u2500\u2500 DubboOrderServiceStarter.java\\n\u2502 \u2514\u2500\u2500 DubboStorageServiceStarter.java\\n\u2514\u2500\u2500 resources\\n\u251c\u2500\u2500 file.conf\\n\u251c\u2500\u2500 jdbc.properties\\n\u251c\u2500\u2500 log4j.properties\\n\u251c\u2500\u2500 registry.conf\\n\u251c\u2500 spring\\n\u2502 \u251c\u2500\u2500 dubbo-account-service.xml\\n\u2502 \u251c\u2500\u2500 dubbo-business.xml\\n\u2502 \u251c\u2500\u2500 dubbo-order-service.xml\\n\u2502 \u2514\u2500\u2500 dubbo-storage-service.xml\\n\u2514\u2500\u2500 sql\\n\u251c\u2500\u2500 dubbo_biz.sql\\n\u2514\u2500\u2500 undo_log.sql\\n\\n13 directories, 27 files\\n```\\n\\n- The four \\\\*Starter classes under the io.seata.samples.dubbo.starter package emulate each of the four microservices described above\\n- Account\\n- Business\\n- Order\\n- Storage\\n\\n- 4 services are standard dubbo services, configuration files in the seata-samples/dubbo/src/main/resources/spring directory\\n- To run the demo, you need to start all four services, and Business is the last one to start.\\n- The main logic is in io.seata.samples.dubbo.service, and the four implementation classes correspond to the business logic of the four microservices.\\n- Configuration file for database information: src/main/resources/jdbc.properties\\n\\n### Timing diagram\\n\\n! [cloc-seata](https://github.com/iportman/p/blob/master/blog/seata-at-demo-in-mac/timing-diagram.png?raw=true)\\n\\nOk, get going, Make It Happen!\\n\\n## Run the demo\\n\\n### MySQL\\n\\n### Create a table\\n\\nExecute the scripts dubbo_biz.sql and undo_log.sql in seata-samples/dubbo/src/main/resources/sql.\\n\\n```sh\\nmysql> show tables;\\n+-----------------+\\n| Tables_in_seata |\\n+-----------------+\\n| account_tbl |\\n| order_tbl |\\n| storage_tbl |\\n| undo_log |\\n+-----------------+\\n4 rows in set (0.01 sec)\\n```\\n\\nAfter execution, there should be 4 tables in the database\\n\\n\\nModify the seata-samples/dubbo/src/main/resources/jdbc.properties file\\n\\nModify the values of the variables according to the environment in which you are running MySQL\\n\\n```properties\\njdbc.account.url=jdbc:mysql://localhost:3306/seata\\njdbc.account.username=your_username\\njdbc.account.password=your_password\\njdbc.account.driver=com.mysql.jdbc.\\n# storage db config\\njdbc.storage.url=jdbc:mysql://localhost:3306/seata\\njdbc.storage.username=your_username\\njdbc.storage.password=your_password\\njdbc.storage.driver=com.mysql.jdbc.\\n# order db config\\njdbc.order.url=jdbc:mysql://localhost:3306/seata\\njdbc.order.username=your_username\\njdbc.order.password=your_password\\njdbc.order.driver=com.mysql.jdbc.\\n```\\n\\n### ZooKeeper\\n\\nStart ZooKeeper, my local Mac is using Homebrew installation to start it\\n\\n```sh\\n$ brew services start zookeeper\\n==> Successfully started `zookeeper` (label: homebrew.\\n\\n$ brew services list\\nName Status User Plist\\ndocker-machine stopped\\nelasticsearch stopped\\nkafka stopped\\nkibana stopped\\nmysql started portman /Users/portman/Librar\\ny/LaunchAgents/homebrew.mxcl.mysql.plist\\nnginx stopped\\npostgresql stopped\\npostgresql stopped\\nzookeeper started portman /Users/portman/Librar\\ny/LaunchAgents/homebrew.mxcl.zookeeper.plist\\n```\\n\\n### Start the TC transaction coordinator\\n\\nIn this [link](https://github.com/apache/incubator-seata/releases) page, download the corresponding version of seata-server, I downloaded version 1.2.0 locally.\\n\\n1. Go to the directory where the file is located and extract the file.\\n2. Enter the seata directory\\n3. Execute the startup script\\n\\n```sh\\n$ tar -zxvf seata-server-1.2.0.tar.gz\\n$ cd seata\\n$ bin/seata-server.sh\\n```\\n\\nObserve the startup log for error messages, if everything is fine and you see the following Server started message, the startup was successful.\\n\\n```sh\\n2020-07-23 13:45:13.810 INFO [main]io.seata.core.rpc.netty.RpcServerBootstrap.start:155 -Server started ...\\n```\\n\\n### Starting a simulated microservice in the IDE\\n\\n1. First import the seata-samples project into your local IDE, I\'m using IntelliJ IDEA here.\\n2. Refresh the Maven project dependencies.\\n3. Start the Account, Order and Storage services before Business can invoke them, the corresponding startup classes are:\\n\\nThe corresponding startup classes are:\\n```java\\nio.seata.samples.dubbo.starter.DubboStorageServiceStarter\\nio.seata.samples.dubbo.starter.DubboOrderServiceStarter\\nio.seata.samples.dubbo.starter.DubboStorageServiceStarter\\n```\\n\\nAfter each service is started, you see this message indicating that the service was started successfully\\n\\n```sh\\nApplication is keep running ...\\n```\\n\\n! [cloc-seata](https://github.com/iportman/p/blob/master/blog/seata-at-demo-in-mac/service-boot.png?raw=true)\\n\\nAfter successful startup, the account_tbl, storage_tbl tables will have two initialised data, the account balance and the product inventory respectively\\n\\n```sh\\nmysql> SELECT * FROM account_tbl; SELECT * FROM storage_tbl;\\n+----+---------+-------+\\n| id | user_id | money |\\n+----+---------+-------+ | id | user_id | money | ----+---------+-------+\\n| 1 | U100001 | 999 |\\n+----+---------+-------+ | 1 row in set (0.00.00)\\n1 row in set (0.00 sec)\\n\\n+----+----------------+-------+\\n| id | commodity_code | count |\\n+----+----------------+-------+ | id | commodity_code | count | ----+----------------+-------+\\n| 1 | C00321 | 100 |\\n+----+----------------+-------+\\n1 row in set (0.00 sec)\\n```\\n\\n### Use Business to verify results\\n\\n#### Normal\\n\\nStill executing the main function of the DubboBusinessTester class in the IDE, the programme will exit automatically after running.\\n\\nIf everything is working properly, everything should be committed for each microservice, and the data should be consistent.\\n\\nLet\'s take a look at the data changes in MySQL\\n\\n```sh\\nmysql> SELECT * FROM account_tbl; SELECT * FROM order_tbl; SELECT * FROM storage_tbl.\\n+----+---------+-------+\\n| id | user_id | money |\\n+----+---------+-------+ | id | user_id | money | ----+---------+-------+\\n| 1 | U100001 | 599 |\\n+----+---------+-------+ | 1 row in set (0.00.00)\\n1 row in set (0.00 sec)\\n\\n+----+---------+----------------+-------+-------+\\n| id | user_id | commodity_code | count | money |\\n+----+---------+----------------+-------+-------+\\n| 1 | U100001 | C00321 | 2 | 400 |\\n+----+---------+----------------+-------+-------+\\n1 row in set (0.00 sec)\\n\\n+----+----------------+-------+\\n| id | commodity_code | count |\\n+----+----------------+-------+ | id | commodity_code | count | ----+----------------+-------+\\n| 1 | C00321 | 98 |\\n+----+----------------+-------+\\n1 row in set (0.00 sec)\\n```\\n\\nFrom the data of the 3 tables, we can see: account balance is deducted by 400; the order table is increased by 1 record; the product inventory is deducted by 2\\n\\nThis result is consistent with the logic of the programme, which means that there is no problem with the transaction.\\n\\n#### exception\\n\\nIn fact, even if you do not join the distributed transaction control, everything is normal, the transaction itself will not be a problem\\n\\nSo let\'s focus on what happens when an exception occurs.\\n\\nNow I\'m going to comment out the exception-throwing code in BusinessServiceImpl and execute DubboBusinessTester once more to see what happens.\\n\\n```java\\n\\t\\t@Override\\n    @GlobalTransactional(timeoutMills = 300000, name = \\"dubbo-demo-tx\\")\\n    public void purchase(String userId, String commodityCode, int orderCount) {\\n        LOGGER.info(\\"purchase begin ... xid: \\" + RootContext.getXID());\\n        storageService.deduct(commodityCode, orderCount); orderService.create(userId)\\n        orderService.create(userId, commodityCode, orderCount); // release this exception throw.\\n\\n        //Leave this exception comment alone to simulate an exception in the application.\\n        throw new RuntimeException(\\"portman\'s foooooobar error.\\");;\\n\\n    }\\n```\\n\\nNext, I executed DubboBusinessTester once again, and during the execution I could see the exception message on the console\\n\\n```java\\nException in thread \\"main\\" java.lang.RuntimeException: portman\'s foooooobar error.\\n```\\n\\nNow we look again at the data changes in MySQL and see that there are no changes in the data, indicating that the distributed transaction control has worked\\n\\n## Questions to ponder\\n\\nThe above steps just demonstrates seata\'s simplest demo programme, more complex cases can be discussed and verified later!\\n\\nThere are still some questions and doubts in the learning process, followed by further study\\n\\n- Global lock on the performance of the degree of impact\\n- undo_log log can be rolled back to the original state, but if the data state has changed how to deal with (for example, increased user points have been spent by other local transactions)\\n\\n## References\\n\\n- [What is Seata?] (/docs/overview/what-is-seata)\\n- [Quickstart] (/docs/user/quickstart)\\n\\n## Author information\\n\\nXu Xiaoga, Software Architect, Kingdee\\n\\n[Github](https://github.com/iportman)"},{"id":"/seata-rpc-refactor","metadata":{"permalink":"/blog/seata-rpc-refactor","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-rpc-refactor.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-rpc-refactor.md","title":"The Refactoring Journey of Seata RPC Module","description":"RPC module is where I initially started to study Seata source code, so I have had some deep research on Seata\'s RPC module. After I did some research, I found that the code in the RPC module needs to be optimised to make the code more elegant and the interaction logic more clear and easy to understand, and in line with the original intention of \\"Let the world have no difficult to understand","date":"2020-07-13T00:00:00.000Z","formattedDate":"July 13, 2020","tags":[],"readingTime":8.82,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"The Refactoring Journey of Seata RPC Module","author":"chenghui.zhang","keywords":["Seata","RPC module","refactoring"],"date":"2020/07/13"},"unlisted":false,"prevItem":{"title":"Setting Up Seata Demo Environment on Mac (AT Mode)","permalink":"/blog/seata-at-demo-in-mac"},"nextItem":{"title":"Seata Source Code - Server Startup Process in Distributed Transactions","permalink":"/blog/seata-sourcecode-server-bootstrap"}},"content":"RPC module is where I initially started to study Seata source code, so I have had some deep research on Seata\'s RPC module. After I did some research, I found that the code in the RPC module needs to be optimised to make the code more elegant and the interaction logic more clear and easy to understand, and in line with the original intention of \\"**Let the world have no difficult to understand\\nIn the spirit of \\"**Let there be no difficult RPC communication code**\\", I started the refactoring of the RPC module.\\n\\nHere I suggest that if you want to know more about Seata interaction details, you may want to start from the source code of RPC module, RPC module is equivalent to Seata\'s hub, Seata all the interaction logic in the RPC module to show the most.\\n\\nThis refactoring of the RPC module will make the Seata hub more robust and easier to interpret.\\n\\n# Refactoring Inheritance\\n\\nIn the old version of Seata, the overall structure of the RPC module was a bit confusing, especially in terms of the inheritance relationships between classes:\\n\\n1. directly inheriting Netty Handler in Remoting class, which makes Remoting class coupled with Netty Handler processing logic;\\n2. The inheritance relationship between the Reomting class on the client side and the Reomting class on the server side is not unified;\\n3. RemotingClient is implemented by RpcClientBootstrap, while RemotingServer is implemented by RpcServer without an independent ServerBootstrap, which seems to be a very confusing relationship. 4. Some interfaces are not necessary to be extracted;\\n4. Some interfaces are not necessary to extract, such as ClientMessageSender, ClientMessageListener, ServerMessageSender and so on, because these interfaces will increase the complexity of the overall structure of the inheritance relationship.\\n\\nIn response to the problems identified above, I did the following during the refactoring process:\\n\\n1) Abstract Netty Handler as an inner class and put it in Remoting class. 2) Put RemotingClient as an inner class and put it in RemotingClass;\\n2. put RemotingClient as the top-level client interface, define the basic methods of client-server interaction, abstract a layer of AbstractNettyRemotingClient, and the following are respectively\\n   RmNettyRemotingClient, TmNettyRemotingClient; RemotingServer is the top-level interface of server, defining the basic methods of interaction between the server and the client, and the implementation of NettyRemotingServer;\\n3. At the same time, ClientMessageSender, ClientMessageListener, ServerMessageSender and other interface methods are grouped into RemotingClient, RemotingServer, and implemented by Reomting.\\n   class to implement RemotingClient and RemotingServer and unify the inheritance relationship of Remoting class;\\n4. Create a new RemotingBootstrap interface and implement NettyClientBootstrap and NettyServerBootstrap on the client and server side respectively, so as to extract the bootstrap logic from the Reomting class.\\n\\nThe inheritance relationship in the latest RPC module is simple and clear, represented by the following class relationship diagram:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20200711111637.png)\\n\\n1. AbstractNettyRemoting: the top level abstraction of Remoting class, contains common member variables and common methods for both client and server, has common request methods (we will talk about it later in the article), and Processor processor invocation logic (we will talk about it later in the article);\\n2. RemotingClient: the client\'s top-level interface, defining the basic methods of client-server interaction;\\n3. RemotingServer: the top-level interface of the server side, defining the basic methods of interaction between the server side and the client side;\\n4. AbstractNettyRemotingClient: client-side abstract class, inherits AbstractNettyRemoting class and implements RemotingClient interface;\\n5. NettyRemotingServer: server implementation class, inherits AbstractNettyRemoting class and implements RemotingServer interface;\\n6. RmNettyRemotingClient: Rm client implementation class, inherits AbstractNettyRemotingClient class;\\n7. TmNettyRemotingClient: Tm client implementation class, inherits AbstractNettyRemotingClient class.\\n\\nAt the same time, the client-side and server-side bootstrap class logic is abstracted out, as shown in the following class relationship diagram:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20200510225359.png)\\n\\n1. RemotingBootstrap: bootstrap class interface with two abstract methods: start and stop. 2;\\n2. NettyClientBootstrap: client-side bootstrap implementation class. 3;\\n3. NettyServerBootstrap: server-side bootstrap implementation class.\\n\\n# Decoupled processing logic\\n\\nDecoupled processing logic is the processing logic of RPC interactions from the Netty Handler abstracted out, and processing logic into a Processor abstraction, why do this? I\'m going to talk about some of the problems that exist right now:\\n\\n1. Netty Handler and Processing Logic are blended together, since both client and server share a set of Processing Logic, in order to be compatible with more interactions, in the Processing Logic you can see a lot of difficult to understand judgement logic. 2. in Seata interactions, the Netty Handler is not a Processor;\\n2. In Seata\'s interaction, some requests are processed asynchronously and some requests are processed synchronously, but the expression of synchronous and asynchronous processing in the old processing code logic is very obscure and difficult to understand;\\n3. It is not possible to clearly express the relationship between the type of request message and the corresponding processing logic in the code logic;\\n4. In the later iterations of Seata, it will be very difficult to add new interaction logic to this part of the code if the processing logic is not extracted from it.\\n\\nBefore extracting the processing logic from the Netty Handler, let\'s take a look at Seata\'s existing interaction logic:\\n\\n- RM client-server interaction logic:\\n\\nRM client request server interaction logic: ![](https://gitee.com/objcoding/md-picture/raw/master/img/Xnip2020-05-12_21-41-45.png)\\n\\n- TM client-server interaction logic:\\n\\nRM Client Request Server Interaction Logic: ![](https://gitee.com/objcoding/md-picture/raw/master/img/Xnip2020-05-12_21-44-04.png)\\n\\n- Interaction logic for a server requesting an RM client:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20200513000620.png)\\n\\nThe interaction logic of Seata can be clearly seen in the above interaction diagram.\\n\\nThe client receives messages from the server side in total:\\n\\n1) Server-side request messages\\n\\n1. BranchCommitRequest, BranchRollbackRequest, UndoLogDeleteRequest\\n\\n2) Server-side response messages\\n\\n1. RegisterRMResponse, BranchRegisterResponse, BranchReportResponse, GlobalLockQueryResponse\\n2.\\nRegisterTMResponse, GlobalBeginResponse, GlobalCommitResponse, GlobalRollbackResponse, GlobalStatusResponse, GlobalReportResponse\\n3. HeartbeatMessage(PONG)\\n\\nThe server receives messages from the client in total:\\n\\n1) Client request messages:\\n\\n1. RegisterRMRequest, BranchRegisterRequest, BranchReportRequest, GlobalLockQueryRequest\\n2.\\nRegisterTMRequest, GlobalBeginRequest, GlobalCommitRequest, GlobalRollbackRequest, GlobalStatusRequest, GlobalReportRequest\\n3. HeartbeatMessage(PING)\\n\\n2) Client response message:\\n\\n1. BranchCommitResponse, BranchRollbackResponse\\n\\nBased on the above analysis of the interaction logic, we can abstract the logic of processing messages into a number of Processor, a Processor can handle one or more message types of messages, only in Seata startup registration will be registered to the message type ProcessorTable\\nA Processor can process messages of one or more message types, just register the message types into the ProcessorTable when Seata starts up, forming a mapping relationship, so that the corresponding Processor can be called to process the message according to the message type, as shown in the following diagram:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/Xnip2020-05-12_22-09-17.png)\\n\\nIn the abstract Remoting class, there is a processMessage method, the logic of the method is to get the corresponding Processor from the ProcessorTable according to the message type.\\n\\nIn this way, the processing logic is completely removed from the Netty Handler, and the Handler#channelRead method only needs to call the processMessage method, and it can dynamically register Processors into the ProcessorTable according to the message type.\\nProcessorTable, the scalability of the processing logic has been greatly improved.\\n\\nThe following is the invocation flow of Processor:\\n\\n1) Client\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20200510234047.png)\\n\\n1. RmBranchCommitProcessor: process the server-side global commit request;\\n2. RmBranchRollbackProcessor: process server-side global rollback request;\\n3. RmUndoLogProcessor: handles server-side undo log deletion requests;\\n4. ClientOnResponseProcessor: client-side processing of server-side response requests, such as: BranchRegisterResponse, GlobalBeginResponse, GlobalCommitResponse and so on;\\n5. ClientHeartbeatProcessor: processing server-side heartbeat response.\\n\\n2) Server-side\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20200510234016.png)\\n\\n1. RegRmProcessor: Handle RM client registration request. 2;\\n2. RegTmProcessor: handle TM client registration request;\\n3. ServerOnRequestProcessor: handle client related requests, such as: BranchRegisterRequest, GlobalBeginRequest, GlobalLockQueryRequest, etc. 4;\\n4. ServerOnResponseProcessor: handle client-related responses, such as: BranchCommitResponse, BranchRollbackResponse and so on;\\n5. ServerHeartbeatProcessor: handle client heartbeat response.\\n\\nBelow is an example of a TM initiating a global transaction commit request to give you a sense of where the Processor sits in the entire interaction:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20200514191842.png)\\n\\n# Refactoring the request method\\n\\nIn older versions of Seata, the request methods for RPC also lacked elegance:\\n\\n1. request methods are too cluttered and not hierarchical;\\n2. sendAsyncRequest method is coupled with too much code, the logic is too confusing, the client and server both share a set of request logic, the method to decide whether to send bulk is based on the parameter address is null or not to decide, to decide whether to synchronise the request is based on whether the timeout is greater than 0, it is extremely unreasonable, and it is not reasonable.\\n   The method to decide whether to send bulk is based on whether the address is null, and to decide whether to make a synchronous request is based on whether the timeout is greater than 0, which is extremely unreasonable;\\n3. request method name style is not uniform, for example, the client sendMsgWithResponse, but the server is called sendSyncRequest;\\n\\nTo address the above shortcomings of the old RPC request methods, I have made the following changes. 1:\\n\\n1. put the request method into the RemotingClient and RemotingServer interfaces as the top-level interface. 2. separate the client-side and server-side request methods;\\n2. Separate the client-side and server-side request logic, and separate the batch request logic into the client-side request method, so that the decision of whether or not to send a batch of requests is no longer based on whether or not the parameter address is null;\\n3. due to Seata\'s\\n   Due to Seata\'s own logic characteristics, the parameters of client-server request methods cannot be unified, so we can extract common synchronous/asynchronous request methods, the client and server implement their own synchronous/asynchronous request logic according to their own request logic characteristics, and then finally call the common synchronous/asynchronous request methods, so that synchronous/asynchronous requests have a clear method, and are no longer decided according to whether or not the\\n   timeout is greater than 0. 4;\\n4. Unify the request name style.\\n\\nFinally, Seata RPC request methods look more elegant and hierarchical.\\n\\nSynchronous requests:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20200513103838.png)\\n\\nAsynchronous request:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20200513103904.png)\\n\\n# Other\\n\\n1. Class Catalogue Adjustment: There is also a netty catalogue in the RPC module catalogue, and it can be found from the catalogue structure that Seata\'s original intention is to be compatible with multiple RPC frameworks, and only netty is implemented at present, but it is found that some of the classes in the netty module are not \\"netty\\" and the classes in the RPC\\n   classes in the netty module are not \\"netty\\", and the RPC classes in the catalogue are not common, so the location of the relevant classes needs to be adjusted;\\n2. some classes are renamed, e.g. netty related classes contain \\"netty\\";\\n\\nThe final RPC module looks like this:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20200711213204.png)\\n\\n# Author Bio\\n\\nZhang Chenghui, currently working in Ant Group, loves to share technology, author of WeChat public number \\"Backend Advanced\\", technical blog ([https://objcoding.com/](https://objcoding.com/)) blogger, Seata Contributor, GitHub\\nID: objcoding."},{"id":"/seata-sourcecode-server-bootstrap","metadata":{"permalink":"/blog/seata-sourcecode-server-bootstrap","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-sourcecode-server-bootstrap.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-sourcecode-server-bootstrap.md","title":"Seata Source Code - Server Startup Process in Distributed Transactions","description":"[Distributed Transaction Seata Source Code Interpretation I] Server-side startup process","date":"2020-07-02T00:00:00.000Z","formattedDate":"July 2, 2020","tags":[],"readingTime":17.195,"hasTruncateMarker":false,"authors":[{"name":"xiaobing.yang"}],"frontMatter":{"title":"Seata Source Code - Server Startup Process in Distributed Transactions","author":"xiaobing.yang","date":"2020/07/02","keywords":["fescar","seata","distributed transaction"]},"unlisted":false,"prevItem":{"title":"The Refactoring Journey of Seata RPC Module","permalink":"/blog/seata-rpc-refactor"},"nextItem":{"title":"How is distributed transaction realized? In-depth interpretation of Seata\'s XA mode","permalink":"/blog/seata-xa-introduce"}},"content":"## [Distributed Transaction Seata Source Code Interpretation I] Server-side startup process\\n\\n### Core points for implementing distributed transactions:\\n1. transaction persistence, the various states of the transaction at the various state of the transaction participants need to be persistent, when the instance is down in order to roll back or commit the transaction based on the persistent data to achieve the ultimate consistency\\n2. Timing on the timeout transaction processing (continue to try to commit or rollback), that is, through the retry mechanism to achieve the ultimate consistency of the transaction\\n3. cross-service instance propagation of distributed transactions, when distributed transactions across multiple instances need to achieve transaction propagation, generally need to adapt to different rpc frameworks.\\n4. transaction isolation level: most distributed transactions for performance, the default isolation level is read uncommitted\\n5. idempotency: for XA or seata\'s AT such distributed transactions, have been implemented by default idempotency, and TCC, Saga interface level implementation of distributed transactions are still required to implement their own business developers to achieve idempotency.\\n\\nThis article introduces the source code of seata-server from the point of view of the startup process of seata-server, the startup flow chart is as follows:\\n\\n! [Insert image description here](https://img-blog.csdnimg.cn/20200726213919467.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10, text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTE0NTg0OA==,size_16,colour_FFFFFF,t_70)\\n\\n\\n#### 1. Startup class Server\\nThe entry class for seata-server is in the Server class with the following source code:\\n```java\\npublic static void main(String[] args) throws IOException {\\n// Get the listening port from an environment variable or runtime parameter, default port 8091\\nint port = PortHelper.getPort(args);\\n\\n    // Set the listening port to SystemProperty, Logback\'s LoggerContextListener implementation class.\\n    // SystemPropertyLoggerContextListener writes the Port to Logback\'s Context.\\n    // The Port variable will be used in the logback.xml file to construct the log file name.\\n    System.setProperty(ConfigurationKeys.SERVER_PORT, Integer.toString(port));; // Create LoggerContextListener.\\n\\n    // Create the Logger\\n    final Logger logger = LoggerFactory.getLogger(Server.class);\\n    if (ContainerHelper.isRunningInContainer()) {\\n        logger.info(\\"The server is running in container.\\"); }\\n    }\\n\\n    // Parsing various configuration parameters for startup and configuration files\\n    ParameterParser parameterParser = new ParameterParser(args); // metrics related, here is the metrics.\\n\\n    // metrics related, here is the SPI mechanism to get the Registry instance object\\n    MetricsManager.get().init(); // read the metrics from the configuration file.\\n\\n\\t// Write the storeMode read from the config file into SystemProperty for use by other classes.\\n    System.setProperty(ConfigurationKeys.STORE_MODE, parameterParser.getStoreMode());\\n\\n\\t// Create an instance of NettyRemotingServer, an rpc framework based on the Netty implementation.\\n\\t// Not initialised at this point, NettyRemotingServer is responsible for network communication with the TM and RM in the client SDK.\\n     nettyRemotingServer = new NettyRemotingServer(WORKING_THREADS);\\n\\n    // Set the listening port\\n    nettyRemotingServer.setListenPort(parameterParser.getPort()); // Set the port to listen to.\\n\\n\\t// Initialise UUIDGenerator, which is implemented based on the snowflake algorithm.\\n\\t// Used to generate ids for global transactions, branch transactions.\\n\\t// Multiple Server instances are configured with different ServerNodes to ensure uniqueness of the ids\\n    UUIDGenerator.init(parameterParser.getServerNode());; // The UUIDGenerator.init(parameterParser.getServerNode()).\\n\\n\\t// SessionHodler is responsible for persistent storage of transaction logs (state).\\n\\t// Currently supports three storage modes: file, db, and redis; for cluster deployment mode, use db or redis mode\\n    SessionHolder.init(parameterParser.getStoreMode()); // Create the initialisation DefaultCoher.\\n\\n  \\t// Create an instance of DefaultCoordinator, the core transaction logic processing class of TC.\\n  \\tDefaultCoordinator is the core transaction logic handling class of TC, // containing logic handling for different transaction types such as AT, TCC, SAGA, etc. at the bottom.\\n    DefaultCoordinator coordinator = new DefaultCoordinator(nettyRemotingServer);\\n    coordinator.init();\\n    nettyRemotingServer.setHandler(coordinator); // register ShutdownHook.\\n    // register ShutdownHook\\n    ShutdownHook.getInstance().addDisposable(coordinator); // register ShutdownHook.\\n    ShutdownHook.getInstance().addDisposable(nettyRemotingServer);; // 127.0.0.1\\n\\n    // 127.0.0.1 and 0.0.0.0 are not valid here.\\n    if (NetUtil.isValidIp(parameterParser.getHost(), false)) {\\n        XID.setIpAddress(parameterParser.getHost());\\n    } else {\\n        XID.setIpAddress(NetUtil.getLocalIp());\\n    }\\n    XID.setPort(nettyRemotingServer.getListenPort()); }\\n\\n    try {\\n        // Initialise Netty, start listening on the port and block here, waiting for the application to shut down.\\n        nettyRemotingServer.init(); } catch (Throwable e); }\\n    } catch (Throwable e) {\\n        logger.error(\\"nettyServer init error:{}\\", e.getMessage(), e);\\n        System.exit(-1); }\\n    }\\n\\n    System.exit(0);\\n}\\n```\\n\\n#### 2. Parsing Configuration\\nThe implementation code for parameter parsing is in the ParameterParser class, the init method source code is as follows:\\n```java\\nprivate void init(String[] args) {\\n   try {\\n   \\t   // Determine if you are running in a container, and if you are, get the configuration from the environment variable.\\n       if (ContainerHelper.isRunningInContainer()) {\\n           this.seataEnv = ContainerHelper.getEnv();\\n           this.host = ContainerHelper.getHost();\\n           this.port = ContainerHelper.getPort();\\n           this.serverNode = ContainerHelper.getServerNode(); this.storeMode = ContainerHelper.getServerNode()\\n           this.storeMode = ContainerHelper.getStoreMode();\\n       } else {\\n           // Based on JCommander\'s ability to get the parameters configured when starting the application.\\n           // JCommander assigns the parameters to the fields of the current class via annotations and reflection.\\n           JCommander jCommander = JCommander.newBuilder().addObject(this).build();\\n           JCommander.parse(args);\\n           if (help) {\\n               jCommander.setProgramName(PROGRAM_NAME);\\n               jCommander.usage();\\n               System.exit(0);\\n           }\\n       }\\n       // serverNode is used as a unique identifier for instances in snowflake maths and needs to be guaranteed unique.\\n       // If you don\'t specify a randomly generated one based on the current server\'s I\\n       if (this.serverNode == null) {\\n           this.serverNode = IdWorker.initWorkerId();\\n       }\\n       if (StringUtils.isNotBlank(seataEnv)) {\\n           System.setProperty(ENV_PROPERTY_KEY, seataEnv);\\n       }\\n       if (StringUtils.isBlank(storeMode)) {\\n           // There is an important Configuration class involved here, ParameterParser is only responsible for getting the core parameters such as ip, port and storeMode.\\n           // All other parameters are taken from the Configuration. Here, if there is no startup parameter that doesn\'t specify a storeMode, // it\'s taken from Configuration.\\n           // is taken from the Configuration class.\\n           storeMode = ConfigurationFactory.getInstance().getConfig(ConfigurationKeys.STORE_MODE, // SERVER_DEFAULT, SERVER_DEFAULT, SERVER_DEFAULT))\\n               SERVER_DEFAULT_STORE_MODE);\\n       }\\n   } catch (ParameterException e) {\\n       printError(e);\\n   }\\n\\n}\\n```\\n\\nThe first call to ConfigurationFactory.getInstance() in the ParameterParser\'s init method initialises a singleton Configuration object, which is responsible for initialising all other configuration parameter information. From the Seata Server side of the source code we can see two configuration files file.conf, registry.conf. So what is the difference between these two configuration files, both files are required? We continue to look at the code.\\n\\nConfigurationFactory.getInstance method is actually to get a singleton object, the core is in the buildConfiguration method, but before the buidlConfiguration method, there is a static code block of the ConfigurationFactory class will be executed first.\\n```Java\\n// Get the singleton object for Configuration.\\npublic static Configuration getInstance() {\\n  if (instance == null) {\\nsynchronized (Configuration.class) {\\n  if (instance == null) {\\n  instance = buildConfiguration();\\n  }\\n  }\\n  }\\n  return instance;\\n}\\n\\n// ConfigurationFactory static code block\\nstatic\\n// Get the name of the configuration file, defaults to registry.conf.\\nString seataConfigName = System.getProperty(SYSTEM_PROPERTY_SEATA_CONFIG_NAME);\\nIf (seataConfigName == null) {\\nseataConfigName = System.getenv(ENV_SEATA_CONFIG_NAME);\\n}\\nif (seataConfigName == null) {\\nseataConfigName = REGISTRY_CONF_PREFIX;\\n}\\nString envValue = System.getProperty(ENV_PROPERTY_KEY);\\nIf (envValue == null) {\\nenvValue = System.getenv(ENV_SYSTEM_KEY);\\n}\\n\\n    // Read the configuration from the registry.Conf file to build the base configuration object\\n    Configuration configuration = (envValue == null) ? new FileConfiguration(seataConfigName + REGISTRY_CONF_SUFFIX,\\n        false) : new FileConfiguration(seataConfigName + \\"-\\" + envValue + REGISTRY_CONF_SUFFIX, false);\\n    Configuration extConfiguration = null;\\n    try {\\n        // ExtConfigurationProvider currently has only one SpringBootConfigurationProvider implementation class\\n        // Used to support the client-side SDK SpringBoot\'s configuration file approach, this logic can be ignored for the Server side.\\n        extConfiguration = EnhancedServiceLoader.load(ExtConfigurationProvider.class).provide(configuration);\\n        if (LOGGER.isInfoEnabled()) {\\n            LOGGER.info(\\"load Configuration:{}\\", extConfiguration == null ?\\n                : extConfiguration.getClass().getSimpleName());\\n        }\\n    } catch (EnhancedServiceNotFoundException ignore) {\\n\\n    } catch (Exception e) {\\n        LOGGER.error(\\"failed to load extConfiguration:{}\\", e.getMessage(), e);\\n    }\\n    CURRENT_FILE_INSTANCE = extConfiguration == null ? Configuration : extConfiguration;\\n}\\n```\\n\\nThe static block in ConfigurationFactory reads configuration information from registry.conf. The conf configuration file is mandatory, the registry.conf configuration file specifies the source for other detailed configurations, the current configuration source supports file, zk, apollo, nacos, etcd3, etc. So file.conf is not required, only when the configuration source is set to the file type will read the contents of the file.conf file.\\n\\nNext buildConfiguration in ConfigurationFactory is to load more configuration items based on the configuration source set in registry.conf.\\n\\n```Java\\nprivate static Configuration buildConfiguration() {\\n    ConfigType configType;\\n    String configTypeName;\\n    try {\\n    \\t// Read the config.type field from the registry.conf configuration file and parse it into the ConfigType enumeration.\\n        configTypeName = CURRENT_FILE_INSTANCE.getConfig(\\n            ConfigurationKeys.FILE_ROOT_CONFIG + ConfigurationKeys.FILE_CONFIG_SPLIT_CHAR\\n                + ConfigurationKeys.FILE_ROOT_TYPE);\\n\\n        if (StringUtils.isBlank(configTypeName)) {\\n            throw new NotSupportYetException(\\"Configuration type cannot be blank\\");\\n        }\\n\\n        configType = ConfigType.getType(configTypeName);\\n    } catch (Exception e) {\\n        throw e;\\n    }\\n    Configuration extConfiguration = null;\\n    Configuration Configuration;\\n    If (ConfigType.File == configType) {\\n    \\t// If the configuration file is of type file, read the config.file.name configuration entry from registry.conf, // i.e. the path to the config file of type file, example config.file.name configuration entry.\\n    \\t// i.e. the path to the file type configuration file, default is file.conf in the example.\\n        String pathDataId = String.join(ConfigurationKeys.FILE_CONFIG_SPLIT_CHAR.File),\\n            ConfigurationKeys.FILE_ROOT_CONFIG, FILE_TYPE, NAME_KEY);\\n        String name = CURRENT_FILE_INSTANCE.getConfig(pathDataId);\\n\\n        // Build the FileConfiguration object based on the path to the file configuration file\\n        Configuration file = new FileConfiguration(name);\\n        try {\\n        \\t// Additional extensions to the configuration, also available only to the client SpringBoot SDK.\\n            extConfiguration = EnhancedServiceLoader.load(ExtConfigurationProvider.class).provide(configuration);\\n            if (LOGGER.isInfoEnabled()) {\\n                LOGGER.info(\\"load Configuration:{}\\", extConfiguration == null\\n                    ? configuration.getClass().getSimpleName() : extConfiguration.getClass().getSimpleName());\\n            }\\n        } catch (EnhancedServiceNotFoundException ignore) {\\n\\n        } catch (Exception e) {\\n            LOGGER.error(\\"failed to load extConfiguration:{}\\", e.getMessage(), e);\\n        }\\n    } else {\\n    \\t// If the configuration file is of a type other than file, e.g. nacos, zk, etc., // then generate it via SPI.\\n    \\t// then generate the corresponding ConfigurationProvider object by way of SPI\\n        ConfigurationProvider = EnhancedServiceLoader\\n            .load(ConfigurationProvider.class, Objects.requireNonNull(configurationType).name()).provide();\\n    }\\n    try {\\n    \\t// ConfigurationCache is a one-time proxy memory cache of the configuration to improve the performance of fetching the configuration.\\n        ConfigurationCache;\\n        if (null ! = extConfiguration) {\\n            configurationCache = ConfigurationCache.getInstance().proxy(extConfiguration);\\n        } else {\\n            configurationCache = ConfigurationCache.getInstance().proxy(configuration);\\n        }\\n        If (null ! = configurationCache) {\\n            extConfiguration = configurationCache;\\n        }\\n    } catch (EnhancedServiceNotFoundException ignore) {\\n\\n    } catch (Exception e) {\\n        LOGGER.error(\\"failed to load configurationCacheProvider:{}\\", e.getMessage(), e);\\n    }\\n    return null == extConfiguration ? configuration : extConfiguration;\\n}\\n```\\n\\n#### 3. Initialisation of UUIDGenerator\\nThe UUIDGenertor initialisation receives a serverNode parameter, the UUIDGenertor currently uses the snowflake algorithm to generate the unique Id, this serverNode is used to ensure that the unique ids generated by multiple seata-server instances are not duplicated.\\n```java\\npublic class UUIDGenerator {\\n\\n    /**\\n     * Generate uuid long.\\n     *\\n     * @return the long\\n     */\\n    public static long generateUUID() {\\n        return IdWorker.getInstance().nextId();\\n    }\\n\\n    /**\\n     * Init.\\n     * * @param serverNode the server node id.\\n     * @param serverNode the server node id\\n     */\\n    public static void init(Long serverNode) {\\n        IdWorker.init(serverNode); }\\n    }\\n}\\n```\\n\\nUUIDGenerator is a wrapper around IdWorker, the core implementation logic for the unique id is in the IdWoker class, and IdWorker is a snowflake algorithm implementation. The IdWorker in this case is again a single instance\\n```java\\npublic class IdWorker\\n/**\\n* Constructor\\n* @param workerId is the ServerNode mentioned above, in the range of\\n* @param workerId is the ServerNode mentioned above, with a value in the range of 0-1023, i.e., 10 digits in the 64-bit UUID.\\n*/\\npublic IdWorker(long workerId) {\\nif (workerId > maxWorkerId || workerId < 0) {\\nthrow new IllegalArgumentException(\\nString.format(\\"Worker Id can\'t be greater than %d or less than 0\\", maxWorkerId));\\n}\\nthis.workerId = workerId;\\n}\\n\\n    /**\\n     * Get the next ID (the method is thread-safe)\\n     } /** * Get the next ID (the method is thread-safe).\\n     * @return SnowflakeId\\n     */\\n    public long nextId() {\\n        public long nextId() { long timestamp = timeGen(); if (timestamp < lastTimestamp) {\\n\\n        if (timestamp < lastTimestamp) {\\n            throw new RuntimeException(String.format(\\n                \\"clock moved backwards. Refusing to generate id for %d milliseconds\\", lastTimestamp - timestamp)); }\\n        }\\n\\n        synchronized (this) {\\n            if (lastTimestamp == timestamp) {\\n                sequence = (sequence + 1) & sequenceMask; if (sequence == 0) { sequence == 0)\\n                if (sequence == 0) {\\n                    timestamp = tilNextMillis(lastTimestamp);\\n                }\\n            } else {\\n                sequence = 0L; }\\n            }\\n            lastTimestamp = timestamp; } else { sequence = 0L; }\\n        }\\n        // Snowflake algorithm 64-bit unique id composition: first 0 + 41-bit timestamp + 10-bit workerId + 12-bit incremental serialisation (self-incrementing within the same timestamp)\\n        return ((timestamp - twepoch) << timestampLeftShift) | (workerId << workerIdShift) | sequence;\\n    }\\n```\\n\\n#### 4. SessionHolder initialisation\\nSessionHolder is responsible for session persistence, a session object corresponds to a transaction, there are two kinds of transaction: GlobalSession and BranchSession.\\nSessionHolder supports two types of persistence: file and db, of which db supports cluster mode and is recommended to use db. The four most important fields in SessionHolder are as follows:\\n```java\\n// ROOT_SESSION_MANAGER is used to get all the Setssion, as well as Session creation, update, deletion, and so on.\\nprivate static SessionManager ROOT_SESSION_MANAGER;\\n// Used to get and update all asynchronous commits.\\nprivate static SessionManager ASYNC_COMMITTING_SESSION_MANAGER; // Used to get and update all sessions that need to be commited asynchronously.\\n// Get and update all sessions that need to retry commits.\\nprivate static SessionManager RETRY_COMMITTING_SESSION_MANAGER; // Used to fetch and update all sessions that need to retry commits.\\n// Used to retrieve and update all sessions that need to retry a rollback.\\nprivate static SessionManager RETRY_ROLLBACKING_SESSION_MANAGER; // for getting and updating all sessions that need to retry rollbacks.\\n```\\n\\nSessionHolder init method\\n```java\\nprivate static SessionManager RETRY_ROLLBACKING_SESSION_MANAGER\\npublic static void init(String mode) throws IOException {\\nif (StringUtils.isBlank(mode)) {\\nmode = CONFIG.getConfig(ConfigurationKeys.STORE_MODE);\\n}\\nStoreMode storeMode = StoreMode.get(mode);\\nif (StoreMode.DB.equals(storeMode)) {\\n// The SPI method of loading the SessionManager is used here again.\\n// In fact, the four SessionManager instances obtained below are all different instances of the same class, DataBaseSessionManager.\\nThe four instances of SessionManager are all different instances of the same class DataBaseSessionManager, // just passing different parameters to the DataBaseSessionManager constructor.\\nROOT_SESSION_MANAGER = EnhancedServiceLoader.load(SessionManager.class, StoreMode.DB.getName());\\nASYNC_COMMITTING_SESSION_MANAGER = EnhancedServiceLoader.load(SessionManager.class, StoreMode.DB.getName(),\\nnew Object[] {ASYNC_COMMITTING_SESSION_MANAGER_NAME});\\nRETRY_COMMITTING_SESSION_MANAGER = EnhancedServiceLoader.load(SessionManager.class, StoreMode.DB.getName(),\\nnew Object[] {RETRY_COMMITTING_SESSION_MANAGER_NAME});\\nRETRY_ROLLBACKING_SESSION_MANAGER = EnhancedServiceLoader.load(SessionManager.class, StoreMode.DB.getName(),\\nnew Object[] {RETRY_ROLLBACKING_SESSION_MANAGER_NAME}); } else if (StoreMode.DB.getName()); }\\n} else if (StoreMode.FILE.equals(storeMode)) {\\n//file mode can be left alone for now\\n...\\n} else {\\nthrow new IllegalArgumentException(\\"unknown store mode:\\" + mode);\\n}\\n// The reload method can be ignored for db mode\\nreload(); }\\n}\\n```\\n\\nThe four SessionManagers in the SessionHolder are all instances of the class DataBaseSessionManager, but they pass different parameters to the constructor, so take a look at the definition of DataBaseSessionManager:\\n```java\\npublic DataBaseSessionManager(String name) {\\n\\tsuper();\\n\\tthis.taskName = name.\\n}\\n\\n// Determine the list of transactions returned by allSessions based on the instance\'s taskName, // if taskName equals ASYNC_COMMITMENT.\\n// If taskName is equal to ASYNC_COMMITTING_SESSION_MANAGER_NAME, then all transactions with status Async_COMMITTING_SESSION_MANAGER_NAME are returned.\\n// All transactions with a status of AsyncCommitting are returned.\\npublic Collection<GlobalSession> allSessions() {\\n\\t// get by taskName\\n\\tif (SessionHolder.ASYNC_COMMITTING_SESSION_MANAGER_NAME.equalsIgnoreCase(taskName)) {\\n\\t\\t\\treturn findGlobalSessions(new SessionCondition(GlobalStatus.AsyncCommitting));\\n\\t} else if (SessionHolder.RETRY_COMMITTING_SESSION_MANAGER_NAME.equalsIgnoreCase(taskName)) {\\n\\t\\t\\treturn findGlobalSessions(new SessionCondition(new GlobalStatus[] {GlobalStatus.CommitRetrying}));\\n\\t} else if (SessionHolder.RETRY_ROLLBACKING_SESSION_MANAGER_NAME.equalsIgnoreCase(taskName)) {\\n\\t\\t\\treturn findGlobalSessions(new SessionCondition(new GlobalStatus[] {GlobalStatus.RollbackRetrying,\\n\\t\\t\\t\\t\\tGlobalStatus.Rollbacking, GlobalStatus.TimeoutRollbacking, GlobalStatus.TimeoutRollbackRetrying}));\\n\\t} else {\\n\\t\\t// A taskName of null corresponds to ROOT_SESSION_MANAGER.\\n\\t\\treturn findGlobalSessions(new SessionCondition(new GlobalStatus[] {\\n\\t\\t\\t\\tGlobalStatus.UnKnown, GlobalStatus.Begin,\\n\\t\\t\\t\\tGlobalStatus.Committing, GlobalStatus.CommitRetrying, GlobalStatus.Rollbacking,\\n\\t\\t\\t\\tGlobalStatus.RollbackRetrying,\\n\\t\\t\\t\\tGlobalStatus.TimeoutRollbacking,\\n\\t\\t\\t\\tGlobalStatus.TimeoutRollbackRetrying,\\n\\t\\t\\t\\tGlobalStatus.AsyncCommitting}));\\n\\t}\\n}\\n```\\n#### 5. Initialise DefaultCoordinator\\nThe DefaultCoordinator is the core of the transaction coordinator, e.g., opening, committing, and rolling back global transactions, registering, committing, and rolling back branch transactions are all coordinated by the DefaultCoordinator.The DefaultCoordinato communicate with remote TMs and RMs through the RpcServer to achieve branch transactions such as commit and rollback. DefaultCoordinato communicates with remote TMs and RMs through the RpcServer to achieve branch transactions.\\n```java\\npublic DefaultCoordinator(ServerMessageSender messageSender) {\\n\\t// The implementation class for the messageSender interface is the RpcServer mentioned above.\\n\\tthis.messageSender = messageSender; // The interface messageSender is implemented in the RpcServer class mentioned above.\\n\\n\\t// DefaultCore encapsulates the implementation classes for AT, TCC, Saga, and other distributed transaction patterns.\\n\\tthis.core = new DefaultCore(messageSender); }\\n}\\n\\n// The init method initialises five timers, which are mainly used for the retry mechanism of distributed transactions.\\n// Because the instability of distributed environments can cause transactions to be in an intermediate state.\\n// because the instability of a distributed environment can cause transactions to be in an intermediate state, // so the ultimate consistency of a transaction is achieved through a constant retry mechanism.\\n// The following timers, except for undoLogDelete, are executed once every 1 second by default.\\npublic void init() {\\n    // Handling transactions that are in a rollback state that can be retried\\n\\tretryRollbacking.scheduleAtFixedRate(() -> {\\n\\t\\thandleRetryRollbacking.scheduleAtFixedRate(() -> {\\n\\t\\t\\thandleRetryRollbacking(); }\\n\\t\\t} catch (Exception e) {\\n\\t\\t\\tLOGGER.info(\\"Exception retry rollbacking ... \\", e);\\n\\t\\t}\\n\\t}, 0, ROLLBACKING_RETRY_PERIOD, TimeUnit.MILLISECONDS);\\n\\n    // Handle state-retryable transactions that can retry committing in the second stage\\n\\tretryCommitting.scheduleAtFixedRate(() -> {\\n\\t\\thandleRetryCommitting.scheduleAtFixedRate(() -> {\\n\\t\\t\\thandleRetryCommitting(); }\\n\\t\\t} catch (Exception e) {\\n\\t\\t\\tLOGGER.info(\\"Exception retry committing ... \\", e);\\n\\t\\t}\\n\\t}, 0, COMMITTING_RETRY_PERIOD, TimeUnit.MILLISECONDS);\\n\\n    // Handle asynchronous committing transactions\\n\\tasyncCommitting.scheduleAtFixedRate(() -> {\\n\\t\\ttry {\\n\\t\\t\\thandleAsyncCommitting(); } catch (Exception e) { asyncCommitting.\\n\\t\\t} catch (Exception e) {\\n\\t\\t\\tLOGGER.info(\\"Exception async committing ... \\", e);\\n\\t\\t}\\n\\t}, 0, ASYNC_COMMITTING_RETRY_PERIOD, TimeUnit.MILLISECONDS);\\n\\n\\t// Checking for a transaction whose first phase has timed out, setting the transaction state to TimeoutRollbacking.\\n\\t// the transaction will be rolled back by another timed task\\n\\ttimeoutCheck.scheduleAtFixedRate(() -> {\\n\\t\\ttimeoutCheck.scheduleAtFixedRate(() -> {\\n\\t\\t\\ttimeoutCheck(); } catch (Exception e) { timeoutCheck.scheduleAtFixedRate()\\n\\t\\t} catch (Exception e) {\\n\\t\\t\\tLOGGER.info(\\"Exception timeout checking ... \\", e);\\n\\t\\t}\\n\\t}, 0, TIMEOUT_RETRY_PERIOD, TimeUnit.MILLISECONDS); }\\n\\n\\t// Call RM to delete the unlog based on the number of days the unlog has been saved\\n\\tundoLogDelete.scheduleAtFixedRate(() -> {\\n\\t\\ttry {\\n\\t\\t\\tundoLogDelete(); }\\n\\t\\t} catch (Exception e) {\\n\\t\\t\\tLOGGER.info(\\"Exception undoLog deleting ... \\", e);\\n\\t\\t}\\n\\t}, UNDO_LOG_DELAY_DELETE_PERIOD, UNDO_LOG_DELETE_PERIOD, TimeUnit.MILLISECONDS); }\\n}\\n```\\n\\n#### 6. Initialising NettyRemotingServer\\nNettyRemotingServer is a simplified version of Rpc server based on Netty implementation, NettyRemotingServer initialisation does two main things:\\n1. **registerProcessor**: registers the Processor that communicates with the Client.\\n2. **super.init()**: the super.init() method is responsible for initialising Netty and registering the IP port of the current instance with the registry\\n\\n```java\\npublic void init() {\\n// registry processor\\nregisterProcessor();\\nif (initialised.compareAndSet(false, true)) {\\nsuper.init(); }\\n}\\n}\\n\\nprivate void registerProcessor() {\\n// 1. Register the core ServerOnRequestProcessor, i.e. the Processor associated with the transaction.\\n// e.g. global transaction start, commit, branch transaction registration, feedback on current state, etc.\\n// ServerOnRequestProcessor\'s constructor passes in the example returned by getHandler(), which handler\\n// is the aforementioned DefaultCoordinator, which is the core processing class for distributed transactions.\\nServerOnRequestProcessor onRequestProcessor =\\nnew ServerOnRequestProcessor(this, getHandler());\\n  super.registerProcessor(MessageType.TYPE_BRANCH_REGISTER, onRequestProcessor, messageExecutor);\\n  super.registerProcessor(MessageType.TYPE_BRANCH_STATUS_REPORT, onRequestProcessor, messageExecutor);\\n  super.registerProcessor(MessageType.TYPE_GLOBAL_BEGIN, onRequestProcessor, messageExecutor);\\n  super.registerProcessor(MessageType.TYPE_GLOBAL_COMMIT, onRequestProcessor, messageExecutor);\\n  super.registerProcessor(MessageType.TYPE_GLOBAL_LOCK_QUERY, onRequestProcessor, messageExecutor);\\n  super.registerProcessor(MessageType.TYPE_GLOBAL_REPORT, onRequestProcessor, messageExecutor);\\n  super.registerProcessor(MessageType.TYPE_GLOBAL_ROLLBACK, onRequestProcessor, messageExecutor);\\n  super.registerProcessor(MessageType.TYPE_GLOBAL_STATUS, onRequestProcessor, messageExecutor);\\n  super.registerProcessor(MessageType.TYPE_SEATA_MERGE, onRequestProcessor, messageExecutor);\\n\\t// 2. Register the ResponseProcessor, which is used to process the message that the Client replies to when the Server initiates a request.\\n\\t// Client replies with a response, e.g., if Server sends a request to Client to commit or roll back a branch transaction, // Client returns a commit/rollback message.\\n\\t// The Client returns the commit/rollback result.\\n\\tServerOnResponseProcessor onResponseProcessor =\\n\\t    new ServerOnResponseProcessor(getHandler(), getFutures()); super.registerProcessor(Message); }\\n\\tsuper.registerProcessor(MessageType.TYPE_BRANCH_COMMIT_RESULT, onResponseProcessor, messageExecutor);\\n\\tsuper.registerProcessor(MessageType.TYPE_BRANCH_ROLLBACK_RESULT, onResponseProcessor, messageExecutor); // 3.\\n\\n\\t// 3. The Processor on the Client side that initiates the RM registration request.\\n\\tRegRmProcessor regRmProcessor = new RegRmProcessor(this); super.registerProcessor(MessageProcessor, messageExecutor); }\\n\\tsuper.registerProcessor(MessageType.TYPE_REG_RM, regRmProcessor, messageExecutor); // 4.\\n\\n\\t// 4. The Processor that will be used when the Client initiates the TM registration request.\\n\\tRegTmProcessor regTmProcessor = new RegTmProcessor(this);\\n\\tsuper.registerProcessor(MessageType.TYPE_REG_CLT, regTmProcessor, null); // 5.\\n\\n\\t// 5. The Processor that the Client sends a heartbeat request to.\\n\\tServerHeartbeatProcessor heartbeatMessageProcessor = new ServerHeartbeatProcessor(this); super.registerProcessor(MessageType.TYPE_REG_CLT, null); }\\n\\tsuper.registerProcessor(MessageType.TYPE_HEARTBEAT_MSG, heartbeatMessageProcessor, null);\\n}\\n```\\nIn NettyRemotingServer there is a call to the init method of the base class AbstractNettyRemotingServer with the following code:\\n```Java\\npublic void init() {\\n// The super.init() method starts a timed task that cleans up timed-out Rpc requests once every 3S. super.init(); // Configure the Netty Server side to start executing a timed task that cleans up timed-out Rpc requests once every 3S.\\nsuper.init(); // Configure the Netty Server side to start a timed task that cleans up timed out Rpc requests, 3S once.\\n// Configure the Netty Server side to start listening on a port.\\nserverBootstrap.start(); // Configure the Netty server side to start listening on the port.\\n}\\n\\n// serverBootstrap.start(); // Configure the server side to start listening on the port.\\npublic void start() {\\n// General configuration of the Netty server side, where two ChannelHandlers are added: // ProtocolV1Decoder, ProtocolV1Decoder, ProtocolV1Decoder.\\n// ProtocolV1Decoder, ProtocolV1Encoder, // corresponding to Seata custom RFIDs, respectively.\\n// Decoder and Encoder, // corresponding to Seata\'s custom RPC protocols, respectively.\\n  this.serverBootstrap.group(this.eventLoopGroupBoss, this.eventLoopGroupWorker)\\n  .channel(NettyServerConfig.SERVER_CHANNEL_CLAZZ)\\n  .option(ChannelOption.SO_BACKLOG, nettyServerConfig.getSoBackLogSize())\\n  .option(ChannelOption.SO_REUSEADDR, true)\\n  .childOption(ChannelOption.SO_KEEPALIVE, true)\\n  .childOption(ChannelOption.TCP_NODELAY, true)\\n  .childOption(ChannelOption.SO_SNDBUF, nettyServerConfig.getServerSocketSendBufSize())\\n  .childOption(ChannelOption.SO_RCVBUF, nettyServerConfig.getServerSocketResvBufSize())\\n  .childOption(ChannelOption.WRITE_BUFFER_WATER_MARK,\\n  new WriteBufferWaterMark(nettyServerConfig.getWriteBufferLowWaterMark(),\\n  nettyServerConfig.getWriteBufferHighWaterMark()))\\n  .localAddress(new InetSocketAddress(listenPort))\\n  .childHandler(new ChannelInitializer<SocketChannel>() {\\n            @Override\\n            public void initChannel(SocketChannel ch) {\\n                ch.pipeline().addLast(new IdleStateHandler(nettyServerConfig.getChannelMaxReadIdleSeconds(), 0, 0))\\n                    .addLast(new ProtocolV1Decoder())\\n                    .addLast(new ProtocolV1Encoder()); if (channelHandlers !) .\\n                if (channelHandlers ! = null) {\\n                    addChannelPipelineLast(ch, channelHandlers); } if (channelHandlers ! = null)\\n                }\\n\\n            }\\n        });\\n\\n    try {\\n\\t\\t// Start listening on the configured port\\n        ChannelFuture future = this.serverBootstrap.bind(listenPort).sync(); {}\\" Start listening on the configured port.\\n        LOGGER.info(\\"Server started, listen port: {}\\", listenPort); // Connect the current server to a new port after Netty starts successfully.\\n\\t\\t// After a successful Netty startup register the current instance with the Registry.Conf configuration file\'s registry\\n        RegistryFactory.getInstance().register(new InetSocketAddress(XID.getIpAddress(), XID.getPort()); // After a successful startup register the current instance with the Registry. Conf configuration file.\\n        Initialisation.set(true); future.channel.config()\\n        future.channel().closeFuture().sync(); } catch (Exception exx).\\n    } catch (Exception exx) {\\n        Throw a new runtime exception (exx); }\\n    }\\n}\\n```"},{"id":"/seata-xa-introduce","metadata":{"permalink":"/blog/seata-xa-introduce","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-xa-introduce.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-xa-introduce.md","title":"How is distributed transaction realized? In-depth interpretation of Seata\'s XA mode","description":"In-depth interpretation of Seata\'s XA mode","date":"2020-04-28T00:00:00.000Z","formattedDate":"April 28, 2020","tags":[],"readingTime":14.365,"hasTruncateMarker":false,"authors":[{"name":"Xuan Yi"}],"frontMatter":{"title":"How is distributed transaction realized? In-depth interpretation of Seata\'s XA mode","keywords":["Seata","distributed transaction","XA","AT"],"description":"In-depth interpretation of Seata\'s XA mode","author":"Xuan Yi","date":"2020/04/28"},"unlisted":false,"prevItem":{"title":"Seata Source Code - Server Startup Process in Distributed Transactions","permalink":"/blog/seata-sourcecode-server-bootstrap"},"nextItem":{"title":"Seata Quick Start","permalink":"/blog/seata-quick-start"}},"content":"Author profile: Xuan Yi, GitHub ID: sharajava, responsible for the GTS development team of Alibaba Middleware, initiator of the SEATA open source project, worked for many years at Oracle Beijing R&D Center, and was engaged in WebLogic core development. He has long been focused on middleware, especially technical practices in the field of distributed transactions.\\n\\nThe 1.2.0 version of Seata has released a new transaction mode: XA mode, which supports the XA protocol.\\n\\nHere, we will interpret this new feature in depth from three aspects:\\n\\n- What: What is XA mode?\\n- Why: Why support XA?\\n- How: How is XA mode implemented and how to use it?\\n\\n# 1. What is XA mode?\\n\\nThere are two basic preliminary concepts here:\\n\\n1. What is XA?\\n2. What is the so-called transaction mode defined by Seata?\\n\\nBased on these two points, understanding XA mode becomes quite natural.\\n\\n## 1.1 What is XA?\\n\\nThe XA specification is a standard for distributed transaction processing (DTP) defined by the X/Open organization.\\n\\nThe XA specification describes the interface between the global transaction manager(TM) and the local resource manager(RM). The purpose of the XA specification is to allow multiple resources (such as databases, application servers, message queues, etc.) to access the same transaction, thus maintaining ACID properties across applications.\\n\\nThe XA specification uses the Two-Phase Commit (2PC) to ensure that all resources are committed or rolled back at the same time for any specific transaction.\\n\\nThe XA specification was proposed in the early 1990s. Currently, almost all mainstream databases support the XA specification.\\n\\n## 1.2 What is Seata\'s transaction mode?\\n\\nSeata defines the framework for global transactions.\\n\\nA global transaction is defined as the overall coordination of several branch transactions:\\n\\n1. The Transaction Manager (TM) requests the Transaction Coordinator (TC) to initiate (Begin), commit (Commit), or rollback (Rollback) the global transaction.\\n2. The TM binds the XID representing the global transaction to the branch transaction.\\n3. The Resource Manager (RM) registers with the TC, associating the branch transaction with the global transaction represented by XID.\\n4. The RM reports the execution result of the branch transaction to the TC. (optional)\\n5. The TC sends a branch commit or branch rollback command to the RM.\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB19qmhOrY1gK0jSZTEXXXDQVXa-1330-924.png\\" alt=\\"seata-mod\\" style={{ zoom:\'50%\' }} />\\n\\nSeata\'s global transaction processing process is divided into two phases:\\n\\n- Execution phase: Execute branch transactions and ensure that the execution results are *rollbackable* and *durable*.\\n- Completion phase: Based on the resolution of the execution phase, the application sends a request for global commit or rollback to the TC through the TM, and the TC commands the RM to drive the branch transaction to commit or rollback.\\n\\nSeata\'s so-called transaction mode refers to the behavior mode of branch transactions running under the Seata global transaction framework. More precisely, it should be called the branch transaction mode.\\n\\nThe difference between different transaction modes lies in the different ways branch transactions achieve the goals of the two phases of the global transaction.  That is, answering the following two questions:\\n\\n- Execution phase: How to execute and ensure that the execution results are *rollbackable* and *durable*.\\n- Completion phase: After receiving the command from the TC, how to submit or rollback the branch transaction?\\n\\nTaking our Seata AT mode and TCC mode as examples:\\n\\nAT mode\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1NTuzOBr0gK0jSZFnXXbRRXXa-1330-924.png\\" alt=\\"at-mod\\" style={{ zoom:\'50%\' }} />\\n\\n- Execution phase:\\n  - Rollbackable: Record rollback logs according to the SQL parsing result\\n  - Durable: Rollback logs and business SQL are committed to the database in the same local transaction\\n\\n- Completion phase:\\n  - Branch commit: Asynchronously delete rollback log records\\n  - Branch rollback: Compensate and update according to the rollback log\\n\\nTCC mode\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1m59pOEY1gK0jSZFCXXcwqXXa-1330-924.png\\" alt=\\"tcc-mod\\" style={{ zoom:\'50%\' }} />\\n\\n- Execution Phase:\\n\\n  - Call the Try method defined by the business (guaranteed *rollback* and *persistence* entirely by the business layer)\\n\\n- Completion Phase:\\n\\n  - Branch Commit: Call the Confirm method defined for each transaction branch\\n  - Branch Rollback: Call the Cancel method defined for each transaction branch\\n\\n## 1.3 What is XA mode in Seata?\\n\\nXA mode:\\n\\nWithin the distributed transaction framework defined by Seata, it is a transaction mode that uses XA protocol mechanisms to manage branch transactions with the support of transaction resources (databases, message services, etc.) for the XA protocol.\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1hSpccIVl614jSZKPXXaGjpXa-1330-924.png\\" alt=\\"xa-mod\\" style={{ zoom:\'50%\' }} />\\n\\n- Execution Phase:\\n\\n  - Rollback: Business SQL operations are performed in an XA branch, and the support of resources for the XA protocol ensures *rollback*\\n  - Persistence: After the XA branch is completed, XA prepare is executed, and similarly, the support of resources for the XA protocol ensures *persistence* (i.e., any unexpected occurrences will not cause situations where rollback is not possible)\\n\\n- Completion Phase:\\n\\n  - Branch Commit: Perform commit for XA branch\\n  - Branch Rollback: Perform rollback for XA branch\\n\\n# 2. Why support XA?\\n\\nWhy add XA mode in Seata? What is the significance of supporting XA?\\n\\n## 2.1 Problems with Compensatory Transaction Mode\\n\\nEssentially, the 3 major transaction modes that Seata already supports: AT, TCC, and Saga, are all compensatory in nature.\\n\\nCompensatory transaction processing mechanisms are built on top of transaction resources (either in the middleware layer or in the application layer), and the transaction resources themselves are unaware of distributed transactions.\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1z.qyOET1gK0jSZFrXXcNCXXa-602-460.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nThe fundamental problem with transaction resources being unaware of distributed transactions is the inability to achieve true *global consistency*.\\n\\nFor example, in a compensatory transaction processing process, a stock record is reduced from 100 to 50. At this point, the warehouse administrator connects to the database and sees the current quantity as 50. Later, the transaction is rolled back due to an unexpected occurrence, and the stock is compensated back to 100. Clearly, the warehouse administrator\'s query finding 50 is *dirty data*.\\n\\nIt can be seen that because compensatory distributed transaction mechanisms do not require the mechanism of transaction resources (such as a database), they cannot guarantee data consistency from a global perspective outside the transaction framework.\\n\\n## 2.2 Value of XA\\n\\nUnlike compensatory transaction modes, the XA protocol requires transaction resources to provide support for standards and protocols.\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1vs9kOvb2gK0jSZK9XXaEgFXa-602-486.png\\" alt=\\"nct\\" style={{ zoom:\'50%\' }} />\\n\\nBecause transaction resources are aware of and participate in the distributed transaction processing process, they (such as databases) can guarantee effective isolation of data from any perspective and satisfy global data consistency.\\n\\nFor example, in the scenario of stock updates mentioned in the previous section, during the XA transaction processing process, the intermediate state of the database holding 50 is guaranteed by the database itself and will not be *seen* in the warehouse administrator\'s query statistics. (Of course, the isolation level needs to be READ_COMMITTED or higher.)\\n\\nIn addition to the fundamental value of *global consistency*, supporting XA also has the following benefits:\\n\\n1. Non-invasive business: Like AT, XA mode will be non-invasive for businesses, without bringing additional burden to application design and development.\\n2. Wide support for databases: XA protocol is widely supported by mainstream relational databases and can be used without additional adaptation.\\n3. Easy multi-language support: Because it does not involve SQL parsing, the XA mode has lower requirements for Seata\'s RM, making it easier for different language development SDKs compared to the AT mode.\\n4. Migration of traditional XA-based applications: Traditional applications based on the XA protocol can be smoothly migrated to the Seata platform using the XA mode.\\n\\n## 2.3 Widely Questioned Issues of XA\\n\\nThere is no distributed transaction mechanism that can perfectly adapt to all scenarios and meet all requirements.\\n\\nThe XA specification was proposed as early as the early 1990s to solve the problems in the field of distributed transaction processing.\\n\\nNow, whether it\'s the AT mode, TCC mode, or the Saga mode, the essence of these modes\' proposals stems from the inability of the XA specification to meet certain scenario requirements.\\n\\nThe distributed transaction processing mechanism defined by the XA specification has some widely questioned issues. What is our thinking regarding these issues?\\n\\n1. **Data Locking**: Data is locked throughout the entire transaction processing until it is finished, and reads and writes are constrained according to the definition of isolation levels.\\n\\n> Thinking:\\n>\\n> Data locking is the cost to obtain higher isolation and global consistency.\\n>\\n> In compensatory transaction processing mechanisms, the completion of branch (local) transactions is done during the execution stage, and data is not locked at the resource level. However, this is done at the cost of sacrificing isolation.\\n>\\n> Additionally, the AT mode uses *global locks* to ensure basic *write isolation*, effectively locking data, but the lock is managed centrally on the TC side, with high unlock efficiency and no blocking issues.\\n\\n2. **Protocol Blocking**: After XA prepare, the branch transaction enters a blocking stage and must wait for XA commit or XA rollback.\\n\\n> Thinking:\\n>\\n> The blocking mechanism of the protocol itself is not the problem. The key issue is the combination of protocol blocking and data locking.\\n>\\n> If a resource participating in the global transaction is \\"offline\\" (does not receive commands to end branch transactions), the data it locks will remain locked. This may even lead to deadlocks.\\n>\\n> This is the core pain point of the XA protocol and is the key problem that Seata aims to solve by introducing the XA mode.\\n>\\n> The basic idea is twofold: avoiding \\"loss of connection\\" and adding a \\"self-release\\" mechanism. (This involves a lot of technical details, which will not be discussed at the moment. They will be specifically discussed in the subsequent evolution of the XA mode.)\\n\\n3. **Poor Performance**: Performance loss mainly comes from two aspects: on one hand, the transaction coordination process increases the RT of individual transactions; on the other hand, concurrent transaction data lock conflicts reduce throughput.\\n\\n> Thinking:\\n>\\n> Compared to running scenarios without distributed transaction support, performance will certainly decline, there is no doubt about that.\\n>\\n> Essentially, the transaction mechanism (whether local or distributed) sacrifices some performance to achieve a simple programming model.\\n>\\n> Compared to the AT mode, which is also *non-invasive for businesses*:\\n>\\n> Firstly, because XA mode also runs under Seata\'s defined distributed transaction framework, it does not generate additional transaction coordination communication overhead.\\n>\\n> Secondly, in concurrent transactions, if data has hotspots and lock conflicts occur, this situation also exists in the AT mode (which defaults to using a global lock).\\n>\\n> Therefore, in the two main aspects affecting performance, the XA mode does not have a significantly obvious disadvantage compared to the AT mode.\\n>\\n> The performance advantage of the AT mode mainly lies in: centralized management of global data locks, where the release of locks does not require RM involvement and is very fast; in addition, the asynchronous completion of the global commit stage.\\n\\n# 3. How Does XA Mode Work and How to Use It?\\n\\n## 3.1 Design of XA Mode\\n\\n### 3.1.1 Design Objectives\\n\\nThe basic design objectives of XA mode mainly focus on two main aspects:\\n\\n1. From the perspective of *scenarios*, it meets the requirement of *global consistency*.\\n2. From the perspective of *applications*, it maintains the non-invasive nature consistent with the AT mode.\\n3. From the perspective of *mechanisms*, it adapts to the characteristics of distributed microservice architecture.\\n\\nOverall idea:\\n\\n1. Same as the AT mode: Construct branch transactions from local transactions in the application program.\\n2. Through data source proxy, wrap the interaction mechanism of the XA protocol at the framework level outside the scope of local transactions in the application program, making the XA programming model transparent.\\n3. Split the 2PC of XA and perform XA prepare at the end of the execution stage of branch transactions, seamlessly integrating the XA protocol into Seata\'s transaction framework, reducing one round of RPC interaction.\\n\\n### 3.1.2 Core Design\\n\\n#### 1. Overall Operating Mechanism\\n\\nXA mode runs within the transaction framework defined by Seata:\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1uM2OaSslXu8jSZFuXXXg7FXa-1330-958.png\\" alt=\\"xa-fw\\" style={{ zoom:\'50%\' }} />\\n\\n- Execution phase (Execute):\\n\\n  - XA start/XA end/XA prepare + SQL + Branch registration\\n\\n- Completion phase (Finish):\\n\\n  - XA commit/XA rollback\\n\\n#### 2. Data Source Proxy\\n\\nXA mode requires XAConnection.\\n\\nThere are two ways to obtain XAConnection:\\n\\n- Method 1: Requires developers to configure XADataSource\\n- Method 2: Creation based on the developer\'s normal DataSource\\n\\nThe first method adds cognitive burden to developers, as they need to learn and use XA data sources specifically for XA mode, which contradicts the design goal of transparent XA programming model.\\n\\nThe second method is more user-friendly, similar to the AT mode, where developers do not need to worry about any XA-related issues and can maintain a local programming model.\\n\\nWe prioritize the implementation of the second method: the data source proxy creates the corresponding XAConnection based on the normal JDBC connection obtained from the normal data source.\\n\\nComparison with the data source proxy mechanism of the AT mode:\\n\\n<img src=\\"/img/xa/pics/ds1.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nHowever, the second method has limitations: it cannot guarantee compatibility correctness.\\n\\nIn fact, this method is what database drivers should do. Different vendors and different versions of database driver implementation mechanisms are vendor-specific, and we can only guarantee correctness on fully tested driver versions, as differences in the driver versions used by developers can lead to the failure of the mechanism.\\n\\nThis is particularly evident in Oracle. See Druid issue: [https://github.com/alibaba/druid/issues/3707](https://github.com/alibaba/druid/issues/3707)\\n\\nTaking everything into account, the data source proxy design for XA mode needs to support the first method: proxy based on XA data source.\\n\\nComparison with the data source proxy mechanism of the AT mode:\\n\\n<img src=\\"/img/xa/pics/ds2.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\n#### 3. Branch Registration\\n\\nXA start requires the Xid parameter.\\n\\nThis Xid needs to be associated with the XID and BranchId of the Seata global transaction, so that the TC can drive the XA branch to commit or rollback.\\n\\nCurrently, the BranchId in Seata is generated uniformly by the TC during the branch registration process, so the timing of the XA mode branch registration needs to be before XA start.\\n\\nA possible optimization in the future:\\n\\nDelay branch registration as much as possible. Similar to the AT mode, register the branch before the local transaction commit to avoid meaningless branch registration in case of branch execution failure.\\n\\nThis optimization direction requires a change in the BranchId generation mechanism to cooperate. BranchId will not be generated through the branch registration process, but will be generated and then used to register the branch.\\n\\n#### 4. Summary\\n\\nHere, only a few important core designs of the XA mode are explained to illustrate its basic operation mechanism.\\n\\nIn addition, important aspects such as *connection maintenance* and *exception handling* are also important and can be further understood from the project code.\\n\\nMore information and exchange will be written and shared with everyone in the future.\\n\\n### 3.1.3 Evolution Plan\\n\\nThe overall evolution plan of the XA mode is as follows:\\n\\n1. Step 1 (already completed): The first version (1.2.0) runs the prototype mechanism of the XA mode. Ensure only addition, no modification, and no new issues introduced to other modes.\\n2. Step 2 (planned to be completed in May): Necessary integration and refactoring with the AT mode.\\n3. Step 3 (planned to be completed in July): Refine the exception handling mechanism and polish for production readiness.\\n4. Step 4 (planned to be completed in August): Performance optimization.\\n5. Step 5 (planned to be completed in 2020): Integrate with Seata project\'s ongoing design for cloud-native Transaction Mesh to create cloud-native capabilities.\\n\\n## 3.2 Usage of XA Mode\\n\\nFrom a programming model perspective, XA mode is exactly the same as the AT mode.\\n\\nYou can refer to the Seata official website sample: [seata-xa](https://github.com/seata/seata-xa)\\n\\nThe example scenario is the classic Seata example, involving the product ordering business of three microservices: inventory, orders, and accounts.\\n\\nIn the example, the upper programming model is the same as the AT mode. By simply modifying the data source proxy, you can switch between XA mode and AT mode.\\n\\n```java\\n@Bean(\\"dataSource\\")\\npublic DataSource dataSource(DruidDataSource druidDataSource) {\\n    // DataSourceProxy for AT mode\\n    // return new DataSourceProxy(druidDataSource);\\n\\n    // DataSourceProxyXA for XA mode\\n    return new DataSourceProxyXA(druidDataSource);\\n}\\n```\\n\\n# 4. Summary\\n\\nAt the current stage of technological development, there is no distributed transaction processing mechanism that can perfectly meet all scenarios\' requirements.\\n\\nConsistency, reliability, ease of use, performance, and many other aspects of system design constraints require different transaction processing mechanisms to meet them.\\n\\nThe core value of the Seata project is to build a standardized platform that comprehensively addresses the distributed transaction problem.\\n\\nBased on Seata, the upper application architecture can flexibly choose the appropriate distributed transaction solution according to the actual scenario\'s needs.\\n\\n<img src=\\"https://img.alicdn.com/tfs/TB1lTSoOqL7gK0jSZFBXXXZZpXa-1028-528.png\\" alt=\\"img\\" style={{ zoom:\'50%\' }} />\\n\\nThe addition of XA mode fills the gap in Seata in the global consistency scenario, forming a landscape of four major transaction modes: AT, TCC, Saga, and XA, which can basically meet all scenarios\' demands for distributed transaction processing.\\n\\nOf course, both XA mode and the Seata project itself are not yet perfect, and there are many areas that need improvement and enhancement. We warmly welcome everyone to participate in the project\'s development and contribute to building a standardized distributed transaction platform together."},{"id":"/seata-quick-start","metadata":{"permalink":"/blog/seata-quick-start","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-quick-start.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-quick-start.md","title":"Seata Quick Start","description":"Getting started with Seata from scratch, setting up Seata services, and integrating distributed transactions into Java projects.","date":"2020-04-19T00:00:00.000Z","formattedDate":"April 19, 2020","tags":[],"readingTime":10.675,"hasTruncateMarker":false,"authors":[{"name":"yudaoyuanma"}],"frontMatter":{"title":"Seata Quick Start","description":"Getting started with Seata from scratch, setting up Seata services, and integrating distributed transactions into Java projects.","keywords":["fescar","seata","distributed transaction"],"author":"yudaoyuanma","date":"2020/04/19"},"unlisted":false,"prevItem":{"title":"How is distributed transaction realized? In-depth interpretation of Seata\'s XA mode","permalink":"/blog/seata-xa-introduce"},"nextItem":{"title":"Seata High Availability Deployment Practice","permalink":"/blog/seata-ha-practice"}},"content":"- [1. Overview](#)\\n- [2. Deploying Standalone TC Server](#)\\n- [3. Deploying Cluster TC Server](#)\\n- [4. Accessing Java Applications](#)\\n\\n# 1. Overview\\n\\n[Seata](https://github.com/apache/incubator-seata) is an open source **Ali** open source **distributed transaction **solution , is committed to providing high-performance and easy-to-use distributed transaction services .\\n\\n## 1.1 Four transaction patterns\\n\\nSeata aims to create a **one-stop** solution for distributed transactions, and will eventually provide four transaction modes:\\n\\n- AT mode: See the [\\"Seata AT mode\\"](/docs/dev/mode/at-mode/) document.\\n- TCC mode: see the Seata TCC mode document (/docs/dev/mode/tcc-mode/).\\n- Saga mode: see the document [\\"SEATA Saga mode\\"](/docs/dev/mode/saga-mode/).\\n- XA mode: under development...\\n\\nCurrently used **popularity** situation is: AT > TCC > Saga. therefore, when we learn Seata, we can spend more energy on **AT mode**, it is best to understand the principle behind the implementation, after all, distributed transaction involves the correctness of the data, the problem needs to be quickly troubleshooting to locate and solve.\\n\\n> Friendly note: specific popularity, friends can choose to look at [Wanted: who\'s using Seata](https://github.com/apache/incubator-seata/issues/1246) each company registered use.\\n\\n## 1.2 Three roles\\n\\nThere are three roles in the architecture of Seata:\\n\\n! [Three Roles](http://www.iocoder.cn/images/Seata/2017-01-01/02.png)\\n\\n- **TC** (Transaction Coordinator) - Transaction Coordinator: maintains the state of global and branch transactions, drives **global transactions** commit or rollback.\\n- **TM** (Transaction Manager) - Transaction Manager: defines the scope of a **global transaction**, starts the global transaction, commits or rolls back the global transaction.\\n- **RM** (Resource Manager) - Resource Manager: manages the resources processed by the **Branch Transaction**, talks to the TC to register the branch transaction and report on the status of the branch transaction, and drives the **Branch Transaction** to commit or rollback.\\n\\nThe TC is a separately deployed **Server** server and the TM and RM are **Client** clients embedded in the application.\\n\\nIn Seata, the **Lifecycle** of a distributed transaction is as follows:\\n\\n! [Architecture diagram](http://www.iocoder.cn/images/Seata/2017-01-01/01.png)\\n\\n> Friendly reminder: look at the red ticks added by the carrots.\\n\\n- The TM requests the TC to open a global transaction. the TC generates a **XID** as the number of this global transaction.\\n\\n> **XID**, which is propagated through the microservice\'s invocation chain, is guaranteed to associate multiple microservice sub-transactions together.\\n\\n- RM requests the TC to register the local transaction as a branch transaction of the global transaction to be associated via the **XID** of the global transaction.\\n- The TM requests the TC to tell the **XID** whether the corresponding global transaction is to be committed or rolled back.\\n- TC drives RMs to commit or rollback their own local transactions corresponding to **XID**.\\n\\n## 1.3 Framework Support\\n\\nSeata currently provides support for the major **microservices frameworks**:\\n\\n- Dubbo\\n\\n> Integration via [`seata-dubbo`](https://github.com/apache/incubator-seata/blob/develop/integration/dubbo/)\\n\\n- SOFA-RPC\\n\\n> integrated via [`seata-sofa-rpc`](https://github.com/apache/incubator-seata/blob/develop/integration/sofa-rpc/)\\n\\n- Motan\\n\\n> Integrated via [`seata-motan`](https://github.com/apache/incubator-seata/blob/develop/integration/motan/)\\n\\n- gRPC\\n\\n> integrated via [`seata-grpc`](https://github.com/apache/incubator-seata/blob/develop/integration/gprc/)\\n\\n- Apache HttpClient\\n\\n> integrated via [`seata-http`](https://github.com/apache/incubator-seata/blob/develop/integration/http/)\\n\\n- Spring Cloud OpenFeign\\n  > via [`spring-cloud-starter-alibaba-seata`](https://github.com/alibaba/spring-cloud-alibaba/blob/master/spring-cloud-alibaba-starters/spring-cloud-starter-alibaba-seata/src/main/java/com/alibaba/cloud/seata/) of [`feign`](https://github.com/alibaba/spring-cloud-alibaba/blob/master/spring-cloud-alibaba-starters/spring-cloud-starter-alibaba-seata/src/main/java/com/alibaba/cloud/seata/feign/) module\\n- Spring RestTemplate\\n  > via [`spring-cloud-starter-alibaba-seata`](https://github.com/alibaba/spring-cloud-alibaba/blob/master/spring-cloud-alibaba-starters/spring-cloud-starter-alibaba-seata/src/main/java/com/alibaba/cloud/seata/feign/SeataBeanPostProcessor.java) of [`rest`](https://github.com/alibaba/spring-cloud-alibaba/blob/master/spring-cloud-alibaba-starters/spring-cloud-starter-alibaba-seata/src/main/java/com/alibaba/cloud/seata/rest/) module\\n\\n\\nSeata also provides a Starter library for easy integration into Java projects:\\n\\n- [`seata-spring-boot-starter`](https://mvnrepository.com/artifact/io.seata/seata-spring-boot-starter)\\n- [`spring-cloud-starter-alibaba-seata`](https://mvnrepository.com/artifact/com.alibaba.cloud/spring-cloud-starter-alibaba-seata)\\n\\nBecause Seata is based on the [DataSource](https://docs.oracle.com/javase/7/docs/api/javax/sql/DataSource.html) data source for **proxy** to extend, it naturally provides very good support for mainstream ORM frameworks:\\n\\n- MyBatis, MyBatis-Plus\\n- JPA, Hibernate\\n\\n## 1.4 Case Scenarios\\n\\nFrom the registration of [Wanted: who\'s using Seata](https://github.com/apache/incubator-seata/issues/1246), Seata has started to land in many teams in China, including many large companies such as DDT and Rhyme. This can be summarised in the figure below:\\n\\n! [summary chart](http://www.iocoder.cn/images/Seata/2017-01-01/03.png)\\n\\nIn addition, in the [awesome-seata](https://github.com/seata/awesome-seata) warehouse, carrots carrots see the drop and so on the company\'s landing when the technology to share, or very real and reliable. As shown in the picture below:! [awesome-seata \u6ef4\u6ef4](http://www.iocoder.cn/images/Seata/2017-01-01/04.png)\\n\\nIn terms of the case, Seata is probably the most reliable distributed transaction solution known to date, or at least it is a very good choice to invest in it technically.\\n\\n# 2. Deploying a Standalone TC Server\\n\\nIn this subsection, we will learn to deploy a **standalone** Seata **TC** Server, which is commonly used for learning or testing purposes, and is not recommended to be deployed in a production environment.\\n\\nBecause TC needs to record global and branch transactions, it needs corresponding **storage**. Currently, TC has two storage modes ( `store.mode`):\\n\\n- file mode: suitable for **standalone** mode, global transaction session information is read/written in **memory** and persisted to local file `root.data`, with high performance.\\n- db mode: suitable for **cluster** mode, global transaction session information is shared via **db**, relatively low performance.\\n\\nObviously, we will adopt the file mode, and finally we deploy the standalone TC Server as shown below: ! [Standalone TC Server](http://www.iocoder.cn/images/Seata/2017-01-01/11.png)\\n\\nAfter so much beeping, we start to formally deploy the standalone TC Server, here carrots carrots use macOS system, and Linux, Windows is similar to the friend of the brain to translate.\\n\\n## 2.1 Download Seata Package\\n\\nOpen the [Seata download page](https://github.com/apache/incubator-seata/releases), and select the version of Seata you want. Here, we choose [v1.1.0](https://github.com/apache/incubator-seata/releases/tag/v1.1.0), the latest version.\\n\\n ```Bash\\n # Create the directory\\n $ mkdir -p /Users/yunai/Seata\\n $ cd /Users/yunai/Seata\\n\\n # Download\\n $ wget https://github.com/apache/incubator-seata/releases/download/v1.1.0/seata-server-1.1.0.tar.gz\\n\\n # Extract\\n $ tar -zxvf seata-server-1.1.0.tar.gz\\n\\n # View directory\\n $ cd seata\\n $ ls -ls\\n 24 -rw-r--r-- 1 yunai staff 11365 May 13 2019 LICENSE\\n 0 drwxr-xr-x 4 yunai staff 128 Apr 2 07:46 bin # Executing scripts\\n 0 drwxr-xr-x 9 yunai staff 288 Feb 19 23:49 conf # configuration file\\n 0 drwxr-xr-x 138 yunai staff 4416 Apr 2 07:46 lib # seata-*.jar + dependency library\\n ```\\n\\n## 2.2 Starting TC Server\\n\\nExecute the `nohup sh bin/seata-server.sh &` command to start TC Server in the background. In the `nohup.out` file, we see the following log, which indicates that the startup was successful:\\n\\n```Java\\n# Using File Storage\\n2020-04-02 08:36:01.302 INFO [main]io.seata.common.loader.EnhancedServiceLoader.loadFile:247 -load TransactionStoreManager[FILE] extension by class[io.seata.server.store.file.FileTransactionStoreManager]\\n2020-04-02 08:36:01.302 INFO [main]io.seata.common.loader.EnhancedServiceLoader.loadFile:247 -load SessionManager[FILE] extension by class [io.seata.server.session.file.FileBasedSessionManager]\\n# Started successfully\\n2020-04-02 08:36:01.597 INFO [main]io.seata.core.rpc.netty.RpcServerBootstrap.start:155 -Server started ...\\n ```\\n\\n - In the default configuration, Seata TC Server starts on the **8091** endpoint.\\n\\n Since we are using file mode, we can see the local file `root.data` for persistence. The command to do this is as follows:\\n\\n ```Bash\\n $ ls -ls sessionStore/\\n total 0\\n 0 -rw-r--r-- 1 yunai staff 0 Apr 2 08:36 root.data\\n ```\\n\\nAs a follow-up, you can read the [\\"4. Getting Started with Java Applications\\"](#) subsection to get started with distributed transactions using Seata.\\n\\n# 3. Deploying a Clustered TC Server\\n\\nIn this subsection, we will learn to deploy **Cluster** Seata **TC** Server to achieve high availability, a must for production environments. In clustering, multiple Seata TC Servers share global transaction session information through the **db** database.\\n\\nAt the same time, each Seata TC Server can register itself to the registry so that applications can get them from the registry. Eventually we deploy the Clustered TC Server as shown below: ! [Cluster TC Server](http://www.iocoder.cn/images/Seata/2017-01-01/21.png)\\n\\nSeata TC Server provides integration with all major registries, as shown in the [discovery](https://github.com/apache/incubator-seata/tree/develop/discovery) directory. Considering the increasing popularity of using Nacos as a registry in China, we will use it here.\\n\\n> Friendly note: If you don\'t know anything about Nacos, you can refer to the [\\"Nacos Installation and Deployment\\"](http://www.iocoder.cn/Nacos/install/?self) article.\\n\\nAfter beeping so much, we start to deploy standalone TC Server formally, here carrots carrots use macOS system, and Linux, Windows is similar to the friend of the brain to translate.\\n\\n## 3.1 Downloading the Seata package\\n\\nOpen the Seata download page (https://github.com/apache/incubator-seata/releases), and select the version of Seata you want. Here, we choose [v1.1.0](https://github.com/apache/incubator-seata/releases/tag/v1.1.0), the latest version.\\n\\n ```Bash\\n # Create the directory\\n $ mkdir -p /Users/yunai/Seata\\n $ cd /Users/yunai/Seata\\n\\n # Download\\n $ wget https://github.com/apache/incubator-seata/releases/download/v1.1.0/seata-server-1.1.0.tar.gz\\n\\n # Extract\\n $ tar -zxvf seata-server-1.1.0.tar.gz\\n\\n # View directory\\n $ cd seata\\n $ ls -ls\\n 24 -rw-r--r-- 1 yunai staff 11365 May 13 2019 LICENSE\\n 0 drwxr-xr-x 4 yunai staff 128 Apr 2 07:46 bin # Executing scripts\\n 0 drwxr-xr-x 9 yunai staff 288 Feb 19 23:49 conf # configuration file\\n 0 drwxr-xr-x 138 yunai staff 4416 Apr 2 07:46 lib # seata-*.jar + dependency library\\n ```\\n\\n## 3.2 Initialising the database\\n\\n\u2460 Use the [``mysql.sql``](https://github.com/apache/incubator-seata/blob/develop/script/server/db/mysql.sql) script to initialise the db database of Seata TC Server. The contents of the script are as follows:\\n\\n ```SQL\\n -- -------------------------------- The script used when storeMode is \'db\' --------------------------------\\n -- the table to store GlobalSession data\\n CREATE TABLE IF NOT EXISTS `global_table`\\n (\\n    `xid`                       VARCHAR(128) NOT NULL,\\n    `transaction_id` BIGINT, `status` TINYL\\n    `status`                    TINYINT      NOT NULL,\\n    `application_id` VARCHAR(32), `transaction_service\\n    `transaction_service_group` VARCHAR(32),\\n    `transaction_name` VARCHAR(128),\\n    `timeout`                   INT,\\n    `begin_time`                BIGINT,\\n    `application_data` VARCHAR(2000), `gmt_create\\n    `gmt_create`                DATETIME,\\n    `gmt_modified`              DATETIME,\\n    PRIMARY KEY (`xid`),\\n    KEY `idx_gmt_modified_status` (`gmt_modified`, `status`),\\n    KEY `idx_transaction_id` (`transaction_id`)\\n ) ENGINE = InnoDB\\n  DEFAULT CHARSET = utf8.\\n\\n -- the table to store BranchSession data\\n CREATE TABLE IF NOT EXISTS `branch_table`\\n (\\n    `branch_id` BIGINT NOT NULL, `xid` VARCHARGE\\n    `xid`               VARCHAR(128) NOT NULL,\\n    `transaction_id`    BIGINT,\\n    `resource_group_id` VARCHAR(32), `resource_id` VARCHAR(32), `transaction_id` BIGINT\\n    `resource_id`       VARCHAR(256),\\n    `branch_type` VARCHAR(8), `status` TINYINT\\n    `status`            TINYINT,\\n    `client_id` VARCHAR(64), `application_data` TINYINT, `client_id` VARCHAR(64), `application_data` TINYINT\\n    `application_data` VARCHAR(2000), `gmt_create\\n    `gmt_create`        DATETIME(6),\\n    `gmt_modified`      DATETIME(6),\\n    PRIMARY KEY (`branch_id`), `branch_id`, `idx_x\\n    KEY `idx_xid` (`xid`)\\n ) ENGINE = InnoDB\\n  DEFAULT CHARSET = utf8; -- the table to store lock data.\\n\\n -- the table to store lock data\\n CREATE TABLE IF NOT EXISTS `lock_table`\\n (\\n    `row_key` VARCHAR(128) NOT NULL, `xid` VARCHAR(128) NOT NULL, -- the table to store lock data\\n    `xid`            VARCHAR(96),\\n    `transaction_id` BIGINT, `branch_id` BIGINT, `branch_id` BIGINT\\n    `branch_id` BIGINT NOT NULL,\\n    `resource_id`    VARCHAR(256),\\n    `table_name`     VARCHAR(32),\\n    `pk` VARCHAR(36), `gmt_create` VARCHAR(256), `gmt_create\\n    `gmt_create` DATETIME, `gmt_modify` VARCHAR(256), `pk` VARCHAR(36), `gmt_create` DATETIME\\n    `gmt_modified`   DATETIME,\\n    PRIMARY KEY (`row_key`),\\n    KEY `idx_branch_id` (`branch_id`)\\n ) ENGINE = InnoDB\\n  DEFAULT CHARSET = utf8.\\n ```\\n\\nIn MySQL, create `seata` database and execute the script under it. The final result is as follows: ! [`seata` Database - MySQL 5.X](http://www.iocoder.cn/images/Seata/2017-01-01/22.png)\\n\\n\u2461 Modify the `conf/file` configuration file to use the db database to share the global transaction session information of Seata TC Server. As shown in the following figure: ! [`conf/file` configuration file](http://www.iocoder.cn/images/Seata/2017-01-01/23.png)\\n\\n\u2462 MySQL8 support\\n\\n> If your friend is using MySQL version 8.X, you need to see this step. Otherwise, you can just skip it.\\n\\nFirstly, you need to download the MySQL 8.X JDBC driver, the command line operation is as follows:\\n\\n ```Bash\\n $ cd lib\\n $ wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.19/mysql-connector-java-8.0.19.jar\\n ```\\n\\nThen, modify the `conf/file` configuration file to use the MySQL 8.X JDBC driver. As shown below: ! [`seata` database - MySQL 8.X](http://www.iocoder.cn/images/Seata/2017-01-01/24.png)\\n\\n## 3.3 Setting up to use the Nacos Registry\\n\\nModify the `conf/registry.conf` configuration file to set up the Nacos registry. As shown in the following figure: ! [`conf/registry.conf` configuration file](http://www.iocoder.cn/images/Seata/2017-01-01/25.png)\\n\\n## 3.4 Starting TC Server\\n\\n\u2460 Execute `nohup sh bin/seata-server.sh -p 18091 -n 1 &` command to start **the first** TC Server in the background.\\n\\n- `-p`: Port on which Seata TC Server listens.\\n- `-n`: Server node. In case of multiple TC Servers, it is necessary to differentiate the respective nodes for generating transactionId transaction numbers for different zones to avoid conflicts.\\n\\nIn the `nohup.out` file, we see the following log, indicating a successful startup:\\n\\n```Java\\n# Using DB Stores\\n2020-04-05 16:54:12.793 INFO [main]io.seata.common.loader.EnhancedServiceLoader.loadFile:247 -load DataSourceGenerator[dbcp] extension by class[io.seata.server.store.db.DbcpDataSourceGenerator]\\nLoading class `com.mysql.jdbc.Driver\'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver\'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\\n2020-04-05 16:54:13.442 INFO [main]io.seata.common.loader.EnhancedServiceLoader.loadFile:247 -load LogStore[DB] extension by class[io. seata.core.store.db.LogStoreDataBaseDAO]\\n2020-04-05 16:54:13.442 INFO [main]io.seata.common.loader.EnhancedServiceLoader.loadFile:247 -load TransactionStoreManager[DB] extension by class[io.seata.server.store.db.DatabaseTransactionStoreManager]\\n2020-04-05 16:54:13.442 INFO [main]io.seata.common.loader.EnhancedServiceLoader.loadFile:247 -load SessionManager[DB] extension by class[ io.seata.server.session.db.DataBaseSessionManager]\\n# Started successfully\\n2020-04-05 16:54:13.779 INFO [main]io.seata.core.rpc.netty.RpcServerBootstrap.start:155 -Server started ...\\n# Using the Nacos Registry\\n2020-04-05 16:54:13.788 INFO [main]io.seata.common.loader.EnhancedServiceLoader.loadFile:247 -load RegistryProvider[Nacos] extension by class[io.seata.discovery.registry.nacos.NacosRegistryProvider]\\n ```\\n\\n \u2461 Execute the `nohup sh bin/seata-server.sh -p 28091 -n 2 &` command to start the **second** TC Server in the background.\\n\\n \u2462 Open the Nacos Registry console and we can see that there are **two** Seata TC Server examples. As shown in the following figure: ! [Nacos console](http://www.iocoder.cn/images/Seata/2017-01-01/26.png)\\n\\n # 4. Accessing Java Applications\\n\\n ## 4.1 AT mode\\n\\n **\u2460 Spring Boot**.\\n\\n 1. [\\"2. AT Mode + Multiple Data Sources\\"](#) subsection of [\\"Getting Started with Taro Road Spring Boot Distributed Transaction Seata\\"](http://www.iocoder.cn/Spring-Boot/Seata/?self) implements distributed transactions for a single Spring Boot project under multiple data sources.\\n\\n ! [Overall diagram](http://www.iocoder.cn/images/Spring-Boot/2020-10-01/01.png)\\n\\n 2. [\\"AT Pattern + HttpClient Remote Call\\"](#) subsection of [\\"Getting Started with Taro Road Spring Boot Distributed Transaction Seata\\"](http://www.iocoder.cn/Spring-Boot/Seata/?self), to implement distributed transactions for multiple Spring Boot projects.\\n\\n ! [Overall diagram](http://www.iocoder.cn/images/Spring-Boot/2020-10-01/21.png)\\n\\n **\u2461 Dubbo**\\n\\n Subsection [\\"2. AT Patterns\\"](#) of [\\"Getting Started with Dubbo Distributed Transaction Seata\\"](http://www.iocoder.cn/Dubbo/Seata/?sef) implements distributed transactions under multiple Dubbo services.\\n\\n ! [Overall Diagram](http://www.iocoder.cn/images/Dubbo/2020-04-15/01.png)\\n\\n **\u2462 Spring Cloud**\\n\\n The [\\"3. AT Patterns + Feign\\"](#) subsection of [\\"Getting Started with Alibaba Distributed Transaction Seata for Taro Road Spring Cloud\\"](http://www.iocoder.cn/Spring-Cloud-Alibaba/Seata/?self) implements multiple Spring Cloud services.\\n\\n ! [Overall diagram](http://www.iocoder.cn/images/Spring-Cloud/2020-07-15/01.png)\\n\\n ## 4.2 TCC Pattern\\n\\n - Documentation: [\\"Seata Documentation -- TCC Mode\\"](/docs/dev/mode/tcc-mode/)\\n - Example: https://github.com/apache/incubator-seata-samples/blob/master/tcc\\n\\n ## 4.3 Saga mode\\n\\n - Documentation: [\\"Seata Documentation -- Saga Mode\\"](/docs/dev/mode/saga-mode/)\\n - Example: https://github.com/apache/incubator-seata-samples/tree/master/saga\\n\\n ## 4.4 XA mode\\n\\n Seata is under development..."},{"id":"/seata-ha-practice","metadata":{"permalink":"/blog/seata-ha-practice","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-ha-practice.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-ha-practice.md","title":"Seata High Availability Deployment Practice","description":"Seata High Availability Deployment Practice","date":"2020-04-10T00:00:00.000Z","formattedDate":"April 10, 2020","tags":[],"readingTime":6.06,"hasTruncateMarker":false,"authors":[{"name":"helloworlde"}],"frontMatter":{"hidden":true,"title":"Seata High Availability Deployment Practice","keywords":["kubernetes","ops"],"description":"Seata High Availability Deployment Practice","author":"helloworlde","date":"2020-04-10T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Seata Quick Start","permalink":"/blog/seata-quick-start"},"nextItem":{"title":"Seata Config Module Source Code Analysis","permalink":"/blog/seata-analysis-config-modular"}},"content":"To make Seata highly available using a configuration centre and database, take Nacos and MySQL as an example and deploy the [cloud-seata-nacos](https://github.com/helloworlde/spring-cloud-alibaba-component/blob/ master/cloud-seata-nacos/) application to a Kubernetes cluster.\\n\\nThe application uses Nacos as the configuration and registration centre, and has three services: order-service, pay-service, and storage-service. The order-service provides an interface for placing an order, and when the balance and inventory are sufficient, the order succeeds and a transaction is submitted; when they are insufficient, an exception is thrown, the order fails, and the transaction is rolled back. Rollback transaction\\n\\n## Preparation\\n\\nYou need to prepare available registry, configuration centre Nacos and MySQL, usually, the registry, configuration centre and database are already available and do not need special configuration, in this practice, for simplicity, only deploy a stand-alone registry, configuration centre and database, assuming they are reliable\\n\\n- Deploying Nacos\\n\\nDeploy Nacos on a server with port 8848 open for seata-server registration at ``192.168.199.2``.\\n\\n```bash\\ndocker run --name nacos -p 8848:8848 -e MODE=standalone nacos/nacos-server\\n ```\\n\\n - Deploying MySQL\\n\\n Deploy a MySQL database to hold transaction data at ``192.168.199.2``.\\n\\n ```bash\\n docker run --name mysql -p 30060:3306-e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7.17\\n ```\\n\\n ## Deploying seata-server\\n\\n - Create the tables needed for seata-server.\\n\\n Refer to [script/server/db](https://github.com/apache/incubator-seata/tree/develop/script/server/db) for the exact SQL, here we are using MySQL\'s script and the database name is `seata`.\\n\\n You also need to create the undo_log table, see [script/client/at/db/](https://github.com/apache/incubator-seata/blob/develop/script/client/at/db/).\\n\\n - Modify the seata-server configuration\\n\\n Add the following configuration to the Nacos Configuration Centre, as described in [script/config-center](https://github.com/apache/incubator-seata/tree/develop/script/config-center)\\n\\n ```\\n service.vgroupMapping.my_test_tx_group=default\\n store.mode=db\\n store.db.datasource=druid\\n store.db.dbType=mysql\\n store.db.driverClassName=com.mysql.jdbc.\\n store.db.url=jdbc:mysql://192.168.199.2:30060/seata?useUnicode=true\\n store.db.user=root\\n store.db.password=123456\\n ```\\n\\n### Deploying seata-server to Kubernetes\\n\\n- seata-server.yaml\\n\\nYou need to change the ConfigMap\'s Registry and Configuration Centre addresses to the appropriate addresses\\n\\n ```yaml\\n apiVersion: v1\\n kind: Service\\n metadata: name: seata-ha-server.yaml\\n  name: seata-ha-server\\n  namespace: default\\n  labels: app.kubernetes.io/name: seata-ha-server\\n    app.kubernetes.io/name: seata-ha-server\\n spec.\\n  type: ClusterIP\\n  spec: type: ClusterIP\\n    - port: 8091\\n      protocol: TCP\\n      name: http\\n  selector: app.kubernetes.io/name: seata-ha-server\\n    app.kubernetes.io/name: seata-ha-server\\n\\n ---apiVersion: apps/v1\\n\\n apiVersion: apps/v1\\n kind: StatefulSet\\n metadata.\\n  name: seata-ha-server\\n  namespace: default\\n  labels: app.kubernetes.io/name: seata-ha-server\\n    app.kubernetes.io/name: seata-ha-server\\n spec: serviceName: seata-ha-server\\n  serviceName: seata-ha-server\\n  replicas: 3\\n  selector: seata-ha-server\\n    matchLabels.\\n      app.kubernetes.io/name: seata-ha-server\\n  template: seata-ha-server\\n    metadata.\\n      labels: app.kubernetes.io/name: seata-ha-server\\n        app.kubernetes.io/name: seata-ha-server\\n    spec.\\n      containers: name: seata-ha-server\\n        - name: seata-ha-server\\n          image: docker.io/seataio/seata-server:latest\\n          imagePullPolicy: IfNotPresent\\n          env: name: SEATA_CONFIG\\n            - name: SEATA_CONFIG_NAME\\n              value: file:/root/seata-config/registry\\n          ports: name: http\\n            - name: http\\n              containerPort: 8091\\n              protocol: TCP\\n          volumeMounts: name: seata-config\\n            - name: seata-config\\n              mountPath: /root/seata-config\\n      volumes: name: seata-config mountPath: /root/seata-config\\n        - name: seata-config\\n          configMap: name: seata-ha-server-config\\n            name: seata-ha-server-config\\n\\n\\n ---apiVersion: v1\\n apiVersion: v1\\n kind: ConfigMap\\n apiVersion: v1 kind: ConfigMap\\n  name: seata-ha-server-config\\n data: name: seata-ha-server-config\\n  registry.conf: |\\n    registry {\\n        type = \\"nacos\\"\\n        nacos {\\n          application = \\"seata-server\\"\\n          serverAddr = \\"192.168.199.2\\"\\n        }\\n    }\\n    config {\\n      type = \\"nacos\\"\\n      nacos {\\n        serverAddr = \\"192.168.199.2\\"\\n        group = \\"SEATA_GROUP\\"\\n      }\\n    }\\n ```\\n\\n- Deployment\\n\\n ```bash\\n kubectl apply -f seata-server.yaml\\n ```\\n\\nWhen the deployment is complete, there will be three pods\\n\\n ```bash\\n kubectl get pod | grep seata-ha-server\\n\\n seata-ha-server-645844b8b6-9qh5j 1/1 Running 0 3m14s\\n seata-ha-server-645844b8b6-pzczs 1/1 Running 0 3m14s\\n seata-ha-server-645844b8b6-wkpw8 1/1 Running 0 3m14s\\n ```\\n\\nAfter the startup is complete, you can find three instances of seata-server in the Nacos service list, so you have completed the highly available deployment of seata-server.\\n\\n- Viewing the service logs\\n\\n ```bash\\n kubelet logs -f seata-ha-server-645844b8b6-9qh5j\\n ```\\n\\n ```java\\n [0.012s][info ][gc] Using Serial\\n 2020-04-15 00:55:09.880 INFO [main]io.seata.server.ParameterParser.init:90 -The server is running in container.\\n 2020-04-15 00:55:10.013 INFO [main]io.seata.config.FileConfiguration.<init>:110 -The configuration file used is file:/root/seata- config/registry.conf\\n 2020-04-15 00:55:12.426 INFO [main]com.alibaba.druid.pool.DruidDataSource.init:947 -{dataSource-1} inited\\n 2020-04-15 00:55:13.127 INFO [main]io.seata.core.rpc.netty.RpcServerBootstrap.start:155 -Server started\\n ```\\n\\nwhere `{dataSource-1} ` indicates that the database is used and initialised properly\\n\\n- Looking at the registry, there are three instances of the seata-serve service at this point\\n\\n! [seata-ha-nacos-list.png](/img/blog/seata-ha-nacos-list.png)\\n\\n\\n## Deploying the business service\\n\\n- Create business tables and initialise data\\n\\nYou can refer to [cloud-seata-nacos/README.md](https://github.com/helloworlde/spring-cloud-alibaba-component/blob/master/cloud-seata- nacos/README.md).\\n\\n- Adding Nacos Configuration\\n\\nUnder the public namespace, create the configurations with data-id `order-service.properties`, `pay-service.properties`, `storage-service.properties`, with the same content. password\\n\\n```\\n# MySQL\\nspring.datasource.url=jdbc:mysql://192.168.199.2:30060/seata?useUnicode=true&characterEncoding=utf8&allowMultiQueries=true &useSSL=false\\nspring.datasource.username=root\\nspring.datasource.password=123456\\nspring.datasource.driver-class-name=com.mysql.cj.jdbc.\\n# Seata\\nspring.cloud.alibaba.seata.tx-service-group=my_test_tx_group\\n ```\\n\\n - Deploying the Service\\n\\n Deploy the service via the application.yaml configuration file, and note that you need to change the `NACOS_ADDR` of the ConfigMap to your Nacos address.\\n\\n ```yaml\\n apiVersion: v1\\n kind: Service\\n metadata: namespace: default\\n  namespace: default\\n  name: seata-ha-service\\n  labels: app.kubernetes.io/name: seata-ha-service\\n    app.kubernetes.io/name: seata-ha-service\\n spec.\\n  type: NodePort\\n  spec: type: NodePort\\n    - nodePort: 30081\\n      nodePort: 30081\\n      protocol: TCP\\n      name: http\\n  selector: app.kubernetes.io/name: seata-ha-service\\n    app.kubernetes.io/name: seata-ha-service\\n\\n ---\\n apiVersion: v1\\n kind: ConfigMap\\n metadata: name: seata-ha-service-config\\n  name: seata-ha-service-config\\n data: NACOS_ADDR: 192.168.199.2:8848\\n  NACOS_ADDR: 192.168.199.2:8848\\n\\n ---apiVersion: v1\\n apiVersion: v1\\n kind: ServiceAccount\\n metadata: name: seata-ha-account\\n  name: seata-ha-account\\n  namespace: default\\n\\n ---apiVersion: rbac.authorisation.k8s.io/v1beta1\\n apiVersion: rbac.authorisation.k8s.io/v1beta1\\n kind: ClusterRoleBinding\\n metadata: name: seata-ha-account\\n  name: seata-ha-account\\n roleRef.\\n  apiGroup: rbac.authorisation.k8s.io/v1beta1 kind: ClusterRoleBinding\\n  roleRef: apiGroup: rbac.authorisation.k8s.io\\n  name: cluster-admin\\n subjects.\\n  - kind: ServiceAccount\\n    name: seata-ha-account\\n    namespace: default\\n\\n ---\\n apiVersion: apps/v1\\n kind: Deployment\\n namespace: default --- --- apiVersion: apps/v1 kind: Deployment\\n  namespace: default\\n  name: seata-ha-service\\n  labels: app.kubernetes.io/name: seata-ha-service\\n    app.kubernetes.io/name: seata-ha-service\\n spec: replicas: 1\\n  replicas: 1\\n  selector.\\n    matchLabels: app.kubernetes.io/name: seata-ha-service\\n      app.kubernetes.io/name: seata-ha-service\\n  template: seata-ha-service\\n    metadata: seata-ha-service template.\\n      labels: app.kubernetes.io/name: seata-ha-service\\n        app.kubernetes.io/name: seata-ha-service\\n    spec: serviceAccountName: seata-ha-service\\n      serviceAccountName: seata-ha-account\\n      containers: name: seata-ha-order\\n        - name: seata-ha-order-service\\n          image: \\"registry.cn-qingdao.aliyuncs.com/hellowoodes/seata-ha-order-service:1.1\\"\\n          imagePullPolicy: IfNotPresent\\n          imagePullPolicy: IfNotPresent\\n            - name: NACOS_ADDR\\n              valueFrom.\\n                configMapKeyRef.\\n                  key: NACOS_ADDR\\n                  name: seata-ha-service-config\\n          name: seata-ha-service-config\\n            - name: http\\n              containerPort: 8081\\n              protocol: TCP\\n        - name: seata-ha-pay-service\\n          image: \\"registry.cn-qingdao.aliyuncs.com/hellowoodes/seata-ha-pay-service:1.1\\"\\n          imagePullPolicy: IfNotPresent\\n          env.\\n            - name: NACOS_ADDR\\n              valueFrom.\\n                configMapKeyRef.\\n                  key: NACOS_ADDR\\n                  name: seata-ha-service-config\\n          name: seata-ha-service-config\\n            - name: http\\n              containerPort: 8082\\n              protocol: TCP\\n        - name: seata-ha-storage-service\\n          image: \\"registry.cn-qingdao.aliyuncs.com/hellowoodes/seata-ha-storage-service:1.1\\"\\n          imagePullPolicy: IfNotPresent\\n          env.\\n            - name: NACOS_ADDR\\n              valueFrom.\\n                NACOS_ADDR valueFrom: NACOS_ADDR valueFrom: NACOS_ADDR valueFrom.\\n                  key: NACOS_ADDR\\n                  name: seata-ha-service-config\\n          name: seata-ha-service-config\\n            - name: http\\n              containerPort: 8083\\n              protocol: TCP\\n ```\\n\\nDeploy the application to the cluster with the following command\\n\\n ```bash\\n kubectl apply -f application.yaml\\n ```\\n\\nThen look at the pods that were created, there are three pods under the seata-ha-service service\\n\\n ```bash\\n kubectl get pod | grep seata-ha-service\\n\\n seata-ha-service-7dbdc6894b-5r8q4 3/3 Running 0 12m\\n ```\\n\\n Once the application is up and running, in the Nacos service list, there will be the corresponding service\\n\\n ! [seata-ha-service-list.png](/img/blog/seata-ha-service-list.png)\\n\\n At this point, if you look at the service\'s logs, you will see that the service has registered with each of the TC\'s\\n\\n ```bash\\n kubectl logs -f seata-ha-service-7dbdc6894b-5r8q4 seata-ha-order-service\\n ```\\n\\n ! [seata-ha-service-register.png](/img/blog/seata-ha-service-register.png)\\n\\n Looking at any TC log, you\'ll see that each service is registered with the TC\\n\\n ```bash\\n kubelet logs -f seata-ha-server-645844b8b6-9qh5j\\n ```\\n\\n! [seata-ha-tc-register.png](/img/blog/seata-ha-tc-register.png)\\n\\n\\n## Test\\n\\n\\n### Test Success Scenario\\n\\nCall the order interface, set the price to 1, because the initial balance is 10, and the order is placed successfully.\\n\\n```bash\\ncurl -X POST \\\\\\nhttp://192.168.199.2:30081/order/placeOrder \\\\\\n-H \'Content-Type: application/json\' \\\\\\n-d \'{\\n\\"userId\\": 1,\\n\\"productId\\": 1,\\n\\"price\\": 1\\n}\'\\n ```\\n\\n At this point the return result is:\\n\\n ```json\\n {\\"success\\":true, \\"message\\":null, \\"data\\":null}\\n ```\\n\\nChecking the TC logs, the transaction was successfully committed:\\n\\n! [seata-ha-commit-tc-success.png](/img/blog/seata-ha-commit-tc-success.png)\\n\\nView the order-service service log\\n! [seata-ha-commit-success.png](/img/blog/seata-ha-commit-service-success.png)\\n\\n\\n### Test failure scenario\\n\\nIf you set the price to 100 and the balance is not enough, the order fails and throws an exception, and the transaction is rolled back.\\n\\n```bash\\ncurl -X POST \\\\\\nhttp://192.168.199.2:30081/order/placeOrder \\\\\\n-H \'Content-Type: application/json\' \\\\\\n-d \'{\\n\\"userId\\": 1,\\n\\"productId\\": 1,\\n\\"price\\": 100\\n}\'\\n ```\\n\\n View the logs for TC:\\n ! [seata-ha-commit-tc-rollback.png](/img/blog/seata-ha-commit-tc-rollback.png)\\n\\n View the logs of the service :\\n ! [seata-ha-commit-service-rollback.png](/img/blog/seata-ha-commit-service-rollback.png)\\n\\n Multiple calls to view the service logs reveal that transaction registration is randomly initiated to one of the T Cs, and when capacity is expanded or scaled down, the corresponding TC participates or withdraws, proving that the high availability deployment is in effect."},{"id":"/seata-analysis-config-modular","metadata":{"permalink":"/blog/seata-analysis-config-modular","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-config-modular.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-config-modular.md","title":"Seata Config Module Source Code Analysis","description":"1. Introduction","date":"2020-01-11T00:00:00.000Z","formattedDate":"January 11, 2020","tags":[],"readingTime":5.52,"hasTruncateMarker":false,"authors":[{"name":"runze.zhao"}],"frontMatter":{"title":"Seata Config Module Source Code Analysis","author":"runze.zhao","keywords":["Seata","distributed transaction"],"date":"2020/1/11"},"unlisted":false,"prevItem":{"title":"Seata High Availability Deployment Practice","permalink":"/blog/seata-ha-practice"},"nextItem":{"title":"Source Code Analysis of Seata-XID Propagation in Dubbo","permalink":"/blog/seata-analysis-dubbo-transmit-xid"}},"content":"## 1. Introduction\\nAccording to the classification defined by [experts](https://www.iteye.com/blog/javatar-949527), configurations can be categorized into three types: environment configuration, descriptive configuration, and extension configuration.\\n\\n- Environment configuration: Typically consists of discrete simple values like parameters for component startup, often in key-value pair format.\\n- Descriptive configuration: Pertains to business logic, such as transaction initiators and participants, and is usually embedded within the lifecycle management of the business. Descriptive configuration contains more information, sometimes with hierarchical relationships.\\n- Extension configuration: Products need to discover third-party implementations, requiring high aggregation of configurations. Examples include various configuration centers and registration centers. The common practice is to place the fully qualified class name files under the META-INF/services directory of the JAR file, with each line representing an implementation class name.\\n\\n## 2. Environment Configuration\\n\\nWhen the Seata server is loaded, it uses `resources/registry.conf` to determine the types of configuration centers and registration centers. Starting from version 1.0, Seata client not only loads configurations using the `conf` file but also allows configuration through YAML files in Spring Boot using `seata.config.{type}` for choosing the configuration center, similar to selecting the registration center. The source code for loading configurations via YAML is located in the `io.seata.spring.boot.autoconfigure.properties.registry` package.\\n\\nIf the user of the Seata client places both a `conf` configuration file under `resources` and configures via YAML files, the configuration in the YAML file will take precedence. Code example:\\n\\n\\n```java\\nCURRENT_FILE_INSTANCE = null == extConfiguration ? configuration : extConfiguration;\\n```\\n\\nHere, `extConfiguration` is an instance of external configuration provided by the `ExtConfigurationProvider#provide()` external configuration provider class, while `configuration` is provided by another configuration provider class, `ConfigurationProvider#provide()`. These two configuration provider classes are loaded through SPI in the static block of the `ConfigurationFactory` in the config module.\\n\\n\\n```java\\nEnhancedServiceLoader.load(ExtConfigurationProvider.class).provide(configuration);\\n```\\n\\nThe selection of configuration center types discussed above is related to determining the configuration environment. Once the type of configuration center to be used is determined, the environment configuration is loaded through the corresponding configuration center. File-based configuration, represented by `File`, is also considered a type of configuration center.\\n\\nBoth the client and server obtain configuration parameters by using `ConfigurationFactory#getInstance()` to get an instance of the configuration class, and then retrieve configuration parameters using the instance. The constants defining configuration keys are mainly found in the `config` file under the `core` module.\\n\\nThe meanings of some important environment configuration properties are documented on the [official website](/docs/user/configurations/).\\n\\nDuring instantiation, the configuration parameters obtained through `ConfigurationFactory` and injected into constructors require a restart to take effect. However, parameters obtained in real-time using `ConfigurationFactory` become effective immediately when the configuration changes.\\n\\nThe `config` module provides the `ConfigurationChangeListener#onChangeEvent` interface method to modify internal attributes of instances. In this method, dynamic changes to properties are monitored, and if the properties used by the instance are found to have changed from the initial injection, the attributes stored in the instance are modified to align with the configuration center. This enables dynamic configuration updates.\\n\\n\\n```java\\npublic class GlobalTransactionalInterceptor implements ConfigurationChangeListener {\\nprivate volatile boolean disable = ConfigurationFactory.getInstance().getBoolean(ConfigurationKeys.DISABLE_GLOBAL_TRANSACTION,false);\\n@Override public Object invoke(Param param) {\\n   if(disable){//Transaction business processing}\\n}\\n@Override public void onChangeEvent(Param param) {\\n   disable = param;\\n}}\\n```\\n\\nThe code snippet above pertains to the pseudo-code related to the `GlobalTransactionalInterceptor` and its degradation properties under the Spring module. When the `GlobalTransactionalScanner` instantiates the interceptor class mentioned above, it registers the interceptor into the list of configuration change listeners. When a configuration change occurs, the listener is invoked:\\n\\n\\n```java\\nConfigurationFactory.getInstance().addConfigListener(ConfigurationKeys.DISABLE_GLOBAL_TRANSACTION,(ConfigurationChangeListener)interceptor);\\n```\\n\\nThe term \\"degradation\\" refers to the scenario where a particular functionality of a service becomes unavailable. By dynamically configuring properties, this functionality can be turned off to avoid continuous attempts and failures. The `interceptor#invoke()` method executes Seata transaction-related business only when the `disable` attribute is set to true.\\n\\n## 3. Descriptive Configuration\\nDescriptive configurations in general frameworks often contain abundant information, sometimes with hierarchical relationships. XML configuration is convenient for describing tree structures due to its strong descriptive capabilities. However, the current trend advocates for eliminating cumbersome prescriptive configurations in favor of using conventions.\\n\\nIn Seata\'s AT (Automatic Transaction) mode, transaction processing is achieved through proxying data sources, resulting in minimal intrusion on the business logic. Simply identifying which business components need to enable global transactions during Seata startup can be achieved using annotations, thus facilitating descriptive configuration.\\n\\n\\n```java\\n@GlobalTransactional(timeoutMills = 300000, name = \\"busi-doBiz\\")\\npublic String doBiz(String msg) {}\\n```\\nIf using the TCC (Try-Confirm-Cancel) mode, transaction participants also need to annotate their involvement:\\n\\n\\n```java\\n@TwoPhaseBusinessAction(name = \\"tccActionForSpringTest\\" , commitMethod = \\"commit\\", rollbackMethod = \\"rollback\\")\\npublic boolean prepare(BusinessActionContext actionContext, int i);\\npublic boolean commit(BusinessActionContext actionContext);\\npublic boolean rollback(BusinessActionContext actionContext);\\n```\\n\\n## 4. Extension Configuration\\nExtension configurations typically have high requirements for product aggregation because products need to discover third-party implementations and incorporate them into their internals.\\n\\n![Image Description](https://img-blog.csdnimg.cn/20200110213751452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3ODA0NzM3,size_16,color_FFFFFF,t_70)\\n\\nHere\'s an example of a custom configuration center provider class. Place a text file with the same name as the interface under META-INF/services, and the content of the file should be the implementation class of the interface. This follows the standard SPI (Service Provider Interface) approach. Then, modify the configuration file `registry.conf` to set `config.type=test`.\\n\\nHowever, if you think that by doing so, Seata can recognize it and replace the configuration center, then you are mistaken. When Seata loads the configuration center, it encapsulates the value of the configuration center type specified in the configuration file using the enum `ConfigType`:\\n\\n\\n```java\\nprivate static Configuration buildConfiguration() {\\n   configTypeName = \\"test\\";//The \'config.type\' configured in \'registry.conf\\n   configType = ConfigType.getType(configTypeName);//An exception will be thrown if ConfigType cannot be retrieved.\\n}\\n```\\n\\nIf a configuration center type like `test` is not defined in `ConfigType`, it will throw an exception. Therefore, merely modifying the configuration file without changing the source code will not enable the use of configuration center provider classes other than those defined in `ConfigType`.\\n\\nCurrently, in version 1.0, the configuration center types defined in `ConfigType` include: File, ZK, Nacos, Apollo, Consul, Etcd3, SpringCloudConfig, and Custom. If a user wishes to use a custom configuration center type, they can use the `Custom` type.\\n\\n![Image Description](https://img-blog.csdnimg.cn/20200110215249152.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3ODA0NzM3,size_16,color_FFFFFF,t_70)\\n\\nOne inelegant approach here is to provide an implementation class with a specified name `ZK` but with a higher priority level (order=3) than the default `ZK` implementation (which has order=1). This will make `ConfigurationFactory` use `TestConfigurationProvider` as the configuration center provider class.\\n\\nThrough the above steps, Seata can be configured to use our own provided code. Modules in Seata such as codec, compressor, discovery, integration, etc., all use the SPI mechanism to load functional classes, adhering to the design philosophy of microkernel + plug-in, treating third parties equally.\\n\\n## 5. Seata Source Code Analysis Series\\nAuthor: Zhao Runze, [Series Address](https://blog.csdn.net/qq_37804737/category_9530078.html)."},{"id":"/seata-analysis-dubbo-transmit-xid","metadata":{"permalink":"/blog/seata-analysis-dubbo-transmit-xid","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-dubbo-transmit-xid.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-dubbo-transmit-xid.md","title":"Source Code Analysis of Seata-XID Propagation in Dubbo","description":"This article explores the propagation of XID in Seata-Dubbo through source code analysis.","date":"2020-01-01T00:00:00.000Z","formattedDate":"January 1, 2020","tags":[],"readingTime":2.22,"hasTruncateMarker":false,"authors":[{"name":"FUNKYE"}],"frontMatter":{"title":"Source Code Analysis of Seata-XID Propagation in Dubbo","keywords":["Seata","Dubbo","distributed transaction","spring"],"description":"This article explores the propagation of XID in Seata-Dubbo through source code analysis.","author":"FUNKYE","date":"2020/01/01"},"unlisted":false,"prevItem":{"title":"Seata Config Module Source Code Analysis","permalink":"/blog/seata-analysis-config-modular"},"nextItem":{"title":"Seata TCC Module Source Code Analysis","permalink":"/blog/seata-analysis-tcc-modular"}},"content":"Author: FUNKYE (Chen Jianbin), Principal Engineer at a certain Internet company in Hangzhou.\\n\\n# Preface\\n\\n1. Let\'s start by examining the package structure. Under seata-dubbo and seata-dubbo-alibaba, there is a common class named TransactionPropagationFilter, corresponding to Apache Dubbo and Alibaba Dubbo respectively.\\n\\n![20200101203229](/img/blog/20200101203229.png)\\n\\n## Source Code Analysis\\n\\n```java\\npackage io.seata.integration.dubbo;\\n\\nimport io.seata.core.context.RootContext;\\nimport org.apache.dubbo.common.Constants;\\nimport org.apache.dubbo.common.extension.Activate;\\nimport org.apache.dubbo.rpc.Filter;\\nimport org.apache.dubbo.rpc.Invocation;\\nimport org.apache.dubbo.rpc.Invoker;\\nimport org.apache.dubbo.rpc.Result;\\nimport org.apache.dubbo.rpc.RpcContext;\\nimport org.apache.dubbo.rpc.RpcException;\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\n\\n@Activate(group = {Constants.PROVIDER, Constants.CONSUMER}, order = 100)\\npublic class TransactionPropagationFilter implements Filter {\\n\\n    private static final Logger LOGGER = LoggerFactory.getLogger(TransactionPropagationFilter.class);\\n\\n    @Override\\n    public Result invoke(Invoker<?> invoker, Invocation invocation) throws RpcException {\\n        // get local XID\\n        String xid = RootContext.getXID();\\n        String xidInterceptorType = RootContext.getXIDInterceptorType();\\n        // get XID from dubbo param\\n        String rpcXid = getRpcXid();\\n        String rpcXidInterceptorType = RpcContext.getContext().getAttachment(RootContext.KEY_XID_INTERCEPTOR_TYPE);\\n        if (LOGGER.isDebugEnabled()) {\\n            LOGGER.debug(\\"xid in RootContext[{}] xid in RpcContext[{}]\\", xid, rpcXid);\\n        }\\n        boolean bind = false;\\n        if (xid != null) {\\n            //transfer xid\\n            RpcContext.getContext().setAttachment(RootContext.KEY_XID, xid);\\n            RpcContext.getContext().setAttachment(RootContext.KEY_XID_INTERCEPTOR_TYPE, xidInterceptorType);\\n        } else {\\n            if (rpcXid != null) {\\n                //bind XID\\n                RootContext.bind(rpcXid);\\n                RootContext.bindInterceptorType(rpcXidInterceptorType);\\n                bind = true;\\n                if (LOGGER.isDebugEnabled()) {\\n                    LOGGER.debug(\\"bind[{}] interceptorType[{}] to RootContext\\", rpcXid, rpcXidInterceptorType);\\n                }\\n            }\\n        }\\n        try {\\n            return invoker.invoke(invocation);\\n        } finally {\\n            if (bind) {\\n                //remove xid which has finished\\n                String unbindInterceptorType = RootContext.unbindInterceptorType();\\n                String unbindXid = RootContext.unbind();\\n                if (LOGGER.isDebugEnabled()) {\\n                    LOGGER.debug(\\"unbind[{}] interceptorType[{}] from RootContext\\", unbindXid, unbindInterceptorType);\\n                }\\n                // if unbind xid is not current rpc xid\\n                if (!rpcXid.equalsIgnoreCase(unbindXid)) {\\n                    LOGGER.warn(\\"xid in change during RPC from {} to {}, xidInterceptorType from {} to {} \\", rpcXid, unbindXid, rpcXidInterceptorType, unbindInterceptorType);\\n                    if (unbindXid != null) {\\n                        // bind xid\\n                        RootContext.bind(unbindXid);\\n                        RootContext.bindInterceptorType(unbindInterceptorType);\\n                        LOGGER.warn(\\"bind [{}] interceptorType[{}] back to RootContext\\", unbindXid, unbindInterceptorType);\\n                    }\\n                }\\n            }\\n        }\\n    }\\n\\n    /**\\n     * get rpc xid\\n     * @return\\n     */\\n    private String getRpcXid() {\\n        String rpcXid = RpcContext.getContext().getAttachment(RootContext.KEY_XID);\\n        if (rpcXid == null) {\\n            rpcXid = RpcContext.getContext().getAttachment(RootContext.KEY_XID.toLowerCase());\\n        }\\n        return rpcXid;\\n    }\\n\\n}\\n```\\n\\n\\n1. Based on the source code, we can deduce the corresponding logic processing.\\n\\n\\n![20200101213336](/img/blog/20200101213336.png)\\n\\n## Key Points\\n\\n1. Dubbo @Activate Annotation:\\n\\n```java\\n@Documented\\n@Retention(RetentionPolicy.RUNTIME)\\n@Target({ElementType.TYPE, ElementType.METHOD})\\npublic @interface Activate {\\n\\n   \\n    String[] group() default {};\\n\\n    \\n    String[] value() default {};\\n\\n    \\n    String[] before() default {};\\n\\n   \\n    String[] after() default {};\\n\\n   \\n    int order() default 0;\\n}\\n```\\nIt can be analyzed that the @Activate annotation on Seata\'s Dubbo filter, with parameters @Activate(group = \\\\{Constants.PROVIDER, Constants.CONSUMER}, order = 100), indicates that both the Dubbo service provider and consumer will trigger this filter. Therefore, our Seata initiator will initiate an XID transmission. The above flowchart and code have clearly represented this.\\n\\n2. Dubbo implicit parameter passing can be achieved through setAttachment and getAttachment on RpcContext for implicit parameter transmission between service consumers and providers.\\n\\nFetching: RpcContext.getContext().getAttachment(RootContext.KEY_XID);\\n\\nPassing: RpcContext.getContext().setAttachment(RootContext.KEY_XID, xid);\\n\\n# Conclusion\\n\\nFor further source code reading, please visit the [Seata official website](https://seata.apache.org/)"},{"id":"/seata-analysis-tcc-modular","metadata":{"permalink":"/blog/seata-analysis-tcc-modular","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-tcc-modular.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-tcc-modular.md","title":"Seata TCC Module Source Code Analysis","description":"\u4e00. Introduction","date":"2019-12-25T00:00:00.000Z","formattedDate":"December 25, 2019","tags":[],"readingTime":7.965,"hasTruncateMarker":false,"authors":[{"name":"runze.zhao"}],"frontMatter":{"title":"Seata TCC Module Source Code Analysis","author":"runze.zhao","keywords":["Seata","distributed transaction"],"date":"2019/12/25"},"unlisted":false,"prevItem":{"title":"Source Code Analysis of Seata-XID Propagation in Dubbo","permalink":"/blog/seata-analysis-dubbo-transmit-xid"},"nextItem":{"title":"Seata Community Meetup\xb7Hangzhou Station","permalink":"/blog/seata-community-meetup-hangzhou-ready"}},"content":"## \u4e00. Introduction\\n\\nIn the analysis of the Spring module, it is noted that Seata\'s Spring module handles beans involved in distributed transactions. Upon project startup, when the `GlobalTransactionalScanner` detects references to TCC services (i.e., TCC transaction participants), it dynamically proxies them by weaving in the implementation class of `MethodInterceptor` under the TCC mode. The initiator of the TCC transaction still uses the `@GlobalTransactional` annotation to initiate it, and a generic implementation class of `MethodInterceptor` is woven in.\\n\\nThe implementation class of `MethodInterceptor` under the TCC mode is referred to as `TccActionInterceptor` (in the Spring module). This class invokes `ActionInterceptorHandler` (in the TCC module) to handle the transaction process under the TCC mode.\\n\\nThe primary functions of TCC dynamic proxy are: generating the TCC runtime context, propagating business parameters, and registering branch transaction records.\\n\\n## \u4e8c. Introduction to TCC Mode\\n\\nIn the Two-Phase Commit (2PC) protocol, the transaction manager coordinates resource management in two phases. The resource manager provides three operations: the prepare operation in the first phase, and the commit operation and rollback operation in the second phase.\\n\\n\\n```java\\npublic interface TccAction {\\n\\n    @TwoPhaseBusinessAction(name = \\"tccActionForTest\\" , commitMethod = \\"commit\\", rollbackMethod = \\"rollback\\")\\n    public boolean prepare(BusinessActionContext actionContext,\\n                           @BusinessActionContextParameter(paramName = \\"a\\") int a,\\n                           @BusinessActionContextParameter(paramName = \\"b\\", index = 0) List b,\\n                           @BusinessActionContextParameter(isParamInProperty = true) TccParam tccParam);\\n\\n    public boolean commit(BusinessActionContext actionContext);\\n    \\n    public boolean rollback(BusinessActionContext actionContext);\\n}\\n```\\n\\nThis is a participant instance in TCC. Participants need to implement three methods, where the first parameter must be BusinessActionContext, and the return type of the methods is fixed. These methods are exposed as microservices to be invoked by the transaction manager.\\n\\n- prepare: Checks and reserves resources. For example, deducting the account balance and increasing the same frozen balance.\\n- commit: Uses the reserved resources to complete the actual business operation. For example, reducing the frozen balance to complete the fund deduction business.\\n- cancel: Releases the reserved resources. For example, adding back the frozen balance to the account balance.\\n\\nThe BusinessActionContext encapsulates the context environment of the current transaction: xid, branchId, actionName, and parameters annotated with @BusinessActionContextParam.\\n\\nThere are several points to note in participant business:\\n1. Ensure business idempotence, supporting duplicate submission and rollback of the same transaction.\\n2. Prevent hanging, i.e., the rollback of the second phase occurs before the try phase.\\n3. Relax consistency protocols, eventually consistent, so it is read-after-write.\\n\\n##  Three. Remoting package analysis\\n\\n![Remoting Package Analysis](https://img-blog.csdnimg.cn/20191124211806237.png?)\\n\\nAll classes in the package serve DefaultRemotingParser. Dubbo, LocalTCC, and SofaRpc are responsible for parsing classes under their respective RPC protocols.\\n\\nMain methods of DefaultRemotingParser:\\n1. Determine if the bean is a remoting bean, code: \\n\\n\\n```java\\n    @Override\\n    public boolean isRemoting(Object bean, String beanName) throws FrameworkException {\\n        //\u5224\u65ad\u662f\u5426\u662f\u670d\u52a1\u8c03\u7528\u65b9\u6216\u8005\u662f\u5426\u662f\u670d\u52a1\u63d0\u4f9b\u65b9\\n        return isReference(bean, beanName) || isService(bean, beanName);\\n    }\\n```\\n2. Remote bean parsing, parses rpc classes into RemotingDesc.\\n\\nCode:\\n\\n\\n```java\\n@Override\\n    public boolean isRemoting(Object bean, String beanName) throws FrameworkException {\\n        //\u5224\u65ad\u662f\u5426\u662f\u670d\u52a1\u8c03\u7528\u65b9\u6216\u8005\u662f\u5426\u662f\u670d\u52a1\u63d0\u4f9b\u65b9\\n        return isReference(bean, beanName) || isService(bean, beanName);\\n    }\\n```\\n\\nUtilize allRemotingParsers to parse remote beans. allRemotingParsers is dynamically loaded in initRemotingParser() by calling EnhancedServiceLoader.loadAll(RemotingParser.class), which implements the SPI loading mechanism for loading subclasses of RemotingParser.\\n\\nFor extension purposes, such as implementing a parser for feign remote calls, simply write the relevant implementation classes of RemotingParser in the SPI configuration. This approach offers great extensibility.\\n\\nRemotingDesc contains specific information about remote beans required for the transaction process, such as targetBean, interfaceClass, interfaceClassName, protocol, isReference, and so on.\\n\\n3. TCC Resource Registration\\n\\n\\n```java\\npublic RemotingDesc parserRemotingServiceInfo(Object bean, String beanName) {\\n        RemotingDesc remotingBeanDesc = getServiceDesc(bean, beanName);\\n        if (remotingBeanDesc == null) {\\n            return null;\\n        }\\n        remotingServiceMap.put(beanName, remotingBeanDesc);\\n\\n        Class<?> interfaceClass = remotingBeanDesc.getInterfaceClass();\\n        Method[] methods = interfaceClass.getMethods();\\n        if (isService(bean, beanName)) {\\n            try {\\n                //service bean, registry resource\\n                Object targetBean = remotingBeanDesc.getTargetBean();\\n                for (Method m : methods) {\\n                    TwoPhaseBusinessAction twoPhaseBusinessAction = m.getAnnotation(TwoPhaseBusinessAction.class);\\n                    if (twoPhaseBusinessAction != null) {\\n                        TCCResource tccResource = new TCCResource();\\n                        tccResource.setActionName(twoPhaseBusinessAction.name());\\n                        tccResource.setTargetBean(targetBean);\\n                        tccResource.setPrepareMethod(m);\\n                        tccResource.setCommitMethodName(twoPhaseBusinessAction.commitMethod());\\n                        tccResource.setCommitMethod(ReflectionUtil\\n                            .getMethod(interfaceClass, twoPhaseBusinessAction.commitMethod(),\\n                                new Class[] {BusinessActionContext.class}));\\n                        tccResource.setRollbackMethodName(twoPhaseBusinessAction.rollbackMethod());\\n                        tccResource.setRollbackMethod(ReflectionUtil\\n                            .getMethod(interfaceClass, twoPhaseBusinessAction.rollbackMethod(),\\n                                new Class[] {BusinessActionContext.class}));\\n                        //registry tcc resource\\n                        DefaultResourceManager.get().registerResource(tccResource);\\n                    }\\n                }\\n            } catch (Throwable t) {\\n                throw new FrameworkException(t, \\"parser remoting service error\\");\\n            }\\n        }\\n        if (isReference(bean, beanName)) {\\n            //reference bean, TCC proxy\\n            remotingBeanDesc.setReference(true);\\n        }\\n        return remotingBeanDesc;\\n    }\\n```\\n\\nFirstly, determine if it is a transaction participant. If so, obtain the interfaceClass from RemotingDesc, iterate through the methods in the interface, and check if there is a @TwoParserBusinessAction annotation on the method. If found, encapsulate the parameters into TCCResource and register the TCC resource through DefaultResourceManager.\\n\\nHere, DefaultResourceManager will search for the corresponding resource manager based on the BranchType of the Resource. The resource management class under the TCC mode is in the tcc module.\\n\\nThis RPC parsing class is mainly provided for use by the spring module. parserRemotingServiceInfo() is encapsulated into the TCCBeanParserUtils utility class in the spring module. During project startup, the GlobalTransactionScanner in the spring module parses TCC beans through the utility class. TCCBeanParserUtils calls TCCResourceManager to register resources. If it is a global transaction service provider, it will weave in the TccActionInterceptor proxy. These processes are functionalities of the spring module, where the tcc module provides functional classes for use by the spring module.\\n\\n## Three. TCC Resource Manager\\n\\nTCCResourceManager is responsible for managing the registration, branching, committing, and rolling back of resources under the TCC mode.\\n\\n1. During project startup, when the GlobalTransactionScanner in the spring module detects that a bean is a tcc bean, it caches resources locally and registers them with the server:\\n\\n\\n```java\\n    @Override\\n    public void registerResource(Resource resource) {\\n        TCCResource tccResource = (TCCResource)resource;\\n        tccResourceCache.put(tccResource.getResourceId(), tccResource);\\n        super.registerResource(tccResource);\\n    }\\n```\\n\\nThe logic for communicating with the server is encapsulated in the parent class AbstractResourceManager. Here, TCCResource is cached based on resourceId. When registering resources in the parent class AbstractResourceManager, resourceGroupId + actionName is used, where actionName is the name specified in the @TwoParseBusinessAction annotation, and resourceGroupId defaults to DEFAULT.\\n\\n2. Transaction branch registration is handled in the rm-datasource package under AbstractResourceManager. During registration, the parameter lockKeys is null, which differs from the transaction branch registration under the AT mode.\\n\\n3. Committing or rolling back branches:\\n\\n\\n```java\\n    @Override\\n    public BranchStatus branchCommit(BranchType branchType, String xid, long branchId, String resourceId,\\n                                     String applicationData) throws TransactionException {\\n        TCCResource tccResource = (TCCResource)tccResourceCache.get(resourceId);\\n        if (tccResource == null) {\\n            throw new ShouldNeverHappenException(\\"TCC resource is not exist, resourceId:\\" + resourceId);\\n        }\\n        Object targetTCCBean = tccResource.getTargetBean();\\n        Method commitMethod = tccResource.getCommitMethod();\\n        if (targetTCCBean == null || commitMethod == null) {\\n            throw new ShouldNeverHappenException(\\"TCC resource is not available, resourceId:\\" + resourceId);\\n        }\\n        try {\\n            boolean result = false;\\n            //BusinessActionContext\\n            BusinessActionContext businessActionContext = getBusinessActionContext(xid, branchId, resourceId,\\n                applicationData);\\n            Object ret = commitMethod.invoke(targetTCCBean, businessActionContext);\\n            if (ret != null) {\\n                if (ret instanceof TwoPhaseResult) {\\n                    result = ((TwoPhaseResult)ret).isSuccess();\\n                } else {\\n                    result = (boolean)ret;\\n                }\\n            }\\n            return result ? BranchStatus.PhaseTwo_Committed : BranchStatus.PhaseTwo_CommitFailed_Retryable;\\n        } catch (Throwable t) {\\n            LOGGER.error(msg, t);\\n            throw new FrameworkException(t, msg);\\n        }\\n    }\\n```\\nRestore the business context using parameters xid, branchId, resourceId, and applicationData.\\n\\nExecute the commit method through reflection based on the retrieved context and return the execution result. The rollback method follows a similar approach.\\n\\nHere, branchCommit() and branchRollback() are provided for AbstractRMHandler, an abstract class for resource processing in the rm module. This handler is a further implementation class of the template method defined in the core module. Unlike registerResource(), which actively registers resources during spring scanning.\\n\\n## Four. Transaction Processing in TCC Mode\\n\\nThe invoke() method of TccActionInterceptor in the spring module is executed when the proxied rpc bean is called. This method first retrieves the global transaction xid passed by the rpc interceptor, and then the transaction process of global transaction participants under TCC mode is still handed over to the ActionInterceptorHandler in the tcc module.\\n\\nIn other words, transaction participants are proxied during project startup. The actual business methods are executed through callbacks in ActionInterceptorHandler.\\n\\n\\n```java\\n    public Map<String, Object> proceed(Method method, Object[] arguments, String xid, TwoPhaseBusinessAction businessAction,\\n                                       Callback<Object> targetCallback) throws Throwable {\\n        Map<String, Object> ret = new HashMap<String, Object>(4);\\n\\n        //TCC name\\n        String actionName = businessAction.name();\\n        BusinessActionContext actionContext = new BusinessActionContext();\\n        actionContext.setXid(xid);\\n        //set action anme\\n        actionContext.setActionName(actionName);\\n\\n        //Creating Branch Record\\n        String branchId = doTccActionLogStore(method, arguments, businessAction, actionContext);\\n        actionContext.setBranchId(branchId);\\n\\n        //set the parameter whose type is BusinessActionContext\\n        Class<?>[] types = method.getParameterTypes();\\n        int argIndex = 0;\\n        for (Class<?> cls : types) {\\n            if (cls.getName().equals(BusinessActionContext.class.getName())) {\\n                arguments[argIndex] = actionContext;\\n                break;\\n            }\\n            argIndex++;\\n        }\\n        //the final parameters of the try method\\n        ret.put(Constants.TCC_METHOD_ARGUMENTS, arguments);\\n        //the final result\\n        ret.put(Constants.TCC_METHOD_RESULT, targetCallback.execute());\\n        return ret;\\n    }\\n```\\n\\nHere are two important operations:\\n\\n1. In the doTccActionLogStore() method, two crucial methods are called:\\n- fetchActionRequestContext(method, arguments): This method retrieves parameters annotated with @BusinessActionContextParam and inserts them into BusinessActionComtext along with transaction-related parameters in the init method below.\\n- DefaultResourceManager.get().branchRegister(BranchType.TCC, actionName, null, xid, applicationContextStr, null): This method performs the registration of transaction branches for transaction participants under TCC mode.\\n\\n2. Callback execution of targetCallback.execute(), which executes the specific business logic of the proxied bean, i.e., the prepare() method.\\n\\n## Five. Summary\\nThe tcc module primarily provides the following functionalities:\\n1. Defines annotations for two-phase protocols, providing attributes needed for transaction processes under TCC mode.\\n2. Provides implementations of ParserRemoting for parsing remoting beans of different RPC frameworks, to be invoked by the spring module.\\n3. Provides the TCC ResourceManager for resource registration, transaction branch registration, submission, and rollback under TCC mode.\\n4. Provides classes for handling transaction processes under TCC mode, allowing MethodInterceptor proxy classes to delegate the execution of specific mode transaction processes to the tcc module.\\n\\n## Related\\nAuthor: Zhao Runze, [Series Link](https://blog.csdn.net/qq_37804737/category_9530078.html)."},{"id":"/seata-community-meetup-hangzhou-ready","metadata":{"permalink":"/blog/seata-community-meetup-hangzhou-ready","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-community-meetup-hangzhou-ready.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-community-meetup-hangzhou-ready.md","title":"Seata Community Meetup\xb7Hangzhou Station","description":"Seata Community Meetup\xb7Hangzhou Station, officially held on December 21st at Zhejiang Youth Innovation Space in Dream Town, Hangzhou.","date":"2019-12-25T00:00:00.000Z","formattedDate":"December 25, 2019","tags":[],"readingTime":0.42,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Seata Community Meetup\xb7Hangzhou Station","keywords":["Seata","Hangzhou","meetup"],"date":"2019/12/25","description":"Seata Community Meetup\xb7Hangzhou Station, officially held on December 21st at Zhejiang Youth Innovation Space in Dream Town, Hangzhou."},"unlisted":false,"prevItem":{"title":"Seata TCC Module Source Code Analysis","permalink":"/blog/seata-analysis-tcc-modular"},"nextItem":{"title":"Seata Core Module Source Code Analysis","permalink":"/blog/seata-analysis-core-modular"}},"content":"### Introduction\\n\\n### Highlights\\n\\n- Seata open source project initiator will present \\"Seata Past, Present and Future\\" and new features of Seata 1.0.\\n- Seata AT, TCC, Saga model explained by Seata core contributors.\\n- Seata\'s Internet Healthcare and DDT\'s practice analysis.\\n\\n### If you can\'t make it\\n\\n- [Book Live (Developer Community)](https://developer.aliyun.com/live/1760)\\n- [Join Seata\'s 1,000-person Spike Group](http://w2wz.com/h2nb)\\n\\n### Onsite Benefits\\n\\n- Speaker\'s PPT download\\n- Tea breaks, Ali dolls, Tmall Genie and other goodies for you to get!\\n\\n### Agenda\\n\\nAgenda! [](https://img.alicdn.com/tfs/TB1K5nYwVP7gK0jSZFjXXc5aXXa-3175-14507.png)"},{"id":"/seata-analysis-core-modular","metadata":{"permalink":"/blog/seata-analysis-core-modular","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-core-modular.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-core-modular.md","title":"Seata Core Module Source Code Analysis","description":"1. Introduction","date":"2019-12-23T00:00:00.000Z","formattedDate":"December 23, 2019","tags":[],"readingTime":5.335,"hasTruncateMarker":false,"authors":[{"name":"runze.zhao"}],"frontMatter":{"title":"Seata Core Module Source Code Analysis","author":"runze.zhao","keywords":["Seata","distributed transaction"],"date":"2019/12/23"},"unlisted":false,"prevItem":{"title":"Seata Community Meetup\xb7Hangzhou Station","permalink":"/blog/seata-community-meetup-hangzhou-ready"},"nextItem":{"title":"Dynamically Creating/Closing Seata Distributed Transactions through AOP","permalink":"/blog/seata-spring-boot-aop-aspectj"}},"content":"## 1. Introduction\\n\\nThe core module defines the types and states of transactions, common behaviors, protocols, and message models for communication between clients and servers, as well as exception handling methods, compilation, compression types, configuration information names, environment context, etc. It also encapsulates RPC based on Netty for use by both clients and servers.\\n\\nLet\'s analyze the main functional classes of the core module according to the package order:\\n\\n![Image Description](https://img-blog.csdnimg.cn/20191223162313317.png)\\n\\ncodec: Defines a codec factory class, which provides a method to find the corresponding processing class based on the serialization type. It also provides an interface class Codec with two abstract methods:\\n\\n```java\\n<T> byte[] encode(T t);\\n```\\n\\n```java\\n<T> T decode(byte[] bytes);\\n```\\n\\n## 1. codec Module\\n\\nIn version 1.0, the codec module has three serialization implementations: SEATA, PROTOBUF, and KRYO.\\n\\ncompressor: Similar to classes under the codec package, there are three classes here: a compression type class, a factory class, and an abstract class for compression and decompression operations. In version 1.0, there is only one compression method: Gzip.\\n\\nconstants: Consists of two classes, ClientTableColumnsName and ServerTableColumnsName, representing the models for transaction tables stored on the client and server sides respectively. It also includes classes defining supported database types and prefixes for configuration information attributes.\\n\\ncontext: The environment class RootContext holds a ThreadLocalContextCore to store transaction identification information. For example, TX_XID uniquely identifies a transaction, and TX_LOCK indicates the need for global lock control for local transactions on update/delete/insert/selectForUpdate SQL operations.\\n\\nevent: Utilizes the Guava EventBus event bus for registration and notification, implementing the listener pattern. In the server module\'s metrics package, MetricsManager registers monitoring events for changes in GlobalStatus, which represents several states of transaction processing in the server module. When the server processes transactions, the callback methods registered for monitoring events are invoked, primarily for statistical purposes.\\n\\nlock: When the server receives a registerBranch message for branch registration, it acquires a lock. In version 1.0, there are two lock implementations: DataBaseLocker and MemoryLocker, representing database locks and in-memory locks respectively. Database locks are acquired based on the rowKey = resourceId + tableName + pk, while memory locks are based directly on the primary key.\\n\\nmodel: BranchStatus, GlobalStatus, and BranchType are used to define transaction types and global/branch states. Additionally, TransactionManager and ResourceManager are abstract classes representing resource managers (RMs) and transaction managers (TMs) respectively. Specific implementations of RMs and TMs are not provided here due to variations in transaction types.\\n\\nprotocol: Defines entity classes used for transmission in the RPC module, representing models for requests and responses under different transaction status scenarios.\\n\\nstore: Defines data models for interacting with databases and the SQL statements used for database interactions.\\n\\n```java\\n    public void exceptionHandleTemplate(Callback callback, AbstractTransactionRequest request,\\n        AbstractTransactionResponse response) {\\n        try {\\n            callback.execute(request, response); //\u6267\u884c\u4e8b\u52a1\u4e1a\u52a1\u7684\u65b9\u6cd5\\n            callback.onSuccess(request, response); //\u8bbe\u7f6eresponse\u8fd4\u56de\u7801\\n        } catch (TransactionException tex) {\\n            LOGGER.error(\\"Catch TransactionException while do RPC, request: {}\\", request, tex);\\n            callback.onTransactionException(request, response, tex); //\u8bbe\u7f6eresponse\u8fd4\u56de\u7801\u5e76\u8bbe\u7f6emsg\\n        } catch (RuntimeException rex) {\\n            LOGGER.error(\\"Catch RuntimeException while do RPC, request: {}\\", request, rex);\\n            callback.onException(request, response, rex);  //\u8bbe\u7f6eresponse\u8fd4\u56de\u7801\u5e76\u8bbe\u7f6emsg\\n        }\\n    }\\n```\\n\\n## 2. Analysis of Exception Handling in the exception Package\\n\\nThis is the UML diagram of AbstractExceptionHandler. Callback and AbstractCallback are internal interfaces and classes of AbstractExceptionHandler. AbstractCallback implements three methods of the Callback interface but leaves the execute() method unimplemented. AbstractExceptionHandler uses AbstractCallback as a parameter for the template method and utilizes its implemented methods. However, the execute() method is left to be implemented by subclasses.\\n\\n![Image Description](https://img-blog.csdnimg.cn/20191211165628768.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3ODA0NzM3,size_16,color_FFFFFF,t_70)\\n\\nFrom an external perspective, AbstractExceptionHandler defines a template method with exception handling. The template includes four behaviors, three of which are already implemented, and the behavior execution is delegated to subclasses.\\n\\n## 3. Analysis of the rpc Package\\n\\nWhen it comes to the encapsulation of RPC by Seata, one need not delve into the details. However, it\'s worth studying how transaction business is handled.\\n\\nThe client-side RPC class is AbstractRpcRemotingClient:\\n\\n![Image Description](https://img-blog.csdnimg.cn/20191211180129741.png)\\n\\nThe important attributes and methods are depicted in the class diagram. The methods for message sending and initialization are not shown in the diagram. Let\'s analyze the class diagram in detail:\\n\\nclientBootstrap: This is a wrapper class for the netty startup class Bootstrap. It holds an instance of Bootstrap and customizes the properties as desired.\\n\\n\\nclientChannelManager: Manages the correspondence between server addresses and channels using a ConcurrentHashMap\\\\<serverAddress,channel> container.\\n\\nclientMessageListener: Handles messages. Depending on the message type, there are three specific processing methods.\\n\\n\\n```java\\npublic void onMessage(RpcMessage request, String serverAddress, ClientMessageSender sender) {\\n        Object msg = request.getBody();\\n        if (LOGGER.isInfoEnabled()) {\\n            LOGGER.info(\\"onMessage:\\" + msg);\\n        }\\n        if (msg instanceof BranchCommitRequest) {\\n            handleBranchCommit(request, serverAddress, (BranchCommitRequest)msg, sender);\\n        } else if (msg instanceof BranchRollbackRequest) {\\n            handleBranchRollback(request, serverAddress, (BranchRollbackRequest)msg, sender);\\n        } else if (msg instanceof UndoLogDeleteRequest) {\\n            handleUndoLogDelete((UndoLogDeleteRequest)msg);\\n        }\\n    }\\n```\\n\\n## 4. Analysis of the rpc Package (Continued)\\n\\nWithin the message class, the TransactionMessageHandler is responsible for handling messages of different types. Eventually, based on the different transaction types (AT, TCC, SAGE), specific handling classes, as mentioned in the second part, exceptionHandleTemplate(), are invoked.\\n\\nmergeSendExecutorService: This is a thread pool with only one thread responsible for merging and sending messages from different addresses. In the sendAsyncRequest() method, messages are offered to the queue LinkedBlockingQueue of the thread pool. The thread is then responsible for polling and processing messages.\\n\\n\\nchannelRead(): Handles server-side HeartbeatMessage.PONG heartbeat messages. Additionally, it processes MergeResultMessage, which are response messages for asynchronous messages. It retrieves the corresponding MessageFuture based on the msgId and sets the result of the asynchronous message.\\n\\ndispatch(): Invokes the clientMessageListener to handle messages sent by the server. Different types of requests have different handling classes.\\n\\nIn summary, when looking at Netty, one should focus on serialization methods and message handling handler classes. Seata\'s RPC serialization method is processed by finding the Codec implementation class through the factory class, and the handler is the TransactionMessageHandler mentioned earlier.\\n\\n## 5. Conclusion\\n\\nThe core module covers a wide range of functionalities, with most classes serving as abstract classes for other modules. Business models are abstracted out, and specific implementations are distributed across different modules. The code in the core module is of high quality, with many classic design patterns such as the template pattern discussed earlier. It is very practical and well-crafted, deserving careful study.\\n\\n## 6. Seata Source Code Analysis Series Links\\n\\n[Series Links](https://blog.csdn.net/qq_37804737/category_9530078.html)"},{"id":"/seata-spring-boot-aop-aspectj","metadata":{"permalink":"/blog/seata-spring-boot-aop-aspectj","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-spring-boot-aop-aspectj.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-spring-boot-aop-aspectj.md","title":"Dynamically Creating/Closing Seata Distributed Transactions through AOP","description":"This article explains how to dynamically create/close Seata distributed transactions using AOP.","date":"2019-12-23T00:00:00.000Z","formattedDate":"December 23, 2019","tags":[],"readingTime":3.765,"hasTruncateMarker":false,"authors":[{"name":"FUNKYE"}],"frontMatter":{"title":"Dynamically Creating/Closing Seata Distributed Transactions through AOP","keywords":["Seata","Nacos","distributed transaction","spring"],"description":"This article explains how to dynamically create/close Seata distributed transactions using AOP.","author":"FUNKYE","date":"2019/12/23"},"unlisted":false,"prevItem":{"title":"Seata Core Module Source Code Analysis","permalink":"/blog/seata-analysis-core-modular"},"nextItem":{"title":"Seata Dynamic Configuration Subscription and Degradation Implementation Principles","permalink":"/blog/seata-dynamic-config-and-dynamic-disable"}},"content":"Dynamically create/close Seata distributed transactions through AOP\\n\\nThis article was written by FUNKYE (Chen Jianbin), the main programmer of an Internet company in Hangzhou.\\n\\n# Foreword\\n\\nThrough the GA conference on the senior R & D engineering Chen Pengzhi drop trip in the drop two-wheeler business practice, found that the need for dynamic degradation is very high, so this simple use of spring boot aop to simply deal with degradation of the relevant processing, this is very thankful to Chen Pengzhi\'s sharing!\\n\\ncan use this demo [project address](https://gitee.com/itCjb/springboot-dubbo-mybatisplus-seata )\\n\\nthrough the following code transformation practice .\\n\\n## Preparation\\n\\n1. Create a TestAspect for testing.\\n\\n```java\\npackage org.test.config;\\n\\nimport java.lang.reflect.\\n\\nimport org.apache.commons.lang3.StringUtils; import org.aspectj.\\nimport org.aspectj.lang.JoinPoint; import org.aspectj.lang.\\nimport org.aspectj.lang.annotation.AfterReturning; import org.aspectj.lang.annotation.\\nimport org.aspectj.lang.annotation.AfterThrowing; import org.aspectj.lang.annotation.\\nimport org.aspectj.lang.JoinPoint.import org.aspectj.annotation.AfterReturning; import org.aspectj.lang.annotation.\\nimport org.aspectj.lang.annotation.\\nimport org.aspectj.lang.reflect.MethodSignature; import org.aspectj.lang.reflect.\\nimport org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.reflect.\\nimport org.slf4j.LoggerFactory; import org.springframework.\\nimport org.springframework.stereotype.Component; import org.springframework.stereotype.\\n\\nimport io.seata.core.context.\\nimport io.seata.core.exception.TransactionException; import io.seata.core.exception.\\nimport io.seata.tm.api.GlobalTransaction; import io.seata.tm.api.\\nimport io.seata.tm.api.GlobalTransactionContext; import io.seata.tm.api.\\n\\n@Aspect\\nGlobalTransactionContext; @Aspect\\npublic class TestAspect {\\nprivate final static Logger logger = LoggerFactory.getLogger(TestAspect.class); @Before(\\"execution\\"); @Before(\\"execution\\")\\n\\n    @Before(\\"execution(* org.test.service.*. *(...))\\")\\n    public void before(JoinPoint joinPoint) throws TransactionException {\\n        MethodSignature signature = (MethodSignature)joinPoint.getSignature();\\n        Method method = signature.getMethod(); logger.info\\n        logger.info(\\"Intercepted method that requires a distributed transaction, \\" + method.getName()); // Use redis or redis.getName() here.\\n        // Here you can use redis or a timed task to get a key to determine if the distributed transaction needs to be closed.\\n        // Simulate a dynamic shutdown of a distributed transaction\\n        if ((int)(Math.random() * 100) % 2 == 0) {\\n            GlobalTransaction tx = GlobalTransactionContext.getCurrentOrCreate();\\n            tx.begin(300000, \\"test-client\\");\\n        } else {\\n            logger.info(\\"Closing distributed transaction\\"); }\\n        }\\n    }\\n\\n    @AfterThrowing(throwing = \\"e\\", pointcut = \\"execution(* org.test.service. *(...))\\")\\n    public void doRecoveryActions(Throwable e) throws TransactionException {\\n        logger.info(\\"Method Execution Exception: {}\\", e.getMessage());\\n        if (!StringUtils.isBlank(RootContext.getXID()))\\n            GlobalTransactionContext.reload(RootContext.getXID()).rollback();\\n    }\\n\\n    @AfterReturning(value = \\"execution(* org.test.service.*. *(...))\\" , returning = \\"result\\")\\n    public void afterReturning(JoinPoint point, Object result) throws TransactionException {\\n        logger.info(\\"End of method execution: {}\\", result);\\n        if ((Boolean)result) {\\n            if (!StringUtils.isBlank(RootContext.getXID())) {\\n                logger.info(\\"DistributedTransactionId:{}\\", RootContext.getXID());\\n                GlobalTransactionContext.reload(RootContext.getXID()).commit();\\n            }\\n        }\\n    }\\n\\n}\\n```\\n\\nPlease note that the package name above can be changed to your own service package name: ``.\\n\\n2. Change the service code.\\n\\n```java\\n    public Object seataCommit() {\\n        testService.Commit(); return true; return true; testService.Commit(); testService.Commit()\\n        testService.Commit(); return true; }\\n    }\\n```\\n\\nBecause of the exception and return results we will intercept, so this side can trycatch or directly let him throw an exception to intercept the line, or directly judge the return results, such as your business code code = 200 for success, then the commit, and vice versa in the interception of the return value of that section of the code plus rollback; # Debugging.\\n\\n# Debugging\\n\\n1. Change the code to actively throw exceptions\\n\\n```java\\n    public Object seataCommit() {\\n        try {\\n            testService.Commit();\\n            testService.Commit(); int i = 1 / 0; return true; return\\n            return true; } catch (Exception e) { testService.\\n        } catch (Exception e) {\\n            // TODO: handle exception\\n            throw new RuntimeException(); } catch (Exception e) { // TODO: handle exception.\\n        }\\n    }\\n```\\n\\nView log:\\n\\n```java\\n2019-12-23 11:57:55.386 INFO 23952 --- [.0-28888-exec-7] org.test.controller.TestController : Intercepted method requiring distributed transaction, seataCommit\\n2019-12-23 11:57:55.489 INFO 23952 --- [.0-28888-exec-7] i.seata.tm.api.DefaultGlobalTransaction : Begin new global transaction [192.168.14.67 :8092:2030765910]\\n2019-12-23 11:57:55.489 INFO 23952 --- [.0-28888-exec-7] org.test.controller.TestController : Creating distributed transaction complete 192.168.14.67 :8092:2030765910\\n2019-12-23 11:57:55.709 INFO 23952 --- [.0-28888-exec-7] org.test.controller.TestController : Method execution exception:null\\n2019-12-23 11:57:55.885 INFO 23952 --- [.0-28888-exec-7] i.seata.tm.api.DefaultGlobalTransaction : [192.168.14.67:8092:2030765910] rollback status: Rollbacked\\n2019-12-23 11:57:55.888 ERROR 23952 --- [.0-28888-exec-7] o.a.c.c.C. [. [. [/]. [dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.RuntimeException] with root cause\\n\\n```\\n\\nYou can see that it has been intercepted and triggered a rollback.\\n\\n2. Restore the code to debug the normal situation:\\n\\n```java\\n    public Object seataCommit() {\\n        testService.Commit(); testService.Commit(); testService.Commit(); testService.Commit()\\n        testService.Commit(); return true; }\\n    }\\n```\\n\\nViewing logs.\\n\\n```\\n2019-12-23 12:00:20.876 INFO 23952 --- [.0-28888-exec-2] org.test.controller.TestController : Intercepted method requiring distributed transaction, seataCommit\\n2019-12-23 12:00:20.919 INFO 23952 --- [.0-28888-exec-2] i.seata.tm.api.DefaultGlobalTransaction : Begin new global transaction [192.168.14.67 :8092:2030765926]\\n2019-12-23 12:00:20.920 INFO 23952 --- [.0-28888-exec-2] org.test.controller.TestController : Creating distributed transaction complete 192.168.14.67 :8092:2030765926\\n2019-12-23 12:00:21.078 INFO 23952 --- [.0-28888-exec-2] org.test.controller.TestController : End of method execution:true\\n2019-12-23 12:00:21.078 INFO 23952 --- [.0-28888-exec-2] org.test.controller.TestController : Distributed transaction Id:192.168.14.67:8092:2030765926\\n2019-12-23 12:00:21.213 INFO 23952 --- [.0-28888-exec-2] i.seata.tm.api.DefaultGlobalTransaction : [192.168.14.67:8092:2030765926] commit status: Committed\\n```\\n\\nYou can see that the transaction has been committed.\\n\\n# Summary\\n\\nFor more details, we hope you will visit the following address to read the detailed documentation.\\n\\n[nacos website](https://nacos.io/zh-cn/index.html)\\n\\n[dubbo website](http://dubbo.apache.org/en-us/)\\n\\n[seata official website](https://seata.apache.org/zh-cn/)\\n\\n[docker official website](https://www.docker.com/)"},{"id":"/seata-dynamic-config-and-dynamic-disable","metadata":{"permalink":"/blog/seata-dynamic-config-and-dynamic-disable","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-dynamic-config-and-dynamic-disable.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-dynamic-config-and-dynamic-disable.md","title":"Seata Dynamic Configuration Subscription and Degradation Implementation Principles","description":"This article explains how Seata adapts to different dynamic configuration subscriptions and implements degradation functionality with support for multiple configuration centers.","date":"2019-12-17T00:00:00.000Z","formattedDate":"December 17, 2019","tags":[],"readingTime":5.895,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"Seata Dynamic Configuration Subscription and Degradation Implementation Principles","author":"chenghui.zhang","keywords":["Seata","Dynamic","Config"],"description":"This article explains how Seata adapts to different dynamic configuration subscriptions and implements degradation functionality with support for multiple configuration centers.","date":"2019/12/17"},"unlisted":false,"prevItem":{"title":"Dynamically Creating/Closing Seata Distributed Transactions through AOP","permalink":"/blog/seata-spring-boot-aop-aspectj"},"nextItem":{"title":"Seata Configuration Center Implementation Principles","permalink":"/blog/seata-config-center"}},"content":"Seata\'s dynamic degradation needs to be combined with the dynamic configuration subscription feature of the configuration centre. Dynamic configuration subscription, that is, through the configuration centre to listen to the subscription, according to the need to read the updated cache value, ZK, Apollo, Nacos and other third-party configuration centre have ready-made listener can be achieved dynamic refresh configuration; dynamic degradation, that is, by dynamically updating the value of the specified configuration parameter, so that Seata can be dynamically controlled in the running process of the global transaction invalidated (at present, only the AT mode has). (currently only AT mode has this feature).\\n\\nSo how do the multiple configuration centres supported by Seata adapt to different dynamic configuration subscriptions and how do they achieve degradation? Here is a detailed explanation from the source code level.\\n\\n\\n\\n# Dynamic Configuration Subscriptions\\n\\nThe Seata Configuration Centre has a listener baseline interface, which has an abstract method and default method, as follows:\\n\\nio.seata.config.ConfigurationChangeListener\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20191216212442.png)\\n\\nThis listener baseline interface has two main implementation types:\\n\\n1. implementation of the registration of configuration subscription event listener: for the implementation of a variety of functions of dynamic configuration subscription, such as GlobalTransactionalInterceptor implements ConfigurationChangeListener, according to the dynamic configuration subscription to the dynamic degradation of the implementation of the function;\\n2. the implementation of the configuration centre dynamic subscription function and adaptation: for the file type default configuration centre that currently does not have dynamic subscription function, you can implement the benchmark interface to achieve dynamic configuration subscription function; for the blocking subscription needs to start another thread to execute, then you can implement the benchmark interface for adaptation, you can also reuse the thread pool of the benchmark interface; and there are also asynchronous subscription, there is subscription to a single key, there is subscription to multiple keys, and so on. key, multiple key subscriptions, and so on, we can implement the baseline interface to adapt to each configuration centre.\\n\\n## Nacos Dynamic Subscription Implementation\\n\\nNacos has its own internal implementation of the listener, so it directly inherits its internal abstract listener, AbstractSharedListener, which is implemented as follows:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20191223212237.png)\\n\\nAs above.\\n\\n- dataId: configuration attribute for the subscription;\\n- listener: configures the subscription event listener, which is used to use the incoming listener as a wrapper to perform the actual change logic.\\n\\nIt\'s worth mentioning that nacos doesn\'t use ConfigurationChangeListener to implement its own listener configuration, on the one hand, because Nacos itself already has a listener subscription function, so it doesn\'t need to implement it; on the other hand, because nacos is a non-blocking subscription, it doesn\'t need to reuse the ConfigurationChangeListener\'s thread pool, i.e., no adaptation is needed.\\n\\nAdd the subscription:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20191223213347.png)\\n\\nThe logic of adding a subscription to a dataId in Nacos Configuration Centre is very simple, create a NacosListener with the dataId and a listener, call the configService#addListener method, and use the NacosListener as a listener for the dataId, and then the dataId can be dynamically configured for subscription. Dynamic Configuration Subscription.\\n\\n## file Dynamic subscription implementation\\n\\nTake its implementation class FileListener as an example, its implementation logic is as follows:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20191215151642.png)\\n\\nAs above.\\n\\n- dataId: configuration attribute for the subscription;\\n\\n- listener: the configuration subscription event listener, used as a wrapper for the incoming listener to perform the real change logic, it is important to note that ** this listener and FileListener also implement the ConfigurationChangeListener interface, except that FileListener is used to provide dynamic configuration subscription to the file, while listener is used to execute configuration subscription events**;\\n\\n- executor: a thread pool used for processing configuration change logic, used in the ConfigurationChangeListener#onProcessEvent method.\\n\\nThe implementation of the **FileListener#onChangeEvent method gives the file the ability to subscribe to dynamic configurations** with the following logic:\\n\\nIt loops indefinitely to get the current value of the subscribed configuration property, fetches the old value from the cache, determines if there is a change, and executes the logic of the external incoming listener if there is a change.\\n\\nConfigurationChangeEvent The event class used to save configuration changes, it has the following member properties:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20191215175232.png)\\n\\n\\n\\nHow does the getConfig method sense changes to the file configuration? We click into it and find that it ends up with the following logic:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20191215162713.png)\\n\\nWe see that it creates a future class, wraps it in a Runnable and puts it into the thread pool to execute asynchronously, and then calls the get method to block the retrieval of the value, so let\'s move on:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20191215170908.png)\\n\\nallowDynamicRefresh: configure switch for dynamic refresh;\\n\\ntargetFileLastModified: time cache of the last change to the file.\\n\\nThe above logic:\\n\\nGet the tempLastModified value of the last update of the file, then compare it with the targetFileLastModified value, if tempLastModified > targetFileLastModified, it means that the configuration has been changed in the meantime. instance is reloaded, replacing the old fileConfig so that later operations can get the latest configuration values.\\n\\nThe logic for adding a configuration property listener is as follows:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20191215161103.png)\\n\\nconfigListenersMap is a configuration listener cache for FileConfiguration with the following data structure:\\n\\n ```java\\n ConcurrentMap<String/*dataId*/, Set<ConfigurationChangeListener>> configListenersMap\\n ```\\n\\nAs you can see from the data structure, each configuration property can be associated with multiple event listeners.\\n\\nEventually the onProcessEvent method is executed, which is the default method in the listener\'s base interface, and it calls the onChangeEvent method, which means that it will eventually call the implementation in the FileListener.\\n\\n\\n\\n# Dynamic Degradation\\n\\nWith the above dynamic configuration subscription functionality, we only need to implement the ConfigurationChangeListener listener to do all kinds of functionality. Currently, Seata only has dynamic degradation functionality for dynamic configuration subscription.\\n\\nIn the article \u300c[Seata AT mode startup source code analysis](https://mp.weixin.qq.com/s/n9MHk47zSsFQmV-gBq_P1A)\u300d, it is said that in the project of Spring integration with Seata, when AT mode is started, it will use the GlobalTransactionalInterceptor replaces the methods annotated with GlobalTransactional and GlobalLock. GlobalTransactionalInterceptor implements MethodInterceptor, which will eventually execute the invoker method, so if you want to achieve dynamic demotion, you can do something here.\\n\\n- Add a member variable to GlobalTransactionalInterceptor:\\n\\n ```java\\n private volatile boolean disable; ``java\\n ```\\n\\nInitialise the assignment in the constructor:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20191215173221.png)\\n\\nConfigurationKeys.DISABLE_GLOBAL_TRANSACTION (service.disableGlobalTransaction) This parameter currently has two functions:\\n\\n1. to determine whether to enable global transactions at startup;\\n2. to decide whether or not to demote a global transaction after it has been enabled.\\n\\n- Implement ConfigurationChangeListener:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20191215173358.png)\\n\\nThe logic here is simple, it is to determine whether the listening event belongs to the ConfigurationKeys.DISABLE_GLOBAL_TRANSACTION configuration attribute, if so, directly update the disable value.\\n\\n- Next, do something in GlobalTransactionalInterceptor#invoke\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20191215174155.png)\\n\\nAs above, when disable = true, no global transaction with global lock is performed.\\n\\n- Configuration Centre Subscription Degradation Listener\\n\\nio.seata.spring.annotation.GlobalTransactionScanner#wrapIfNecessary\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/20191215174409.png)\\n\\nThe current Configuration Centre will subscribe to the demotion event listener during wrap logic in Spring AOP.\\n\\n# Author Bio\\n\\nZhang Chenghui, currently working in the technology platform department of Zhongtong Technology Information Centre as a Java engineer, mainly responsible for the development of Zhongtong messaging platform and all-links pressure testing project, love to share technology, WeChat public number \\"back-end advanced\\" author, technology blog ([https://objcoding.com/](https://objcoding.com/)) Blogger, Seata Contributor, GitHub ID: objcoding."},{"id":"/seata-config-center","metadata":{"permalink":"/blog/seata-config-center","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-config-center.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-config-center.md","title":"Seata Configuration Center Implementation Principles","description":"Seata supports multiple third-party configuration centers, but how does Seata simultaneously accommodate so many configuration centers?","date":"2019-12-12T00:00:00.000Z","formattedDate":"December 12, 2019","tags":[],"readingTime":5.565,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"Seata Configuration Center Implementation Principles","author":"chenghui.zhang","keywords":["Seata","Config"],"description":"Seata supports multiple third-party configuration centers, but how does Seata simultaneously accommodate so many configuration centers?","date":"2019/12/12"},"unlisted":false,"prevItem":{"title":"Seata Dynamic Configuration Subscription and Degradation Implementation Principles","permalink":"/blog/seata-dynamic-config-and-dynamic-disable"},"nextItem":{"title":"Docker Deployment of Seata Integration with Nacos","permalink":"/blog/seata-nacos-docker"}},"content":"Seata can support multiple third-party configuration centres, so how is Seata compatible with so many configuration centres at the same time? Below I will give you a detailed introduction to the principle of Seata Configuration Centre implementation.\\n\\n\\n# Configuration Centre Property Loading\\n\\nIn Seata Configuration Centre, there are two default configuration files:\\n\\n! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191211193041.png)\\n\\nfile.conf is the default configuration properties, and registry.conf mainly stores third-party registry and configuration centre information, and has two main blocks:\\n\\n```json\\nregistry {\\n# file, nacos, eureka, redis, zk, consul, etcd3, sofa\\n# ...\\n}\\n\\nconfig {\\n# file, nacos , apollo, zk, consul, etcd3\\ntype = \\"file\\"\\nnacos {\\nserverAddr = \\"localhost\\"\\nnamespace = \\"\\"\\n}\\nfile {\\nname = \\"file.conf\\"\\n}\\n# ...\\n}\\n ```\\n\\n The registry is the configuration attribute of the registry, which is not mentioned here, and the config is the value of the attribute of the configuration centre, which is of type file by default, i.e., it will load the attributes inside the local file.conf, and if the type is of any other type, it will load the value of the configuration attribute from the third-party configuration centre.\\n\\n In the core directory of the config module, there is a configuration factory class ConfigurationFactory, which has the following structure:\\n\\n ! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191210211022.png)\\n\\n You can see that there are some static constants for configuration:\\n\\n REGISTRY_CONF_PREFIX, REGISTRY_CONF_SUFFIX: the name of the configuration file, the default configuration file type;\\n\\n SYSTEM_PROPERTY_SEATA_CONFIG_NAME, ENV_SEATA_CONFIG_NAME, ENV_SYSTEM_KEY, ENV_PROPERTY_KEY: custom filename configuration variables, which also indicates that we can customise the configuration centre\'s property files.\\n\\n There is a static code block inside ConfigurationFactory as follows:\\n\\n io.seata.config.ConfigurationFactory\\n\\n io.seata.config.ConfigurationFactory ! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191211102702.png)\\n\\n According to the custom file name configuration variable to find out the name and type of configuration file, if not configured, the default use registry.conf, FileConfiguration is the default configuration implementation class of Seata, if the default value, it will be more registry.conf configuration file to generate the FileConfiguration default configuration object, here you can also use the SPP configuration centre. Configuration object, here you can also use the SPI mechanism to support third-party extended configuration implementation, the specific implementation is to inherit the ExtConfigurationProvider interface, create a file in `META-INF/services/` and fill in the full path name of the implementation class, as shown below:\\n\\n ! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191211194643.png)\\n\\n\\n\\n # Third-party configuration centre implementation class loading\\n\\n After the static code block logic loads the configuration centre properties, how does Seata select the configuration centre and get the configuration centre property values?\\n\\n As we just said FileConfiguration is the default configuration implementation class for Seata, it inherits from AbstractConfiguration, which has a base class Configuration and provides methods to get parameter values:\\n\\n ```java\\n short getShort(String dataId, int defaultValue, long timeoutMills);\\n int getInt(String dataId, int defaultValue, long timeoutMills);\\n long getLong(String dataId, long defaultValue, long timeoutMills); int getInt(String dataId, int defaultValue, long timeoutMills); long getLong(String dataId, long defaultValue, long timeoutMills); //\\n // ....\\n ```\\n\\n So that means that all that is needed is for a third party configuration centre to implement this interface and integrate into the Seata Configuration Centre, I\'ll use zk as an example below:\\n\\n First, the third-party configuration centre needs to implement a Provider class:\\n\\n ! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191211200155.png)\\n\\n The provider method, as its name suggests, mainly outputs a specific Configuration implementation class.\\n\\n So how do we get the corresponding third-party Configuration Centre implementation class based on the configuration?\\n\\n In the Seata project, this is how to get a third-party Configuration Centre implementation:\\n\\n ```java\\n Configuration CONFIG = ConfigurationFactory.getInstance(); ``java\\n ```\\n\\nIn the getInstance() method the singleton pattern is mainly used to construct the configuration implementation class, which is constructed as follows:\\n\\nio.seata.configuration.ConfigurationFactory#buildConfiguration:\\n\\n! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191211102905.png)\\n\\nFirst of all, the static code block in ConfigurationFactory gets the configuration centre used by the current environment from the CURRENT_FILE_INSTANCE created by registry.conf, which is of type File by default. We can also configure other third-party configuration centres in registry.conf. We can also configure other third-party configuration centers in registry.conf. Here, we also use the SPI mechanism to load the implementation class of the third-party configuration centre, the specific implementation is as follows:\\n\\n! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191211205127.png)\\n\\nAs above, that is what I just said ZookeeperConfigurationProvider configuration implementation output class, let\'s take a look at this line of code:\\n\\n ```java\\n EnhancedServiceLoader.load(ConfigurationProvider.class,Objects.requireNonNull(configType).name()).provide();\\n ``\\n\\n The EnhancedServiceLoader is the core class of the Seata SPI implementation, and this line of code loads the class names of the files in the `META-INF/services/` and `META-INF/seata/` directories, so what happens if more than one of these Configuration Centre implementation classes are loaded?\\n\\n We notice that the ZookeeperConfigurationProvider class has an annotation above it:\\n\\n ```java\\n @LoadLevel(name = \\"ZK\\", order = 1)\\n ```\\n\\nWhen loading multiple Configuration Centre implementation classes, they are sorted according to order:\\n\\nio.seata.common.loader.EnhancedServiceLoader#findAllExtensionClass:\\n\\n! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191211210438.png)\\n\\nio.seata.common.loader.EnhancedServiceLoader#loadFile:\\n\\n! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191211210347.png)\\n\\nIn this way, there is no conflict.\\n\\nBut we find that Seata can also use this method for selection, and Seata passes a parameter when calling the load method:\\n\\n ```java\\n Objects.requireNonNull(configType).name()\\n ```\\n\\nConfigType is the configuration centre type, which is an enumerated class:\\n\\n ```java\\n public enum ConfigType {\\n  File, ZK, Nacos, Apollo, Consul, Etcd3, SpringCloudConfig, Custom.\\n }\\n ```\\n\\nWe notice that there is also a name attribute on the LoadLevel annotation, which Seata also does when filtering implementation classes:\\n\\n! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191211211210.png)\\n\\nIf the name is equal to LoadLevel\'s name attribute, then it is the currently configured third-party configuration centre implementation class.\\n\\n\\n\\n# Third-party configuration centre implementation class\\n\\nZookeeperConfiguration inherits AbstractConfiguration and has the following constructor:\\n\\n! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191211202510.png)\\n\\nThe constructor creates a zkClient object, what is FILE_CONFIG here?\\n\\n```java\\nprivate static final Configuration FILE_CONFIG = ConfigurationFactory.CURRENT_FILE_INSTANCE;\\n ```\\n\\n It turns out to be the registry.conf configuration implementation class created in the static code block, from which you get the properties of the third-party Configuration Centre, construct the third-party Configuration Centre client, and then implement the Configuration interface:\\n\\n ! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191211203735.png)\\n\\n Then you can use the relevant methods of the client to get the corresponding parameter values from the third-party configuration.\\n\\n\\n\\n # Third-party configuration centre configuration synchronization script\\n\\n I wrote it last weekend and submitted it to PR, it\'s still under review, and it\'s expected to be available in Seata 1.0, so please look forward to it.\\n\\n It\'s located in the script directory of the Seata project:\\n\\n ! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191211212141.png)\\n\\n config.txt is a locally configured value, after setting up the third-party configuration centre, running the script will sync the config.txt configuration to the third-party configuration centre.\\n\\n\\n # Author\'s Bio\\n\\n Zhang Chenghui, currently working in the technology platform department of the information centre of Zhongtong Technology, as a Java engineer, mainly responsible for the development of the Zhongtong messaging platform and the all-links pressure test project, loves to share technology, author of WeChat\'s public number \\"Backend Advancement\\", and technology blog ([https://objcoding.com/](https://objcoding.com/)) Blogger, Seata Contributor, GitHub ID: objcoding."},{"id":"/seata-nacos-docker","metadata":{"permalink":"/blog/seata-nacos-docker","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-nacos-docker.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-nacos-docker.md","title":"Docker Deployment of Seata Integration with Nacos","description":"This article explains how to deploy Seata integrated with Nacos configuration using Docker.","date":"2019-12-03T00:00:00.000Z","formattedDate":"December 3, 2019","tags":[],"readingTime":12.13,"hasTruncateMarker":false,"authors":[{"name":"FUNKYE"}],"frontMatter":{"title":"Docker Deployment of Seata Integration with Nacos","keywords":["Seata","Nacos","distributed transaction"],"description":"This article explains how to deploy Seata integrated with Nacos configuration using Docker.","author":"FUNKYE","date":"2019/12/03"},"unlisted":false,"prevItem":{"title":"Seata Configuration Center Implementation Principles","permalink":"/blog/seata-config-center"},"nextItem":{"title":"Configuring Seata Distributed Transaction with Nacos as the Configuration Center","permalink":"/blog/seata-nacos-analysis"}},"content":"Running the demo used [project address](https://gitee.com/itCjb/springboot-dubbo-mybatisplus-seata)\\n\\nAuthor: FUNKYE (Chen Jianbin), Hangzhou, an Internet company main program.\\n\\n# Preface\\n\\nSeata configuration for direct connection [blog](/blog/springboot-dubbo-mybatisplus-seata/)\\n\\nSeata Integration with Nacos Configuration [blog](/blog/seata-nacos-analysis/)\\n\\nLet\'s go back to the basics of the previous posts to configure nacos as a configuration centre and dubbo registry.\\n\\n## Preparation\\n\\n1. Install docker\\n\\n ```shell\\n yum -y install docker\\n ```\\n\\n 2. Create the nacos and seata databases.\\n\\n ```mysql\\n /******************************************/\\n /* Full database name = nacos */\\n /* Table name = config_info */\\n /******************************************/\\n CREATE TABLE `config_info` (\\n  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT \'id\', `data_id` v\\n  `data_id` varchar(255) NOT NULL COMMENT \'data_id\', `group_id` varchar(255) AUTO_INCREMENT COMMENT\\n  `group_id` varchar(255) DEFAULT NULL, `content` longtext NOT NULL\\n  `content` longtext NOT NULL COMMENT \'content\', `md5` varchar(255)\\n  `md5` varchar(32) DEFAULT NULL COMMENT \'md5\', `gmt_create` longtext NOT NULL COMMENT\\n  `gmt_create` datetime NOT NULL DEFAULT \'2010-05-05 00:00:00\' COMMENT \'Creation time\',\\n  `gmt_modified` datetime NOT NULL DEFAULT \'2010-05-05 00:00:00\' COMMENT \'Modified\', `src_user` datetime NOT NULL\\n  `src_user` text COMMENT \'source user\',\\n  `src_ip` varchar(20) DEFAULT NULL COMMENT \'source ip\', `app_name` varchar(20) DEFAULT NULL COMMENT \'2010-05-05 00:00:00\' COMMENT\\n  `app_name` varchar(128) DEFAULT NULL, `tenant_id` varchar(20)\\n  `tenant_id` varchar(128) DEFAULT \'\' COMMENT \'tenant field\',\\n  `c_desc` varchar(256) DEFAULT NULL,\\n  `c_use` varchar(64) DEFAULT NULL, `c_desc` varchar(256) DEFAULT\\n  `effect` varchar(64) DEFAULT NULL,\\n  `type` varchar(64) DEFAULT NULL,\\n  `c_schema` text, `c_schema` text, `c_schema` text\\n  PRIMARY KEY (`id`),\\n  UNIQUE KEY `uk_configinfo_datagrouptenant` (`data_id`,`group_id`,`tenant_id`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=\'config_info\';\\n\\n /******************************************/\\n /* Full database name = nacos_config */\\n /* Table name = config_info_aggr */\\n /******************************************/\\n CREATE TABLE `config_info_aggr` (\\n  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT \'id\',\\n  `data_id` varchar(255) NOT NULL COMMENT \'data_id\', `group_id` varchar(255) AUTO_INCREMENT COMMENT\\n  `group_id` varchar(255) NOT NULL COMMENT \'group_id\', `datum_id` varchar(255) NOT NULL COMMENT\\n  `datum_id` varchar(255) NOT NULL COMMENT \'datum_id\', `content` longtext NOT NULL COMMENT \'data_id\', `group_id` varchar(255)\\n  `content` longtext NOT NULL COMMENT \'\u5185\u5bb9\',\\n  `gmt_modified` datetime NOT NULL COMMENT \'modification time\', `app_name` varchar(255) NOT NULL COMMENT\\n  `app_name` varchar(128) DEFAULT NULL, `tenant_id` varchar(128) COMMENT\\n  `tenant_id` varchar(128) DEFAULT \'\' COMMENT \'Tenant field\',\\n  PRIMARY KEY (`id`),\\n  UNIQUE KEY `uk_configinfoaggr_datagrouptenantdatum` (`data_id`,`group_id`,`tenant_id`,`datum_id`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=\'Add tenant field\';\\n\\n\\n /******************************************/\\n /* Full database name = nacos_config */\\n /* Table name = config_info_beta */\\n /******************************************/\\n CREATE TABLE `config_info_beta` (\\n  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT \'id\',\\n  `data_id` varchar(255) NOT NULL COMMENT \'data_id\', `group_id` varchar(255) AUTO_INCREMENT COMMENT\\n  `group_id` varchar(128) NOT NULL COMMENT \'group_id\', `app_name` varchar(255) NOT NULL COMMENT\\n  `app_name` varchar(128) DEFAULT NULL COMMENT \'app_name\', `content` longtext NOT NULL\\n  `content` longtext NOT NULL COMMENT \'content\', `beta_ips` varchar(128)\\n  `beta_ips` varchar(1024) DEFAULT NULL COMMENT \'betaIps\', `md5` varchar(1024) DEFAULT NULL COMMENT\\n  `md5` varchar(32) DEFAULT NULL COMMENT \'md5\', `gmt_create` varchar(1024) DEFAULT NULL COMMENT\\n  `gmt_create` datetime NOT NULL DEFAULT \'2010-05-05 00:00:00\' COMMENT \'Creation Time\',\\n  `gmt_modified` datetime NOT NULL DEFAULT \'2010-05-05 00:00:00\' COMMENT \'Modified\', `src_user` datetime NOT NULL\\n  `src_user` text COMMENT \'source user\',\\n  `src_ip` varchar(20) DEFAULT NULL COMMENT \'source ip\', `tenant_id` varchar(20) DEFAULT NULL COMMENT \'2010-05-05 00:00:00\' COMMENT\\n  `tenant_id` varchar(128) DEFAULT \'\' COMMENT \'tenant field\',\\n  PRIMARY KEY (`id`),\\n  UNIQUE KEY `uk_configinfobeta_datagrouptenant` (`data_id`,`group_id`,`tenant_id`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=\'config_info_beta\';\\n\\n /******************************************/\\n /* Full database name = nacos_config */\\n /* Table name = config_info_tag */\\n /******************************************/\\n CREATE TABLE `config_info_tag` (\\n  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT \'id\',\\n  `data_id` varchar(255) NOT NULL COMMENT \'data_id\', `group_id` varchar(255) AUTO_INCREMENT COMMENT\\n  `group_id` varchar(128) NOT NULL COMMENT \'group_id\', `tenant_id` varchar(255) NOT NULL COMMENT\\n  `tenant_id` varchar(128) DEFAULT \'\' COMMENT \'tenant_id\', `tag_id` varchar(128) DEFAULT\\n  `tag_id` varchar(128) NOT NULL COMMENT \'tag_id\', `app_name` varchar(128) DEFAULT \'\' COMMENT\\n  `app_name` varchar(128) DEFAULT NULL COMMENT \'app_name\',\\n  `content` longtext NOT NULL COMMENT \'content\', `md5` varchar(128) DEFAULT NULL COMMENT\\n  `md5` varchar(32) DEFAULT NULL COMMENT \'md5\', `gmt_create\\n  `gmt_create` datetime NOT NULL DEFAULT \'2010-05-05 00:00:00\' COMMENT \'Creation time\',\\n  `gmt_modified` datetime NOT NULL DEFAULT \'2010-05-05 00:00:00\' COMMENT \'Modified\', `src_user` datetime NOT NULL\\n  `src_user` text COMMENT \'source user\',\\n  `src_ip` varchar(20) DEFAULT NULL COMMENT \'source ip\', `src_user` text COMMENT\\n  PRIMARY KEY (`id`), UNIQUE KEY `src_ip` varchar(20)\\n  UNIQUE KEY `uk_configinfotag_datagrouptenanttag` (`data_id`,`group_id`,`tenant_id`,`tag_id`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=\'config_info_tag\';\\n\\n /******************************************/\\n /* Full database name = nacos_config */\\n /* Table name = config_tags_relation */\\n /******************************************/\\n CREATE TABLE `config_tags_relation` (\\n  `id` bigint(20) NOT NULL COMMENT \'id\', `tag_name` v\\n  `tag_name` varchar(128) NOT NULL COMMENT \'tag_name\', `tag_type` varchar(20) NOT NULL COMMENT\\n  `tag_type` varchar(64) DEFAULT NULL COMMENT \'tag_type\',\\n  `data_id` varchar(255) NOT NULL COMMENT \'data_id\', `group_id` varchar(255) DEFAULT COMMENT\\n  `group_id` varchar(128) NOT NULL COMMENT \'group_id\', `tenant_id` varchar(255) NOT NULL COMMENT\\n  `tenant_id` varchar(128) DEFAULT \'\' COMMENT \'tenant_id\', `nid` bigint(128) NOT NULL COMMENT\\n  `nid` bigint(20) NOT NULL AUTO_INCREMENT,\\n  PRIMARY KEY (`nid`),\\n  UNIQUE KEY `uk_configtagrelation_configidtag` (`id`,`tag_name`,`tag_type`),\\n  KEY `idx_tenant_id` (`tenant_id`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=\'config_tag_relation\';\\n\\n /******************************************/\\n /* Full database name = nacos_config */\\n /* Table name = group_capacity */\\n /******************************************/\\n CREATE TABLE `group_capacity` (\\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT \'Primary key ID\',\\n  `group_id` varchar(128) NOT NULL DEFAULT \'\' COMMENT \'Group ID, null character indicates entire cluster\', `quota` int(10)\\n  `quota` int(10) unsigned NOT NULL DEFAULT \'0\' COMMENT \'Quota, a 0 indicates that the default value is being used\',\\n  `usage` int(10) unsigned NOT NULL DEFAULT \'0\' COMMENT \'Usage\',\\n  `max_size` int(10) unsigned NOT NULL DEFAULT \'0\' COMMENT \'Individual configuration size limit in bytes, 0 means use the default\',\\n  `max_aggr_count` int(10) unsigned NOT NULL DEFAULT \'0\' COMMENT \'Maximum number of aggregate subconfigurations,, 0 means use default\',, `max_aggr_count` int(10) unsigned NOT NULL DEFAULT \'0\'\\n  `max_aggr_size` int(10) unsigned NOT NULL DEFAULT \'0\' COMMENT \'Maximum subconfiguration size in bytes for a single aggregated data,, 0 means use default\',, `max_history_size` int(10) unsigned NOT NULL DEFAULT \'0\'\\n  `max_history_count` int(10) unsigned NOT NULL DEFAULT \'0\' COMMENT \'Maximum number of change history counts\',\\n  `gmt_create` datetime NOT NULL DEFAULT \'2010-05-05 00:00:00\' COMMENT \'Creation Time\',\\n  `gmt_modified` datetime NOT NULL DEFAULT \'2010-05-05 00:00:00\' COMMENT \'Modified time\',\\n  PRIMARY KEY (`id`),\\n  UNIQUE KEY `uk_group_id` (`group_id`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=\'Cluster, Capacity Information Table by Group\';\\n\\n /******************************************/\\n /* Full database name = nacos_config */\\n /* Table name = his_config_info */\\n /******************************************/\\n CREATE TABLE `his_config_info` (\\n  `id` bigint(64) unsigned NOT NULL, `nid` bigint(64) unsigned NOT NULL, `his_config_info\\n  `nid` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `data_id` v\\n  `data_id` varchar(255) NOT NULL, `group_id` varchar(255) NOT NULL, `group_id` varchar(255) NOT NULL, `group_id` varchar(255) NOT NULL\\n  `group_id` varchar(128) NOT NULL, `app_name` varchar(255)\\n  `app_name` varchar(128) DEFAULT NULL COMMENT \'app_name\',\\n  `content` longtext NOT NULL, `md5` varchar(128) DEFAULT NULL\\n  `md5` varchar(32) DEFAULT NULL, `gmt_create\\n  `gmt_create` datetime NOT NULL DEFAULT \'2010-05-05 00:00:00\',\\n  `gmt_modified` datetime NOT NULL DEFAULT \'2010-05-05 00:00:00\', `src_user` datetime NOT NULL\\n  `src_user` text, `src_ip` datetime NOT NULL\\n  `src_ip` varchar(20) DEFAULT NULL, `op_type` char(20) DEFAULT\\n  `op_type` char(10) DEFAULT NULL, `tenant_id` varchar(20)\\n  `tenant_id` varchar(128) DEFAULT \'\' COMMENT \'Tenant field\',\\n  PRIMARY KEY (`nid`),\\n  KEY `idx_gmt_create` (`gmt_create`),\\n  KEY `idx_gmt_modified` (`gmt_modified`),\\n  KEY `idx_did` (`data_id`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=\'Multi-Tenant Transformation\';\\n\\n\\n /******************************************/\\n /* Full database name = nacos_config */\\n /* Table name = tenant_capacity */\\n /******************************************/\\n CREATE TABLE `tenant_capacity` (\\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT \'Primary key ID\', `tenant_id` v\\n  `tenant_id` varchar(128) NOT NULL DEFAULT \'\' COMMENT \'Tenant ID\',\\n  `quota` int(10) unsigned NOT NULL DEFAULT \'0\' COMMENT \'Quota, 0 means use the default value\',\\n  `usage` int(10) unsigned NOT NULL DEFAULT \'0\' COMMENT \'Usage\',\\n  `max_size` int(10) unsigned NOT NULL DEFAULT \'0\' COMMENT \'Individual configuration size limit in bytes, 0 means use the default\',\\n  `max_aggr_count` int(10) unsigned NOT NULL DEFAULT \'0\' COMMENT \'Maximum number of aggregated sub-configurations\',\\n  `max_aggr_size` int(10) unsigned NOT NULL DEFAULT \'0\' COMMENT \'Maximum subconfiguration size in bytes for a single aggregation data, 0 means use default\',\\n  `max_history_count` int(10) unsigned NOT NULL DEFAULT \'0\' COMMENT \'Maximum number of change history counts\',\\n  `gmt_create` datetime NOT NULL DEFAULT \'2010-05-05 00:00:00\' COMMENT \'Creation time\',\\n  `gmt_modified` datetime NOT NULL DEFAULT \'2010-05-05 00:00:00\' COMMENT \'Modified time\',\\n  PRIMARY KEY (`id`),\\n  UNIQUE KEY `uk_tenant_id` (`tenant_id`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=\'Tenant capacity information table\';\\n\\n\\n CREATE TABLE `tenant_info` (\\n  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT \'id\',\\n  `kp` varchar(128) NOT NULL COMMENT \'kp\', `tenant_id` varchar(20) AUTO_INCREMENT COMMENT\\n  `tenant_id` varchar(128) default \'\' COMMENT \'tenant_id\',\\n  `tenant_name` varchar(128) default \'\' COMMENT \'tenant_name\',\\n  `tenant_desc` varchar(256) DEFAULT NULL COMMENT \'tenant_desc\', `create_source` varchar(256)\\n  `create_source` varchar(32) DEFAULT NULL COMMENT \'create_source\', `gmt_create` varchar(256) DEFAULT NULL COMMENT\\n  `gmt_create` bigint(20) NOT NULL COMMENT \'create_time\', `gmt_modify` varchar(32) DEFAULT NULL COMMENT\\n  `gmt_modified` bigint(20) NOT NULL COMMENT \'modified_time\', `gmt_modified` bigint(20) NOT NULL COMMENT\\n  PRIMARY KEY (`id`),\\n  UNIQUE KEY `uk_tenant_info_kptenantid` (`kp`,`tenant_id`),\\n  KEY `idx_tenant_id` (`tenant_id`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT=\'tenant_info\';\\n\\n CREATE TABLE users (\\n  username varchar(50) NOT NULL PRIMARY KEY, password varchar(500) NOT NULL\\n  password varchar(500) NOT NULL,\\n  enabled boolean NOT NULL\\n NULL, enabled boolean NOT NULL ); CREATE TABLE users\\n\\n CREATE TABLE roles (\\n  username varchar(50) NOT NULL, role varchar(50) NOT NULL, enabled boolean NOT NULL ); CREATE TABLE\\n  role varchar(50) NOT NULL\\n ); INSERT INTO users (username varchar(50) NOT NULL, role varchar(50) NOT NULL ); CREATE TABLE\\n\\n INSERT INTO users (username, password, enabled) VALUES (\'nacos\', \'$2a$10$EuWPZHzz32dJN7jexM34MOeYirDdFAZm2kuWj7VEOJhhZkDrxfvUu\', TRUE);; CREATE TABLE roles ( username, role)\\n\\n INSERT INTO roles (username, role) VALUES (\'nacos\', \'ROLE_ADMIN\');\\n\\n ```\\n\\n ```mysql\\n -- the table to store GlobalSession data\\n CREATE TABLE IF NOT EXISTS `global_table`\\n (\\n    `xid` VARCHAR(128) NOT NULL, `transaction_id` BARCHAR(128)\\n    `transaction_id` BIGINT, `status` TINYL\\n    `status`                    TINYINT      NOT NULL,\\n    `application_id` VARCHAR(32), `transaction_service\\n    `transaction_service_group` VARCHAR(32),\\n    `transaction_name` VARCHAR(128),\\n    `timeout`                   INT,\\n    `begin_time`                BIGINT,\\n    `application_data` VARCHAR(2000), `gmt_create\\n    `gmt_create`                DATETIME,\\n    `gmt_modified`              DATETIME,\\n    PRIMARY KEY (`xid`),\\n    KEY `idx_gmt_modified_status` (`gmt_modified`, `status`),\\n    KEY `idx_transaction_id` (`transaction_id`)\\n ) ENGINE = InnoDB\\n  DEFAULT CHARSET = utf8.\\n\\n -- the table to store BranchSession data\\n CREATE TABLE IF NOT EXISTS `branch_table`\\n (\\n    `branch_id` BIGINT NOT NULL, `xid` VARCHARGE\\n    `xid`               VARCHAR(128) NOT NULL,\\n    `transaction_id`    BIGINT,\\n    `resource_group_id` VARCHAR(32), `resource_id` VARCHAR(32), `transaction_id` BIGINT\\n    `resource_id`       VARCHAR(256),\\n    `branch_type` VARCHAR(8), `status` TINYINT\\n    `status`            TINYINT,\\n    `client_id` VARCHAR(64), `application_data` TINYINT, `client_id` VARCHAR(64), `application_data` TINYINT\\n    `application_data` VARCHAR(2000), `gmt_create\\n    `gmt_create`        DATETIME(6),\\n    `gmt_modified`      DATETIME(6),\\n    PRIMARY KEY (`branch_id`), `branch_id`, `idx_x\\n    KEY `idx_xid` (`xid`)\\n ) ENGINE = InnoDB\\n  DEFAULT CHARSET = utf8; -- the table to store lock data.\\n\\n -- the table to store lock data\\n CREATE TABLE IF NOT EXISTS `lock_table`\\n (\\n    `row_key` VARCHAR(128) NOT NULL, `xid` VARCHAR(128) NOT NULL, -- the table to store lock data\\n    `xid`            VARCHAR(128),\\n    `transaction_id` BIGINT, `branch_id` BIGINT, `branch_id` BIGINT\\n    `branch_id` BIGINT NOT NULL, `resource_id` VARCHAR(128)\\n    `resource_id`    VARCHAR(256),\\n    `table_name`     VARCHAR(32),\\n    `pk` VARCHAR(36), `gmt_create` VARCHAR(256), `gmt_create\\n    `gmt_create` DATETIME, `gmt_modify` VARCHAR(256), `pk` VARCHAR(36), `gmt_create` DATETIME\\n    `gmt_modified`   DATETIME,\\n    PRIMARY KEY (`row_key`),\\n    KEY `idx_branch_id` (`branch_id`)\\n ) ENGINE = InnoDB\\n  DEFAULT CHARSET = utf8.\\n\\n ```\\n\\n3. Pull the nacos and seata mirrors and run them.\\n\\n ```shell\\n docker run -d --name nacos -p 8848:8848 -e MODE=standalone -e MYSQL_MASTER_SERVICE_HOST=your mysql ip -e MYSQL_MASTER_SERVICE_DB_NAME=nacos -e MYSQL_MASTER_SERVICE_USER=root -e MYSQL_MASTER_SERVICE_PASSWORD=mysql password -e MYSQL_SLAVE_SERVICE_HOST=your mysql ip -e SPRING_DATASOURCE_PLATFORM=mysql PLATFORM=mysql -e MYSQL_DATABASE_NUM=1 nacos/nacos-server:latest\\n ```\\n\\n ```shell\\n docker run -d --name seata -p 8091:8091 -e SEATA_IP=the ip you want to specify -e SEATA_PORT=8091 seataio/seata-server:1.4.2\\n ```\\n\\n## Seata Configuration\\n\\n1. Since there is no built-in vim in the seata container, we can directly cp the folder to the host and then cp it to go back.\\n\\n```\\ndocker cp container id:seata-server/resources The directory you want to place the folder in.\\n ```\\n\\n 2. Get the ip addresses of the two containers using the following code\\n\\n ```\\ndocker inspect --format=\'{{.NetworkSettings.IPAddress}}\' ID/NAMES\\n ```\\n\\n 3. nacos-config.txt is edited as follows\\n\\n ```\\ntransport.type=TCP\\ntransport.server=NIO\\ntransport.heartbeat=true\\ntransport.enableClientBatchSendRequest=false\\ntransport.threadFactory.bossThreadPrefix=NettyBoss\\ntransport.threadFactory.workerThreadPrefix=NettyServerNIOWorker\\ntransport.threadFactory.serverExecutorThreadPrefix=NettyServerBizHandler\\ntransport.threadFactory.shareBossWorker=false\\ntransport.threadFactory.clientSelectorThreadPrefix=NettyClientSelector\\ntransport.threadFactory.clientSelectorThreadSize=1\\ntransport.threadFactory.clientWorkerThreadPrefix=NettyClientWorkerThread\\ntransport.threadFactory.bossThreadSize=1\\ntransport.threadFactory.workerThreadSize=default\\ntransport.shutdown.wait=3\\nservice.vgroupMapping.Your transaction group name=default\\nservice.default.grouplist=127.0.0.1:8091\\nservice.enableDegrade=false\\nservice.disableGlobalTransaction=false\\nclient.rm.asyncCommitBufferLimit=10000\\nclient.rm.lock.retryInterval=10\\nclient.rm.lock.retryTimes=30\\nclient.rm.lock.retryPolicyBranchRollbackOnConflict=true\\nclient.rm.reportRetryCount=5\\nclient.rm.tableMetaCheckEnable=false\\nclient.rm.tableMetaCheckerInterval=60000\\nclient.rm.sqlParserType=druid\\nclient.rm.reportSuccessEnable=false\\nclient.rm.sagaBranchRegisterEnable=false\\nclient.rm.commitRetryCount=5\\nclient.tm.rollbackRetryCount=5\\nclient.tm.defaultGlobalTransactionTimeout=60000\\nclient.tm.degradeCheck=false\\nclient.tm.degradeCheckAllowTimes=10\\nclient.tm.degradeCheckPeriod=2000\\nstore.mode=file\\nstore.publicKey=\\nstore.file.dir=file_store/data\\nstore.file.maxBranchSessionSize=16384\\nstore.file.maxGlobalSessionSize=512\\nstore.file.fileWriteBufferCacheSize=16384\\nstore.file.flushDiskMode=async\\nstore.file.sessionReloadReadSize=100\\nstore.db.datasource=druid\\nstore.db.dbType=mysql\\nstore.db.driverClassName=com.mysql.jdbc.\\nstore.db.url=jdbc:mysql://your mysql host ip:3306/seata?useUnicode=true&rewriteBatchedStatements=true\\nstore.db.user=mysql account\\nstore.db.password=mysql password\\nstore.db.minConn=5\\nstore.db.maxConn=30\\nstore.db.globalTable=global_table\\nstore.db.branchTable=branch_table\\nstore.db.queryLimit=100\\nstore.db.lockTable=lock_table\\nstore.db.maxWait=5000\\nserver.recovery.committingRetryPeriod=1000\\nserver.recovery.asynCommittingRetryPeriod=1000\\nserver.recovery.rollbackingRetryPeriod=1000\\nserver.recovery.timeoutRetryPeriod=1000\\nserver.maxCommitRetryTimeout=-1\\nserver.maxRollbackRetryTimeout=-1\\nserver.rollbackRetryTimeoutUnlockEnable=false\\nclient.undo.dataValidation=true\\nclient.undo.logSerialisation=jackson\\nclient.undo.onlyCareUpdateColumns=true\\nserver.undo.logSaveDays=7\\nserver.undo.logDeletePeriod=86400000\\nclient.undo.logTable=undo_log\\nclient.undo.compress.enable=true\\nclient.undo.compress.type=zip\\nclient.undo.compress.threshold=64k\\nlog.exceptionRate=100\\ntransport.serialisation=seata\\ntransport.compressor=none\\nmetrics.enabled=false\\nmetrics.registryType=compact\\nmetrics.exporterList=prometheus\\nmetrics.exporterPrometheusPort=9898\\n ```\\n\\n Click [here](/docs/user/configurations/) for detailed parameter configurations.\\n\\n 4. registry.conf is edited as follows\\n\\n ```\\nregistry {\\n# file, nacos, eureka, redis, zk, consul, etcd3, sofa\\ntype = \\"nacos\\"\\n\\nnacos {\\nserverAddr = \\"nacos container ip:8848\\"\\nnamespace = \\"\\"\\ncluster = \\"default\\"\\n}\\n}\\n\\nconfig {\\n# file, nacos, apollo, zk, consul, etcd3\\ntype = \\"nacos\\"\\n\\nnacos {\\nserverAddr = \\"nacos container ip:8848\\"\\nnamespace = \\"\\"\\n}\\n}\\n ```\\n\\n 5. After the configuration is complete, use the following command to copy the modified registry.conf to the container, and reboot to view the logs running\\n\\n ```shell\\n docker cp /home/seata/resources/registry.conf seata:seata-server/resources/\\n docker restart seata\\n docker logs -f seata\\n ```\\n\\n 6. Run nacos-config.sh to import the Nacos configuration.\\n\\n eg: sh $\\\\{SEATAPATH}/script/config-center/nacos/nacos-config.sh -h localhost -p 8848 -g SEATA_GROUP -t 5a3c7d6c-f497-4d68-a71a-2e5e3340b3ca - u username -w password u username -w password\\n\\n Refer to [Configuration Import Instructions](https://github.com/apache/incubator-seata/blob/1.4.2/script/config-center/README.md) for specific parameter definitions.\\n\\n 7. Log in to the nacos control centre to see\\n\\n ! [20191202205912](/img/blog/20191202205912.png)\\n\\n As shown in the picture is successful.\\n\\n # Debugging\\n\\n 1. Pull the project shown in the blog post and modify the application.yml and registry.conf of test-service.\\n\\n ```\\nregistry {\\ntype = \\"nacos\\"\\nnacos {\\nserverAddr = \\"host ip:8848\\"\\nnamespace = \\"\\"\\ncluster = \\"default\\"\\n}\\n}\\nconfig {\\ntype = \\"nacos\\"\\nnacos {\\nserverAddr = \\"host ip:8848\\"\\nnamespace = \\"\\"\\ncluster = \\"default\\"\\n}\\n}\\n\\n ```\\n\\n ```\\nserver.\\nport: 38888\\nspring.\\nname: test-service\\nname: test-service\\ndatasource: type: com.alibaba.druid.pool.\\ntype: com.alibaba.druid.pool.\\nurl: jdbc:mysql://mysqlip:3306/test?useUnicode=true&characterEncoding=UTF-8&serverTimezone=UTC\\ndriver-class-name: com.mysql.cj.jdbc.\\ndriver-class-name: com.mysql.cj.jdbc.\\ndriver-class-name: com.mysql.cj.jdbc.driver username: root\\ndriver-class-name: com mysql.cj.jdbc.\\ndriver-class-name: com mysql.cj.jdbc.\\nthreadpool: cached\\nscan.\\nbase-packages: com.example\\napplication: qos-enable: false\\nqos-enable: false\\nname: testserver\\nregistry: id: my-registry\\nid: my-registry\\naddress: nacos://host ip:8848\\nmybatis-plus.\\nmapper-locations: classpath:/mapper/*Mapper.xml\\ntypeAliasesPackage: org.test.entity\\nglobal-config.\\ndb-config.\\nfield-strategy: not-empty\\ndb-config: field-strategy: not-empty\\ndb-type: mysql\\nconfiguration: map-underscore-to-camel-case: true\\nmap-underscore-to-camel-case: true\\ncache-enabled: true\\nlog-impl: org.apache.ibatis.logging.stdout.\\nauto-mapping-unknown-column-behavior: none\\n ```\\n\\n 2. Copy the modified registry.conf to test-client-resources, and modify the application\\n\\n ```\\nspring: application.\\napplication: name: test\\nname: test\\ndatasource: driver-class-name: com.mysql.\\ndriver-class-name: com.mysql.cj.jdbc.\\nurl: jdbc:mysql://mysqlIp:3306/test?userSSL=true&useUnicode=true&characterEncoding=UTF8&serverTimezone=Asia/Shanghai\\nusername: root\\npassword: 123456\\nmvc.\\nservlet.\\nload-on-startup: 1\\nhttp.\\nhttp: http: encoding: http: encoding: http: force: true\\nforce: true\\ncharset: utf-8\\nenabled: true\\nmultipart: max-file-size: 10MB\\nmax-file-size: 10MB\\nmax-request-size: 10MB\\ndubbo.\\ndubbo: registry: id: my-registry\\nid: my-registry\\naddress: nacos://host ip:8848\\napplication.\\nname: dubbo-demo-client\\nqos-enable: false\\nserver: name: dubbo-demo-client qos-enable: false\\nport: 28888\\nmax-http-header-size: 8192\\naddress: 0.0.0.0\\ntomcat: max-http-post-size: 104857600\\nmax-http-post-size: 104857600\\n ```\\n\\n 4. Execute the undo_log script on each db involved.\\n\\n ```sql\\n CREATE TABLE IF NOT EXISTS `undo_log`\\n (\\n    `branch_id` BIGINT NOT NULL COMMENT \'branch transaction id\', `xid` VARCHARCHARCHARCHARCHARCHARCHARCHARCHARGE\\n    `xid` VARCHAR(128) NOT NULL COMMENT \'global transaction id\', `context` VARCHAR(128) NOT NULL COMMENT\\n    `context` VARCHAR(128) NOT NULL COMMENT \'undo_log context,such as serialisation\', `rollback_info` VARCHAR(128) NOT NULL COMMENT\\n    `rollback_info` LONGBLOB NOT NULL COMMENT \'rollback info\', `log_status` INTRODUCTION\\n    `log_status` INT(11) NOT NULL COMMENT \'0:normal status,1:defence status\', `log_created` DAT\\n    `log_created` DATETIME(6) NOT NULL COMMENT \'creation datetime\', `log_modified` DATETIME(6) NOT NULL COMMENT\\n    `log_modified` DATETIME(6) NOT NULL COMMENT \'modify datetime\', `log_modified` DATETIME(6) NOT NULL COMMENT\\n    UNIQUE KEY `ux_undo_log` (`xid`, `branch_id`)\\n ) ENGINE = InnoDB\\n  AUTO_INCREMENT = 1\\n  DEFAULT CHARSET = utf8 COMMENT =\'AT transaction mode undo table\';\\n ```\\n\\n 5. Run test-service,test-client in that order.\\n\\n 6. See if the list of services in nacos is as shown below.\\n\\n ! [20191203132351](/img/blog/20191203132351.png)\\n\\n # Summary\\n\\n The docker deployment of nacos and seata has been completed, for more details I would like you to visit the following address to read the detailed documentation\\n\\n [nacos official website](https://nacos.io/zh-cn/index.html)\\n\\n [dubbo official website](http://dubbo.apache.org/en-us/)\\n\\n [seata website](https://seata.apache.org/zh-cn/)\\n\\n [docker official website](https://www.docker.com/)"},{"id":"/seata-nacos-analysis","metadata":{"permalink":"/blog/seata-nacos-analysis","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-nacos-analysis.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-nacos-analysis.md","title":"Configuring Seata Distributed Transaction with Nacos as the Configuration Center","description":"This article explains how to integrate Seata with Nacos for configuration.","date":"2019-12-02T00:00:00.000Z","formattedDate":"December 2, 2019","tags":[],"readingTime":5.56,"hasTruncateMarker":false,"authors":[{"name":"FUNKYE"}],"frontMatter":{"title":"Configuring Seata Distributed Transaction with Nacos as the Configuration Center","keywords":["Seata","Nacos","distributed transaction"],"description":"This article explains how to integrate Seata with Nacos for configuration.","author":"FUNKYE","date":"2019/12/02"},"unlisted":false,"prevItem":{"title":"Docker Deployment of Seata Integration with Nacos","permalink":"/blog/seata-nacos-docker"},"nextItem":{"title":"Seata Community Meetup\xb7Hangzhou Station","permalink":"/blog/seata-meetup-hangzhou"}},"content":"[Project address](https://gitee.com/itCjb/springboot-dubbo-mybatisplus-seata )\\n\\nThis article was written by FUNKYE (Chen Jianbin), the main programme of an Internet company in Hangzhou.\\n\\n# Preface\\n\\nThe last release of the direct connection method of seata configuration, you can see the details of this [blog](/blog/springboot-dubbo-mybatisplus-seata)\\n\\nWe then go on the basis of the previous article to configure nacos to do configuration centre and dubbo registry.\\n\\n## Preparation\\n\\n1. First of all, go to the nacos github to download [the latest version](https://github.com/alibaba/nacos/releases/tag/1.1.4)\\n\\n! [](/img/blog/20191202203649.png)\\n\\n2. after the download, very simple, unzip to the bin directory to start on it, see as shown in the picture on it:\\n\\n! [](/img/blog/20191202203943.png)\\n\\n3. start finished visit:http://127.0.0.1:8848/nacos/#/login\\n\\n! [](/img/blog/20191202204101.png)\\n\\nDid you see this interface? Enter nacos (account password is the same), go in and take a look.\\n\\nAt this time you can find that there is no service registration\\n\\n! [20191202204147](/img/blog/20191202204147.png)\\n\\nDon\'t worry, let\'s get the seata service connected.\\n\\n## Seata configuration\\n\\n1. Go to seata\'s conf folder and see this ?\\n\\nSee this folder? [](/img/blog/20191202204259.png)\\n\\nThat\'s it, edit it: !\\n\\n! [20191202204353](/img/blog/20191202204353.png)\\n\\n! [20191202204437](/img/blog/20191202204437.png)\\n\\n2. Then remember to save it! Next we open the registry.conf file to edit it:\\n\\n```\\nregistry {\\n  # file \u3001nacos \u3001eureka\u3001redis\u3001zk\u3001consul\u3001etcd3\u3001sofa\\n  type = \\"nacos\\"\\n\\n  nacos {\\n    serverAddr = \\"localhost\\"\\n    namespace = \\"\\"\\n    cluster = \\"default\\"\\n  }\\n  eureka {\\n    serviceUrl = \\"http://localhost:8761/eureka\\"\\n    application = \\"default\\"\\n    weight = \\"1\\"\\n  }\\n  redis {\\n    serverAddr = \\"localhost:6379\\"\\n    db = \\"0\\"\\n  }\\n  zk {\\n    cluster = \\"default\\"\\n    serverAddr = \\"127.0.0.1:2181\\"\\n    session.timeout = 6000\\n    connect.timeout = 2000\\n  }\\n  consul {\\n    cluster = \\"default\\"\\n    serverAddr = \\"127.0.0.1:8500\\"\\n  }\\n  etcd3 {\\n    cluster = \\"default\\"\\n    serverAddr = \\"http://localhost:2379\\"\\n  }\\n  sofa {\\n    serverAddr = \\"127.0.0.1:9603\\"\\n    application = \\"default\\"\\n    region = \\"DEFAULT_ZONE\\"\\n    datacenter = \\"DefaultDataCenter\\"\\n    cluster = \\"default\\"\\n    group = \\"SEATA_GROUP\\"\\n    addressWaitTime = \\"3000\\"\\n  }\\n  file {\\n    name = \\"file.conf\\"\\n  }\\n}\\n\\nconfig {\\n  # file\u3001nacos \u3001apollo\u3001zk\u3001consul\u3001etcd3\\n  type = \\"nacos\\"\\n\\n  nacos {\\n    serverAddr = \\"localhost\\"\\n    namespace = \\"\\"\\n  }\\n  consul {\\n    serverAddr = \\"127.0.0.1:8500\\"\\n  }\\n  apollo {\\n    app.id = \\"seata-server\\"\\n    apollo.meta = \\"http://192.168.1.204:8801\\"\\n  }\\n  zk {\\n    serverAddr = \\"127.0.0.1:2181\\"\\n    session.timeout = 6000\\n    connect.timeout = 2000\\n  }\\n  etcd3 {\\n    serverAddr = \\"http://localhost:2379\\"\\n  }\\n  file {\\n    name = \\"file.conf\\"\\n  }\\n}\\n\\n```\\n\\n After all the editing, we run nacos-config.sh, and the content of our configured nacos-config.txt is sent to nacos as shown in the figure:\\n\\n ! [20191202205743](/img/blog/20191202205743.png)\\n\\n The appearance of the above similar code is an indication of success, then we log in to the nacos configuration centre to view the configuration list, the appearance of the list as shown in the figure shows that the configuration is successful:\\n\\n ! [20191202205912](/img/blog/20191202205912.png)\\n\\n see it, your configuration has all been committed, if then git tool run sh does not work, try to edit the sh file, try to change the operation to the following\\n\\n```shell\\nfor line in $(cat nacos-config.txt)\\n\\ndo\\n\\nkey=${line%%=*}\\nvalue=${line#*=}\\necho \\"\\\\r\\\\n set \\"${key}\\" = \\"${value}\\n\\nresult=`curl -X POST \\"http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=$key&group=SEATA_GROUP&content=$value\\"`\\n\\nif [ \\"$result\\"x == \\"true\\"x ]; then\\n\\n  echo \\"\\\\033[42;37m $result \\\\033[0m\\"\\n\\nelse\\n\\n  echo \\"\\\\033[41;37 $result \\\\033[0m\\"\\n  let error++\\n\\nfi\\n\\ndone\\n\\n\\nif [ $error -eq 0 ]; then\\n\\necho  \\"\\\\r\\\\n\\\\033[42;37m init nacos config finished, please start seata-server. \\\\033[0m\\"\\n\\nelse\\n\\necho  \\"\\\\r\\\\n\\\\033[41;33m init nacos config fail. \\\\033[0m\\"\\n\\nfi\\n```\\n\\n 3. At present, our preparations are all complete, we go to seata-service/bin to run the seata service it, as shown in the figure on the success!\\n\\n ! [20191202210112](/img/blog/20191202210112.png)\\n\\n # Debugging\\n\\n 1. first springboot-dubbo-mybatsiplus-seata project pom dependency changes, remove zk these configurations, because we use nacos to do the registry.\\n\\n ```java\\n\\t<properties>\\n\\t\\t<webVersion>3.1</webVersion>\\n\\t\\t<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\\n\\t\\t<project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\\n\\t\\t<maven.compiler.source>1.8</maven.compiler.source>\\n\\t\\t<maven.compiler.target>1.8</maven.compiler.target>\\n\\t\\t<HikariCP.version>3.2.0</HikariCP.version>\\n\\t\\t<mybatis-plus-boot-starter.version>3.2.0</mybatis-plus-boot-starter.version>\\n\\t</properties>\\n\\t<parent>\\n\\t\\t<groupId>org.springframework.boot</groupId>\\n\\t\\t<artifactId>spring-boot-starter-parent</artifactId>\\n\\t\\t<version>2.1.8.RELEASE</version>\\n\\t</parent>\\n\\t<dependencies>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>com.alibaba.nacos</groupId>\\n\\t\\t\\t<artifactId>nacos-client</artifactId>\\n\\t\\t\\t<version>1.1.4</version>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.apache.dubbo</groupId>\\n\\t\\t\\t<artifactId>dubbo-registry-nacos</artifactId>\\n\\t\\t\\t<version>2.7.4.1</version>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.apache.dubbo</groupId>\\n\\t\\t\\t<artifactId>dubbo-spring-boot-starter</artifactId>\\n\\t\\t\\t<version>2.7.4.1</version>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.apache.commons</groupId>\\n\\t\\t\\t<artifactId>commons-lang3</artifactId>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>com.alibaba</groupId>\\n\\t\\t\\t<artifactId>fastjson</artifactId>\\n\\t\\t\\t<version>1.2.60</version>\\n\\t\\t</dependency>\\n\\t\\t\x3c!-- <dependency> <groupId>javax</groupId> <artifactId>javaee-api</artifactId>\\n\\t\\t\\t<version>7.0</version> <scope>provided</scope> </dependency> --\x3e\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>io.springfox</groupId>\\n\\t\\t\\t<artifactId>springfox-swagger2</artifactId>\\n\\t\\t\\t<version>2.9.2</version>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>io.springfox</groupId>\\n\\t\\t\\t<artifactId>springfox-swagger-ui</artifactId>\\n\\t\\t\\t<version>2.9.2</version>\\n\\t\\t</dependency>\\n\\n\\t\\t\x3c!-- mybatis-plus begin --\x3e\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>com.baomidou</groupId>\\n\\t\\t\\t<artifactId>mybatis-plus-boot-starter</artifactId>\\n\\t\\t\\t<version>${mybatis-plus-boot-starter.version}</version>\\n\\t\\t</dependency>\\n\\t\\t\x3c!-- mybatis-plus end --\x3e\\n\\t\\t\x3c!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --\x3e\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.projectlombok</groupId>\\n\\t\\t\\t<artifactId>lombok</artifactId>\\n\\t\\t\\t<scope>provided</scope>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>io.seata</groupId>\\n\\t\\t\\t<artifactId>seata-all</artifactId>\\n\\t\\t\\t<version>0.9.0.1</version>\\n\\t\\t</dependency>\\n\\t\\t\x3c!-- <dependency> <groupId>com.baomidou</groupId> <artifactId>dynamic-datasource-spring-boot-starter</artifactId>\\n\\t\\t\\t<version>2.5.4</version> </dependency> --\x3e\\n\\n\\t\\t\x3c!-- <dependency> <groupId>com.baomidou</groupId> <artifactId>mybatis-plus-generator</artifactId>\\n\\t\\t\\t<version>3.1.0</version> </dependency> --\x3e\\n\\t\\t\x3c!-- https://mvnrepository.com/artifact/org.freemarker/freemarker --\x3e\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.freemarker</groupId>\\n\\t\\t\\t<artifactId>freemarker</artifactId>\\n\\t\\t</dependency>\\n\\t\\t\x3c!-- https://mvnrepository.com/artifact/com.alibaba/druid-spring-boot-starter --\x3e\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>com.alibaba</groupId>\\n\\t\\t\\t<artifactId>druid-spring-boot-starter</artifactId>\\n\\t\\t\\t<version>1.1.20</version>\\n\\t\\t</dependency>\\n\\t\\t\x3c!-- \u52a0\u4e0a\u8fd9\u4e2a\u624d\u80fd\u8fa8\u8ba4\u5230log4j2.yml\u6587\u4ef6 --\x3e\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>com.fasterxml.jackson.dataformat</groupId>\\n\\t\\t\\t<artifactId>jackson-dataformat-yaml</artifactId>\\n\\t\\t</dependency>\\n\\t\\t<dependency> \x3c!-- \u5f15\u5165log4j2\u4f9d\u8d56 --\x3e\\n\\t\\t\\t<groupId>org.springframework.boot</groupId>\\n\\t\\t\\t<artifactId>spring-boot-starter-log4j2</artifactId>\\n\\t\\t</dependency>\\n\\t\\t\x3c!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --\x3e\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>mysql</groupId>\\n\\t\\t\\t<artifactId>mysql-connector-java</artifactId>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.springframework.boot</groupId>\\n\\t\\t\\t<artifactId>spring-boot-starter-web</artifactId>\\n\\t\\t\\t<exclusions>\\n\\t\\t\\t\\t<exclusion>\\n\\t\\t\\t\\t\\t<groupId>org.springframework.boot</groupId>\\n\\t\\t\\t\\t\\t<artifactId>spring-boot-starter-logging</artifactId>\\n\\t\\t\\t\\t</exclusion>\\n\\t\\t\\t\\t<exclusion>\\n\\t\\t\\t\\t\\t<groupId>org.slf4j</groupId>\\n\\t\\t\\t\\t\\t<artifactId>slf4j-log4j12</artifactId>\\n\\t\\t\\t\\t</exclusion>\\n\\t\\t\\t</exclusions>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.springframework.boot</groupId>\\n\\t\\t\\t<artifactId>spring-boot-starter-aop</artifactId>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.springframework.boot</groupId>\\n\\t\\t\\t<artifactId>spring-boot-starter-test</artifactId>\\n\\t\\t\\t<scope>test</scope>\\n\\t\\t</dependency>\\n\\t\\t\x3c!-- <dependency> <groupId>org.scala-lang</groupId> <artifactId>scala-library</artifactId>\\n\\t\\t\\t<version>2.11.0</version> </dependency> --\x3e\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.springframework.boot</groupId>\\n\\t\\t\\t<artifactId>spring-boot-configuration-processor</artifactId>\\n\\t\\t\\t<optional>true</optional>\\n\\t\\t</dependency>\\n\\t</dependencies>\\n\\n```\\n\\n2. Then change the directory structure of test-service, delete the configuration of zk and change the application.yml file, directory structure and code.\\n\\n```yaml\\nserver:\\n  port: 38888\\nspring:\\n  application:\\n    name: test-service\\n  datasource:\\n    type: com.alibaba.druid.pool.DruidDataSource\\n    url: jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&characterEncoding=UTF-8&serverTimezone=UTC\\n    driver-class-name: com.mysql.cj.jdbc.Driver\\n    username: root\\n    password: 123456\\ndubbo:\\n  protocol:\\n    loadbalance: leastactive\\n    threadpool: cached\\n  scan:\\n    base-packages: org\u3002test.service\\n  application:\\n    qos-enable: false\\n    name: testserver\\n  registry:\\n    id: my-registry\\n    address:  nacos://127.0.0.1:8848\\nmybatis-plus:\\n  mapper-locations: classpath:/mapper/*Mapper.xml\\n  typeAliasesPackage: org.test.entity\\n  global-config:\\n    db-config:\\n      field-strategy: not-empty\\n      id-type: auto\\n      db-type: mysql\\n  configuration:\\n    map-underscore-to-camel-case: true\\n    cache-enabled: true\\n    auto-mapping-unknown-column-behavior: none\\n```\\n\\n <img src=\\"/img/blog/20191202211833.png\\" alt=\\"20191202211833\\" style={{ zoom:\'100%\' }} />\\n\\n 3.then change the registry.conf file, if your nacos is another server, please change it to the corresponding ip and port.\\n\\n```java\\n registry {\\n  type = \\"nacos\\"\\n  file { name = \\"file.conf\\n    name = \\"file.conf\\"\\n  }\\n   zk {\\n    cluster = \\"default\\"\\n    serverAddr = \\"127.0.0.1:2181\\"\\n    session.timeout = 6000\\n    connect.timeout = 2000\\n  }\\n    nacos {\\n    serverAddr = \\"localhost\\"\\n    namespace = \\"\\"\\n    cluster = \\"default\\"\\n  }\\n }\\n config {\\n  type = \\"nacos\\"\\n  file { name = \\"file.conf\\n    name = \\"file.conf\\"\\n  }\\n  zk {\\n    serverAddr = \\"127.0.0.1:2181\\"\\n    session.timeout = 6000\\n    connect.timeout = 2000\\n  }\\n    nacos {\\n    serverAddr = \\"localhost\\"\\n    namespace = \\"\\"\\n    cluster = \\"default\\"\\n  }\\n }\\n ```\\n\\n4. Next, we run provideApplication\\n\\n! [20191202212000](/img/blog/20191202212000.png)\\n\\nThe startup is successful, and we look at the seata logs: !\\n\\n[20191202212028 [20191202212028](/img/blog/20191202212028.png)\\n\\nSuccess, this time we are the same, to modify the contents of the test-client, first of all the same application.yml, zk replaced by nacos, here will not describe in detail, the test-service within the registry.conf, copy to the client project resources to cover the original registry.conf.\\n\\nThen we can run clientApplication: !\\n\\n! [20191202212114](/img/blog/20191202212114.png)\\n\\n5. Confirm that the service has been published and test that the transaction is running correctly\\n\\n! [20191202212203](/img/blog/20191202212203.png)\\n\\nThe service is successfully published and consumed. Now let\'s go back to swagger and test the rollback to see if everything is ok, visit http://127.0.0.1:28888/swagger-ui.html\\n\\n! [20191202212240](/img/blog/20191202212240.png)\\n\\nCongratulations, see this must be as successful as me!\\n\\n# Summary\\n\\nAbout the use of nacos and seata simple build has been completed, more detailed content hope you visit the following address to read the detailed documentation\\n\\n[nacos official website](https://nacos.io/zh-cn/index.html)\\n\\n[dubbo official website](http://dubbo.apache.org/en-us/)\\n\\n[seata official website](http://seata.apache.org/zh-cn/)"},{"id":"/seata-meetup-hangzhou","metadata":{"permalink":"/blog/seata-meetup-hangzhou","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-meetup-hangzhou.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-meetup-hangzhou.md","title":"Seata Community Meetup\xb7Hangzhou Station","description":"Seata Community Meetup\xb7Hangzhou Station was successfully held at Zhejiang Youth Innovation Space, Dream Town, Hangzhou on December 21st.","date":"2019-12-01T00:00:00.000Z","formattedDate":"December 1, 2019","tags":[],"readingTime":0.715,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Seata Community Meetup\xb7Hangzhou Station","keywords":["Seata","Hangzhou","meetup"],"date":"2019/12/01","description":"Seata Community Meetup\xb7Hangzhou Station was successfully held at Zhejiang Youth Innovation Space, Dream Town, Hangzhou on December 21st."},"unlisted":false,"prevItem":{"title":"Configuring Seata Distributed Transaction with Nacos as the Configuration Center","permalink":"/blog/seata-nacos-analysis"},"nextItem":{"title":"Resolving the Issue of Losing Mybatis-Plus Features in Seata AT Mode Integration through Source Code","permalink":"/blog/seata-mybatisplus-analysis"}},"content":"![](https://img.alicdn.com/tfs/TB1qH2YwVP7gK0jSZFjXXc5aXXa-2002-901.jpg)\\n\\n### Event Introduction\\n\\n### Highlight Interpretation\\n\\n- Seata project founder presented \\"Seata Past, Present, and Future\\" along with new features of Seata 1.0.\\n- Seata core contributors elaborated on Seata AT, TCC, and Saga modes.\\n- Implementation analysis of Seata in internet healthcare and Didi Chuxing.\\n\\n- [Replay benefits (Developer Community)](https://developer.aliyun.com/live/1760)\\n- [Join Seata thousand-member DingTalk group](http://w2wz.com/h2nb)\\n\\n### Guest Speakers\\n\\n- Ji Min (Qing Ming) \\"Seata Past, Present, and Future\\" [slides](https://github.com/funky-eyes/awesome-seata/blob/master/slides/meetup/201912%40hangzhou/%E5%AD%A3%E6%95%8F%EF%BC%88%E6%B8%85%E9%93%AD%EF%BC%89%E3%80%8ASeata%20%E7%9A%84%E8%BF%87%E5%8E%BB%E3%80%81%E7%8E%B0%E5%9C%A8%E5%92%8C%E6%9C%AA%E6%9D%A5%E3%80%8B.pdf)\\n\\n  ![](https://img.alicdn.com/tfs/TB1BALWw4z1gK0jSZSgXXavwpXa-6720-4480.jpg)\\n\\n- Wu Jiangke \\"My Open Source Journey with SEATA and SEATA\'s Application in Internet Healthcare Systems\\" [slides](https://github.com/seata/awesome-seata/blob/master/slides/meetup/201912%40hangzhou/%E5%AD%A3%E6%95%8F%EF%BC%88%E6%B8%85%E9%93%AD%EF%BC%89%E3%80%8ASeata%20%E7%9A%84%E8%BF%87%E5%8E%BB%E3%80%81%E7%8E%B0%E5%9C%A8%E5%92%8C%E6%9C%AA%E6%9D%A5%E3%80%8B.pdf)\\n\\n  ![1577282651](https://img.alicdn.com/tfs/TB1Xzz1w4v1gK0jSZFFXXb0sXXa-6720-4480.jpg)\\n\\n- Shen Haiqiang (Xuan Yi) \\"Essence of Seata AT Mode\\" [slides](https://github.com/seata/awesome-seata/tree/master/slides/meetup/201912%40hangzhou)\\n\\n  ![1577282652](https://img.alicdn.com/tfs/TB1UK22w7T2gK0jSZPcXXcKkpXa-6720-4480.jpg)\\n\\n- Zhang Sen \\"Detailed Explanation of TCC Mode in Distributed Transaction Seata\\"\\n\\n  ![1577282653](https://img.alicdn.com/tfs/TB1fCPZw.T1gK0jSZFhXXaAtVXa-6720-4480.jpg)\\n\\n- Chen Long (Yiyuan) \\"Seata Long Transaction Solution Saga Mode\\"\\n\\n  ![1577282654](https://img.alicdn.com/tfs/TB1zLv3wYj1gK0jSZFuXXcrHpXa-6720-4480.jpg)\\n\\n- Chen Pengzhi \\"Seata Practice in Didi Chuxing\'s Motorcycle Business\\" [slides](https://github.com/seata/awesome-seata/blob/master/slides/meetup/201912%40hangzhou/%E9%99%88%E9%B9%8F%E5%BF%97%E3%80%8ASeata%20%E5%9C%A8%E6%BB%B4%E6%BB%B4%E4%B8%A4%E8%BD%AE%E8%BD%A6%E4%B8%9A%E5%8A%A1%E7%9A%84%E5%AE%9E%E8%B7%B5%E3%80%8B.pdf)\\n\\n  ![1577282655](https://img.alicdn.com/tfs/TB1phvYw4n1gK0jSZKPXXXvUXXa-6720-4480.jpg)\\n\\n### Special Awards\\n\\n![](https://img.alicdn.com/tfs/TB1khDVw.z1gK0jSZLeXXb9kVXa-6720-4480.jpg)"},{"id":"/seata-mybatisplus-analysis","metadata":{"permalink":"/blog/seata-mybatisplus-analysis","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-mybatisplus-analysis.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-mybatisplus-analysis.md","title":"Resolving the Issue of Losing Mybatis-Plus Features in Seata AT Mode Integration through Source Code","description":"This article explains how to resolve the issue of losing Mybatis-Plus features in Seata integration through source code.","date":"2019-11-30T00:00:00.000Z","formattedDate":"November 30, 2019","tags":[],"readingTime":10.58,"hasTruncateMarker":false,"authors":[{"name":"FUNKYE"}],"frontMatter":{"title":"Resolving the Issue of Losing Mybatis-Plus Features in Seata AT Mode Integration through Source Code","keywords":["Seata","Mybatis-Plus","distributed transaction"],"description":"This article explains how to resolve the issue of losing Mybatis-Plus features in Seata integration through source code.","author":"FUNKYE","date":"2019/11/30"},"unlisted":false,"prevItem":{"title":"Seata Community Meetup\xb7Hangzhou Station","permalink":"/blog/seata-meetup-hangzhou"},"nextItem":{"title":"Integrating Seata Distributed Transaction with SpringBoot+Dubbo+MybatisPlus","permalink":"/blog/springboot-dubbo-mybatisplus-seata"}},"content":"Project address: https://gitee.com/itCjb/springboot-dubbo-mybatisplus-seata\\n\\nAuthor: FUNKYE (Chen Jianbin), Hangzhou, an Internet company programmer.\\n\\n# Introduction\\n\\nMybatis-Plus: [MyBatis-Plus](https://github.com/baomidou/mybatis-plus) (MP for short) is an [MyBatis](http://www.mybatis.org/mybatis-3/) enhancement tool in the MyBatis on the basis of only enhancements do not change , in order to simplify the development , improve efficiency and born .\\n\\nMP configuration:\\n\\n```xml\\n<bean id=\\"sqlSessionFactory\\" class=\\"com.baomidou.mybatisplus.extension.spring.MybatisSqlSessionFactoryBean\\">\\n<property name=\\"dataSource\\" ref=\\"dataSource\\"/>\\n</bean\\n```\\n\\n Seata: Seata is an open source distributed transaction solution , is committed to providing high-performance and easy to use distributed transaction services . Seata will provide users with AT, TCC, SAGA and XA transaction patterns , to create a one-stop distributed solution for users .\\n\\n AT mode mechanism:\\n\\n - Phase I: Business data and rollback log records are committed in the same local transaction, releasing local locks and connection resources.\\n - Phase II:\\n  - Commit asynchronised and completed very quickly.\\n  - Rollbacks are back-compensated by the phase 1 rollback log.\\n\\n ## Analyse the causes\\n\\n 1. First of all, through the introduction, we can see that mp is required to register the sqlSessionFactory and inject the data source, while Seata is to ensure the normal rollback and commit of the transaction through the proxy data source.\\n\\n 2. Let\'s look at the SeataAutoConfig code based on the official Seata demo.\\n\\n ```java\\n package org.test.config;\\n\\n import javax.sql.DataSource;\\n\\n import org.apache.ibatis.session.\\n import org.slf4j.Logger; import org.slf4j.\\n import org.slf4j.LoggerFactory; import org.springframework.\\n import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.\\n import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties; import org.springframework.boot.autoconfigure.jdbc.\\n import org.springframework.context.annotation.\\n import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.\\n import org.springframework.context.annotation.\\n\\n import com.alibaba.druid.pool.DruidDataSource; import com.baomidou.pool.\\n import com.alibaba.druid.pool.DruidDataSource; import com.baomidou.mybatisplus.extension.spring.\\n\\n import io.seata.rm.datasource.DataSourceProxy; import io.seata.rm.datasource.\\n import io.seata.rm.datasource.DataSourceProxy; import io.seata.spring.annotation.\\n\\n @Configuration\\n public class SeataAutoConfig {\\n  @Autowired(required = true)\\n  private DataSourceProperties dataSourceProperties; private final static Logger logger; @Autowired(required = true)\\n  private final static Logger logger = LoggerFactory.getLogger(SeataAutoConfig.class);\\n\\n  @Bean(name = \\"dataSource\\") // Declare it as a bean instance.\\n  @Primary // In the same DataSource, first use the labelled DataSource\\n  public DataSource druidDataSource() {\\n   DruidDataSource druidDataSource = new DruidDataSource();\\n   logger.info(\\"dataSourceProperties.getUrl():{}\\",dataSourceProperties.getUrl());\\n   druidDataSource.setUrl(dataSourceProperties.getUrl());\\n   druidDataSource.setUsername(dataSourceProperties.getUsername());\\n   druidDataSource.setPassword(dataSourceProperties.getPassword());\\n   druidDataSource.setDriverClassName(dataSourceProperties.getDriverClassName()); druidDataSource.setDriverClassName(dataSourceProperties.getDriverClassName());\\n   druidDataSource.setInitialSize(0);\\n   druidDataSource.setMaxActive(180);\\n   druidDataSource.setMaxWait(60000);\\n   druidDataSource.setMinIdle(0); druidDataSource.setMinIdle(0);\\n   druidDataSource.setValidationQuery(\\"Select 1 from DUAL\\");\\n   druidDataSource.setTestOnBorrow(false); druidDataSource.setTestOnBorrow(false);\\n   druidDataSource.setTestOnReturn(false); druidDataSource.\\n   druidDataSource.setTestWhileIdle(true); druidDataSource.\\n   druidDataSource.setTimeBetweenEvictionRunsMillis(60000); druidDataSource.\\n   druidDataSource.setMinEvictableIdleTimeMillis(25200000); druidDataSource.\\n   druidDataSource.setRemoveAbandoned(true);\\n   druidDataSource.setRemoveAbandonedTimeout(1800); druidDataSource.setRemoveAbandonedTimeout(1800);\\n   druidDataSource.setLogAbandoned(true);\\n   logger.info(\\"Loading dataSource ........\\") ;\\n   return druidDataSource;\\n  }\\n\\n  /**\\n   * init datasource proxy\\n   * @Param: druidDataSource datasource bean instance\\n   * @Param: druidDataSource datasource bean instance\\n   * @Return: DataSourceProxy datasource proxy\\n   */\\n  @Bean\\n  public DataSourceProxy dataSourceProxy(DataSource dataSource) {\\n   logger.info(\\"Proxy dataSource ........\\") ;\\n   return new DataSourceProxy(dataSource);\\n  }\\n\\n  @Bean\\n  public SqlSessionFactory sqlSessionFactory(DataSourceProxy dataSourceProxy) throws Exception {\\n   MybatisSqlSessionFactoryBean factory = new MybatisSqlSessionFactoryBean();\\n   MybatisSqlSessionFactoryBean = new MybatisSqlSessionFactoryBean(); factory.setDataSource(dataSourceProxy);\\n        factory.setMapperLocations(new PathMatchingResourcePatternResolver()); factory.setMapperLocations(new PathMatchingResourcePatternResolver())\\n            .getResources(\\"classpath*:/mapper/*.xml\\")); factory.setMapperLocations(new PathMatchingResourcePatternResolver())\\n   return factory.getObject();\\n  }\\n\\n  /**\\n   * init global transaction scanner\\n   * @Return: GlobalTransactionScanner\\n   * @Return: GlobalTransactionScanner\\n   */\\n  @Bean\\n  public GlobalTransactionScanner globalTransactionScanner() {\\n   logger.info(\\"Configuring seata........\\") ;\\n   return new GlobalTransactionScanner(\\"test-service\\", \\"test-group\\");\\n  }\\n }\\n\\n ```\\n\\nFirst of all, we see that in our seata configuration datasource class, we have configured a datasource, and then we have configured a seata proxy datasource bean, and this time.\\n\\nThen we if we directly start the mp integration seata project will find that paging and other plug-ins will be directly invalid , even scanning mapper have to write from the code , this is why?\\n\\nBy reading the above code, because we have another configuration of a sqlSessionFactory, resulting in mp\'s sqlSessionFactory failure, this time we found the problem, even if we do not configure sqlSessionFactoryl, but also because of the mp data source used is not seata proxy After the data source used by mp is not proxied by seata, resulting in distributed transaction failure. But how to solve this problem?\\n\\nWe need to read the source code of mp and find its startup class.\\n\\n ```java\\n /* /* /* /* /* /* /* /*\\n * Copyright (c) 2011-2020, baomidou (jobob@qq.com).\\n * <p>\\n * Licensed under the Apache License, Version 2.0 (the \\"License\\"); you may not\\n * use this file except in compliance with the Licence. You may obtain a copy of * the Licence at\\n * the License at\\n * <p>\\n * https://www.apache.org/licenses/LICENSE-2.0\\n * <p>\\n * Unless required by applicable law or agreed to in writing, software * distributed under the Licence is distributed on an \\"AS IS\\" BASIS.\\n * distributed under the Licence is distributed on an \\"AS IS\\" BASIS, WITHOUT\\n * WARRANTIES OR CONDITIONS OF ANY KIND, either expressed or implied.\\n * Licence for the specific language governing permissions and limitations under\\n * the Licence.\\n */\\n package com.baomidou.mybatisplus.autoconfigure;\\n\\n\\n import com.baomidou.mybatisplus.core.MybatisConfiguration; import com.baomidou.mybatisplus.core.config.\\n import com.baomidou.mybatisplus.core.config.GlobalConfig; import com.baomidou.mybatisplus.core.\\n import com.baomidou.mybatisplus.core.handlers.\\n import com.baomidou.mybatisplus.core.incrementer.IKeyGenerator; import com.baomidou.mybatisplus.core.\\n import com.baomidou.mybatisplus.core.injector.ISqlInjector; import com.baomidou.mybatisplus.core.injector.\\n import com.baomidou.mybatisplus.extension.spring.MybatisSqlSessionFactoryBean; import org.apache.ibache.\\n import org.apache.ibatis.annotations.\\n import org.apache.ibatis.mapping.DatabaseIdProvider; import org.apache.ibatis.mapping.\\n import org.apache.ibatis.mapping.DatabaseIdProvider; import org.apache.ibatis.plugin.\\n import org.apache.ibatis.scripting.LanguageDriver; import org.apache.ibatis.scripting.\\n import org.apache.ibatis.scripting.LanguageDriver; import org.apache.ibatis.session.\\n import org.apache.ibatis.session.SqlSessionFactory; import org.apache.ibatis.session.\\n import org.apache.ibatis.session.SqlSessionFactory; import org.apache.ibatis.type.\\n import org.mybatis.spring.SqlSessionFactoryBean; import org.mybatis.spring.\\n import org.mybatis.spring.SqlSessionTemplate; import org.mybatis.spring.\\n import org.mybatis.spring.mapper.MapperFactoryBean; import org.mybatis.spring.mapper.\\n import org.mybatis.spring.mapper.MapperScannerConfigurer; import org.mybatis.spring.mapper.\\n import org.slf4j.Logger; import org.slf4j.\\n import org.slf4j.LoggerFactory; import org.springframework.\\n import org.springframework.beans.BeanWrapper; import org.springframework.beans.\\n import org.springframework.beans.BeanWrapperImpl; import org.springframework.beans.\\n import org.springframework.beans.factory.BeanFactory; import org.springframework.beans.\\n import org.springframework.beans.factory.BeanFactoryAware; import org.springframework.beans.factory.\\n import org.springframework.beans.factory.InitialisingBean; import org.springframework.beans.factory.\\n import org.springframework.beans.factory.ObjectProvider; import org.springframework.beans.factory.\\n import org.springframework.beans.factory.support.BeanDefinitionBuilder; import org.springframework.beans.factory.support.\\n import org.springframework.beans.factory.support.BeanDefinitionRegistry; import org.springframework.beans.factory.support.\\n import org.springframework.boot.autoconfigure.AutoConfigurationPackages; import org.springframework.boot.autoconfigure.\\n import org.springframework.boot.autoconfigure.AutoConfigureAfter; import org.springframework.boot.autoconfigure.\\n import org.springframework.boot.autoconfigure.EnableAutoConfiguration; import org.springframework.boot.autoconfigure.\\n import org.springframework.boot.autoconfigure.condition.ConditionalOnClass; import org.springframework.boot.autoconfigure.\\n import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean; import org.springframework.boot.autoconfigure.condition.\\n import org.springframework.boot.autoconfigure.condition.ConditionalOnSingleCandidate; import org.springframework.boot.autoconfigure.condition.\\n import org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration; import org.springframework.boot.autoconfigure.jdbc.\\n import org.springframework.boot.context.properties.EnableConfigurationProperties; import org.springframework.boot.context.properties.\\n import org.springframework.context.ApplicationContext; import org.springframework.context.\\n import org.springframework.context.annotation.\\n import org.springframework.context.annotation.\\n import org.springframework.context.annotation.Import; import org.springframework.context.annotation.\\n import org.springframework.context.annotation.ImportBeanDefinitionRegistrar; import org.springframework.context.annotation.\\n import org.springframework.context.annotation.ImportBeanDefinitionRegistrar; import org.springframework.core.io.\\n import org.springframework.core.io.ResourceLoader; import org.springframework.core.io.\\n import org.springframework.core.type.AnnotationMetadata; import org.springframework.core.io.\\n import org.springframework.core.type.AnnotationMetadata; import org.springframework.util.\\n import org.springframework.util.CollectionUtils; import org.springframework.util.\\n import org.springframework.util.ObjectUtils; import org.springframework.util.\\n import org.springframework.util.StringUtils; import org.springframework.util.\\n\\n import javax.sql.DataSource; import java.util.\\n import java.util.List; import java.util.\\n import java.util.Optional; import java.util.\\n import java.util.stream.\\n\\n /**\\n * {@link EnableAutoConfiguration Auto-Configuration} for Mybatis. Contributes a\\n * {@link SqlSessionFactory} and a {@link SqlSessionTemplate}.\\n * <p>\\n * If {@link org.mybatis.spring.annotation.MapperScan} is used, or a\\n * configuration file is specified as a property, those will be considered, * otherwise this auto-configuration will be considered.\\n * otherwise this auto-configuration will attempt to register mappers based on\\n * the interface definitions in or under the root auto-configuration package.\\n * </p\\n * <p> copy from {@link org.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration}</p>\\n * @author Edd\xfa Mel\xe9n\\n * @author Edd\xfa Mel\xe9ndez\\n * @author Josh Long\\n * @author Kazuki Shimizu\\n * @author Eduardo Macarr\xf3n\\n */\\n @Configuration\\n @ConditionalOnClass({SqlSessionFactory.class, SqlSessionFactoryBean.class})\\n @ConditionalOnSingleCandidate(DataSource.class)\\n @EnableConfigurationProperties(MybatisPlusProperties.class)\\n @AutoConfigureAfter(DataSourceAutoConfiguration.class)\\n public class MybatisPlusAutoConfiguration implements InitialisingBean {\\n\\n    private static final Logger logger = LoggerFactory.getLogger(MybatisPlusAutoConfiguration.class);\\n\\n    private final MybatisPlusProperties properties.\\n\\n    private final Interceptor[] interceptors; private final\\n\\n    private final TypeHandler[] typeHandlers; private final MybatisPlusProperties properties; private final\\n\\n    private final LanguageDriver[] languageDrivers.\\n\\n    private final ResourceLoader resourceLoader;\\n\\n    private final DatabaseIdProvider databaseIdProvider; private final\\n\\n    private final List<ConfigurationCustomizer> configurationCustomizers; private final List<ConfigurationCustomizer> configurationCustomizers.\\n\\n    private final List<MybatisPlusPropertiesCustomizer> mybatisPlusPropertiesCustomizers;\\n\\n    private final ApplicationContext applicationContext;\\n\\n\\n    public MybatisPlusAutoConfiguration(MybatisPlusProperties properties, MybatisPlusPropertiesCustomizers)\\n                                        ObjectProvider<Interceptor[]> interceptorsProvider, ObjectProvider<TypeHandler[]> interceptorsProvider, MybatisPlusAutoConfiguration(MybatisPlusProperties)\\n                                        ObjectProvider<TypeHandler[]> typeHandlersProvider, ObjectProvider<LanguageProvider\\n                                        ObjectProvider<LanguageDriver[]> languageDriversProvider,\\n                                        ResourceLoader resourceLoader,\\n                                        ObjectProvider<DatabaseIdProvider> databaseIdProvider,\\n                                        ObjectProvider<List<ConfigurationCustomizer>> configurationCustomizersProvider,\\n                                        ObjectProvider<List<MybatisPlusPropertiesCustomizer>> mybatisPlusPropertiesCustomizerProvider,\\n                                        ApplicationContext applicationContext) {\\n        this.properties = properties; this.interceptors = interceptors\\n        this.interceptors = interceptorsProvider.getIfAvailable();\\n        this.typeHandlers = typeHandlersProvider.getIfAvailable(); this.\\n        this.languageDrivers = languageDriversProvider.getIfAvailable(); this.\\n        this.resourceLoader = resourceLoader; this.databaseIdProvider.getIfAvailable()\\n        this.databaseIdProvider = databaseIdProvider.getIfAvailable(); this.\\n        this.configurationCustomizers = configurationCustomizersProvider.getIfAvailable(); this.\\n        this.mybatisPlusPropertiesCustomizers = mybatisPlusPropertiesCustomizerProvider.getIfAvailable(); this.\\n        this.applicationContext = applicationContext;\\n    }\\n\\n    @Override\\n    public void afterPropertiesSet() {\\n        if (!CollectionUtils.isEmpty(mybatisPlusPropertiesCustomizers)) {\\n            mybatisPlusPropertiesCustomizers.forEach(i -> i.customise(properties));\\n        }\\n        checkConfigFileExists();\\n    }\\n\\n    private void checkConfigFileExists() {\\n        if (this.properties.isCheckConfigLocation() && StringUtils.hasText(this.properties.getConfigLocation())) {\\n            Resource resource = this.resourceLoader.getResource(this.properties.getConfigLocation());\\n            Assert.state(resource.exists(),\\n                \\"Cannot find config location: \\" + resource + \\" (please add config file or check your Mybatis configuration)\\");\\n        }\\n    }\\n\\n    @SuppressWarnings(\\"SpringJavaInjectionPointsAutowiringInspection\\")\\n    @Bean\\n    @ConditionalOnMissingBean\\n    public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception {\\n        // TODO uses MybatisSqlSessionFactoryBean instead of SqlSessionFactoryBean.\\n        MybatisSqlSessionFactoryBean factory = new MybatisSqlSessionFactoryBean();\\n        factory.setDataSource(dataSource); factory.setVfs(SpringBean); factory.setVfs(SpringBean)\\n        factory.setVfs(SpringBootVFS.class);\\n        if (StringUtils.hasText(this.properties.getConfigLocation())) {\\n            factory.setConfigLocation(this.resourceLoader.getResource(this.properties.getConfigLocation())); }\\n        }\\n        applyConfiguration(factory).\\n        if (this.properties.getConfigurationProperties() ! = null) {\\n            factory.setConfigurationProperties(this.properties.getConfigurationProperties());\\n        }\\n        if (!ObjectUtils.isEmpty(this.interceptors)) {\\n            factory.setPlugins(this.interceptors); }\\n        }\\n        if (this.databaseIdProvider ! = null) {\\n            factory.setDatabaseIdProvider(this.databaseIdProvider); }\\n        }\\n        if (StringUtils.hasLength(this.properties.getTypeAliasesPackage())) {\\n            factory.setTypeAliasesPackage(this.properties.getTypeAliasesPackage()); }\\n        }\\n        if (this.properties.getTypeAliasesSuperType() ! = null) {\\n            factory.setTypeAliasesSuperType(this.properties.getTypeAliasesSuperType()); }\\n        }\\n        if (StringUtils.hasLength(this.properties.getTypeHandlersPackage())) {\\n            factory.setTypeHandlersPackage(this.properties.getTypeHandlersPackage()); }\\n        }\\n        if (!ObjectUtils.isEmpty(this.typeHandlers)) {\\n            factory.setTypeHandlers(this.typeHandlers); }\\n        }\\n        if (!ObjectUtils.isEmpty(this.properties.resolveMapperLocations())) {\\n            factory.setMapperLocations(this.properties.resolveMapperLocations()); }\\n        }\\n\\n        // TODO makes some changes to the source code (because it adapts to an older version of mybatis, but we don\'t need to).\\n        Class<? extends LanguageDriver> defaultLanguageDriver = this.properties.getDefaultScriptingLanguageDriver(); if (!\\n        if (!ObjectUtils.isEmpty(this.languageDrivers)) {\\n            factory.setScriptingLanguageDrivers(this.languageDrivers); }\\n        }\\n        Optional.ofNullable(defaultLanguageDriver).ifPresent(factory::setDefaultScriptingLanguageDriver);\\n\\n        // TODO custom enum package\\n        if (StringUtils.hasLength(this.properties.getTypeEnumsPackage())) {\\n            factory.setTypeEnumsPackage(this.properties.getTypeEnumsPackage());\\n        }\\n        // TODO This must be non-NULL.\\n        GlobalConfig globalConfig = this.properties.getGlobalConfig(); // TODO inject the filler.\\n        // TODO inject the filler\\n        if (this.applicationContext.getBeanNamesForType(MetaObjectHandler.class,\\n            false, false).length > 0) {\\n            MetaObjectHandler metaObjectHandler = this.applicationContext.getBean(MetaObjectHandler.class);\\n            globalConfig.setMetaObjectHandler(metaObjectHandler);\\n        }\\n        // TODO inject the primary key generator\\n        if (this.applicationContext.getBeanNamesForType(IKeyGenerator.class, false\\n            false).length > 0) {\\n            IKeyGenerator keyGenerator = this.applicationContext.getBean(IKeyGenerator.class);\\n            globalConfig.getDbConfig().setKeyGenerator(keyGenerator);\\n        }\\n        // TODO injecting the sql injector\\n        if (this.applicationContext.getBeanNamesForType(ISqlInjector.class, false,\\n            false).length > 0) {\\n            ISqlInjector iSqlInjector = this.applicationContext.getBean(ISqlInjector.class);\\n            globalConfig.setSqlInjector(iSqlInjector);\\n        }\\n        // TODO set GlobalConfig to MybatisSqlSessionFactoryBean\\n        factory.setGlobalConfig(globalConfig); return factory.getObject(MybatisSqlSessionFactoryBean); }\\n        factory.setGlobalConfig(globalConfig); return factory.getObject();\\n    }\\n\\n    // TODO entry using MybatisSqlSessionFactoryBean.\\n    private void applyConfiguration(MybatisSqlSessionFactoryBean factory) {\\n        // TODO using MybatisConfiguration\\n        MybatisConfiguration configuration = this.properties.getConfiguration(); if (configuration == null & null); }\\n        if (configuration == null && !StringUtils.hasText(this.properties.getConfigLocation()) {\\n            configuration = new MybatisConfiguration();\\n        }\\n        if (configuration ! = null && !CollectionUtils.isEmpty(this.configurationCustomizers)) {\\n            for (ConfigurationCustomizer customizer : this.configurationCustomizers) {\\n                customizer.customize(configuration);\\n            }\\n        }\\n        factory.setConfiguration(configuration); }\\n    }\\n\\n    @Bean\\n    @ConditionalOnMissingBean\\n    public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactory sqlSessionFactory) {\\n        ExecutorType executorType = this.properties.getExecutorType(); if (executorType !\\n        if (executorType ! = null) {\\n            return new SqlSessionTemplate(sqlSessionFactory, executorType); if (executorType !\\n        } else {\\n            return new SqlSessionTemplate(sqlSessionFactory); } else { new SqlSessionTemplate(sqlSessionFactory); }\\n        }\\n    }\\n\\n    /**} }\\n     * This will just scan the same base package as Spring Boot does. If you want more power, you can explicitly use\\n     * {@link org.mybatis.spring.annotation.MapperScan} but this will get typed mappers working correctly, out-of-the-box, * similar to using Spring Data JPA repositories.\\n     * similar to using Spring Data JPA repositories.\\n     */\\n    public static class AutoConfiguredMapperScannerRegistrar implements BeanFactoryAware, ImportBeanDefinitionRegistrar {\\n\\n        private BeanFactory beanFactory;\\n\\n        private BeanFactory beanFactory; @Override\\n        public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) {\\n\\n            if (!AutoConfigurationPackages.has(this.beanFactory)) {\\n                logger.debug(\\"Could not determine auto-configuration package, automatic mapper scanning disabled.\\"); return; { if (!AutoConfigurationPackages.has(this.beanFactory)) { if (!\\n                return;\\n            }\\n\\n            logger.debug(\\"Searching for mappers annotated with @Mapper\\");\\n\\n            List<String> packages = AutoConfigurationPackages.get(this.beanFactory);\\n            if (logger.isDebugEnabled()) {\\n                packages.forEach(pkg -> logger.debug(\\"Using auto-configuration base package \'{}\'\\", pkg));\\n            }\\n\\n            BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(MapperScannerConfigurer.class);\\n            builder.addPropertyValue(\\"ProcessPropertyPlaceHolders\\", true);\\n            builder.addPropertyValue(\\"annotationClass\\", Mapper.class); builder.addPropertyValue(\\"processPropertyPlaceHolders\\", true);\\n            builder.addPropertyValue(\\"basePackage\\", StringUtils.collectionToCommaDelimitedString(packages));\\n            BeanWrapper beanWrapper = new BeanWrapperImpl(MapperScannerConfigurer.class);\\n            Stream.of(beanWrapper.getPropertyDescriptors())\\n                // Need to mybatis-spring 2.0.2+\\n                .filter(x -> x.getName().equals(\\"lazyInitialisation\\")).findAny()\\n                .ifPresent(x -> builder.addPropertyValue(\\"lazyInitialization\\", \\"${mybatis.lazy-initialization:false}\\"));\\n            registry.registerBeanDefinition(MapperScannerConfigurer.class.getName(), builder.getBeanDefinition());\\n        }\\n\\n        @Override\\n        public void setBeanFactory(BeanFactory beanFactory) {\\n            this.beanFactory = beanFactory; } @Override public void setBeanFactory(beanFactory) { this.\\n        }\\n    }\\n\\n    /**\\n     * If mapper registering configuration or mapper scanning configuration not present, this configuration allow to scan\\n     * mappers based on the same component-scanning path as Spring Boot itself.\\n     */\\n    @Configuration\\n    @Import(AutoConfiguredMapperScannerRegistrar.class)\\n    @ConditionalOnMissingBean({MapperFactoryBean.class, MapperScannerConfigurer.class})\\n    public static class MapperScannerRegistrarNotFoundConfiguration implements InitialisingBean {\\n\\n        public void afterPropertiesSet\\n        public void afterPropertiesSet() {\\n            logger.debug(\\n                \\"Not found configuration for registering mapper bean using @MapperScan, MapperFactoryBean and MapperScannerConfigurer.\\");\\n        }\\n    }\\n }\\n\\n ```\\n\\nSee the sqlSessionFactory method in the mp startup class, it injects a data source in the same way, at this point you should know the solution, right?\\n\\nThat\'s right, is to be proxied to the data source to the mp sqlSessionFactory.\\n\\nIt\'s very simple, we need to slightly change our seata configuration class on the line\\n\\n```java\\npackage org.test.config; import javax.sql.\\n\\nimport javax.sql.DataSource; import org.mybatis.\\n\\nimport org.mybatis.spring.annotation.\\nimport org.slf4j.Logger; import org.slf4j.\\nimport org.slf4j.LoggerFactory; import org.springframework.\\nimport org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.\\nimport org.springframework.boot.autoconfigure.jdbc.DataSourceProperties; import org.springframework.boot.autoconfigure.jdbc.\\nimport org.springframework.context.annotation.\\nimport org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.\\nimport org.springframework.context.annotation.\\n\\nimport com.alibaba.druid.pool.DruidDataSource; import com.alibaba.druid.pool.\\n\\nimport io.seata.rm.datasource.DataSourceProxy; import io.seata.rm.datasource.\\nimport io.seata.spring.annotation.GlobalTransactionScanner; import io.seata.rm.datasource.\\n\\n@Configuration\\n@MapperScan(\\"com.baomidou.springboot.mapper*\\")\\npublic class SeataAutoConfig {\\n@Autowired(required = true)\\nprivate DataSourceProperties dataSourceProperties; private final static Logger logger; @Autowired(required = true)\\nprivate final static Logger logger = LoggerFactory.getLogger(SeataAutoConfig.class);\\nprivate DataSourceProxy dataSourceProxy;\\n\\n    @Bean(name = \\"dataSource\\") // Declare it as a bean instance.\\n    @Primary // In the same DataSource, the labelled DataSource is used first\\n    public DataSource druidDataSource() {\\n        DruidDataSource druidDataSource = new DruidDataSource();\\n        logger.info(\\"dataSourceProperties.getUrl():{}\\", dataSourceProperties.getUrl());\\n        druidDataSource.setUrl(dataSourceProperties.getUrl());\\n        druidDataSource.setUsername(dataSourceProperties.getUsername());\\n        druidDataSource.setPassword(dataSourceProperties.getPassword());\\n        druidDataSource.setDriverClassName(dataSourceProperties.getDriverClassName()); druidDataSource.setDriverClassName(dataSourceProperties.getDriverClassName());\\n        druidDataSource.setInitialSize(0);\\n        druidDataSource.setMaxActive(180);\\n        druidDataSource.setMaxWait(60000);\\n        druidDataSource.setMinIdle(0); druidDataSource.setMinIdle(0);\\n        druidDataSource.setValidationQuery(\\"Select 1 from DUAL\\");\\n        druidDataSource.setTestOnBorrow(false); druidDataSource.setTestOnBorrow(false);\\n        druidDataSource.setTestOnReturn(false); druidDataSource.\\n        druidDataSource.setTestWhileIdle(true); druidDataSource.\\n        druidDataSource.setTimeBetweenEvictionRunsMillis(60000); druidDataSource.\\n        druidDataSource.setMinEvictableIdleTimeMillis(25200000); druidDataSource.\\n        druidDataSource.setRemoveAbandoned(true);\\n        druidDataSource.setRemoveAbandonedTimeout(1800); druidDataSource.setRemoveAbandonedTimeout(1800);\\n        druidDataSource.setLogAbandoned(true);\\n        logger.info(\\"Loading dataSource ........\\") ;\\n        dataSourceProxy = new DataSourceProxy(druidDataSource);\\n        return dataSourceProxy;\\n    }\\n\\n    /**\\n     * init datasource proxy\\n     } /** * init datasource proxy\\n     * @Param: druidDataSource datasource bean instance\\n     * @Return: DataSourceProxy datasource proxy\\n     */\\n    @Bean\\n    public DataSourceProxy dataSourceProxy() {\\n        logger.info(\\"Proxy dataSource ........\\") ;\\n        return dataSourceProxy;\\n    }\\n\\n    /**\\n     * init global transaction scanner\\n     * @Return: GlobalTransactionScanner\\n     * @Return: GlobalTransactionScanner\\n     */\\n    @Bean\\n    public GlobalTransactionScanner globalTransactionScanner() {\\n        logger.info(\\"Configuring seata........\\") ;\\n        return new GlobalTransactionScanner(\\"test-service\\", \\"test-group\\");\\n    }\\n}\\n\\n```\\n\\nLook at the code, we removed their own configuration of the sqlSessionFactory, directly let the DataSource bean return is a proxied bean, and we added @Primary, resulting in mp priority to use our configuration of the data source, which solves the problem of mp because of seata proxy data source with the creation of a new sqlSessionFactory, resulting in mp\'s plug-ins and components fail the bug!\\n\\n# Summary\\n\\nstepping into the pit is not terrible, the main and patience along the principle of each component implementation, and then go to think, look for the corresponding conflict of the code block, you will be able to find a compatible method of the two."},{"id":"/springboot-dubbo-mybatisplus-seata","metadata":{"permalink":"/blog/springboot-dubbo-mybatisplus-seata","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/springboot-dubbo-mybatisplus-seata.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/springboot-dubbo-mybatisplus-seata.md","title":"Integrating Seata Distributed Transaction with SpringBoot+Dubbo+MybatisPlus","description":"This article explains how to build the integration of Seata with SpringBoot+Dubbo+MybatisPlus using the direct connection approach.","date":"2019-11-29T00:00:00.000Z","formattedDate":"November 29, 2019","tags":[],"readingTime":20.835,"hasTruncateMarker":false,"authors":[{"name":"FUNKYE"}],"frontMatter":{"title":"Integrating Seata Distributed Transaction with SpringBoot+Dubbo+MybatisPlus","keywords":["Seata","dubbo","mybatis","distributed transaction"],"description":"This article explains how to build the integration of Seata with SpringBoot+Dubbo+MybatisPlus using the direct connection approach.","author":"FUNKYE","date":"2019/11/29"},"unlisted":false,"prevItem":{"title":"Resolving the Issue of Losing Mybatis-Plus Features in Seata AT Mode Integration through Source Code","permalink":"/blog/seata-mybatisplus-analysis"},"nextItem":{"title":"Does Seata Client Need to Start RM and TM Simultaneously?","permalink":"/blog/seata-at-mode-start-rm-tm"}},"content":"[Project address](https://gitee.com/itCjb/springboot-dubbo-mybatisplus-seata )\\n\\nThis article was written by FUNKYE (Chen Jianbin), Hangzhou, an Internet company main program.\\n\\n# Preface\\n\\n**Transaction**: Transaction is a reliable independent unit of work composed of a set of operations, the transaction has the characteristics of ACID, namely atomicity, consistency, isolation and persistence.\\n**Distributed Transaction**: When an operation involves multiple services, multiple databases to collaborate on the completion (such as sub-tables and libraries, business split), multiple services, the local Transaction has been unable to cope with this situation , in order to ensure data consistency, you need to use distributed transactions.\\n**Seata** : is an open source distributed transaction solution , is committed to providing high performance and ease of use in the microservices architecture of distributed transaction services .\\n**Purpose of this article** : Nowadays, microservices are becoming more and more popular , and the market can be described as a number of distributed transaction solutions , uneven , more popular to MQ on behalf of the guarantee is the ultimate consistency of the message solution ( consumption confirmation , message lookback , message compensation mechanism , etc.) , and TX-LCN LCN mode to coordinate local transactions to ensure that the transaction unified commit or rollback (has stopped updating , incompatible with Dubbo2.7). MQ\'s distributed transactions are too complex, TX-LCN break more, this time the need for an efficient and reliable and easy to get started with the distributed transaction solution, Seata stands out, this article is to introduce how to quickly build a Demo project to integrate Seata, together!\\n\\n# Preparation\\n\\n1. First of all, install mysql, eclipse and other commonly used tools, which does not expand.\\n\\n2. visit the seata download centre [address](/download/seata-server) we use version 0.9.0\\n\\n3. Download and unzip seata-server.\\n\\n## Build the library and table\\n\\n1.first we link mysql to create a database named seata, and then run the table building sql, this in the seata-server conf folder db_store.sql is what I need to use the sql.\\n\\n ```mysql\\n /*\\n Navicat MySQL Data Transfer\\n Source Server : mysql\\n Source Server Version : 50721\\n Source Host : localhost:3306\\n Source Database : seata\\n Target Server Type : MYSQL\\n Target Server Version : 50721\\n File Encoding : 65001\\n Date: 2019-11-23 22:03:18\\n */\\n\\n SET FOREIGN_KEY_CHECKS=0;\\n\\n -- ----------------------------\\n\\n -- Table structure for branch_table\\n\\n -- ----------------------------\\n\\n DROP TABLE IF EXISTS `branch_table`;\\n CREATE TABLE `branch_table` (\\n  `branch_id` bigint(20) NOT NULL, `xid` varchar\\n  `xid` varchar(128) NOT NULL, `transaction_id` bigint(20)\\n  `transaction_id` bigint(20) DEFAULT NULL, `resource_group_id\\n  `resource_group_id` varchar(32) DEFAULT NULL, `resource_id` varchar(32)\\n  `resource_id` varchar(256) DEFAULT NULL, `lock_key` varchar(256)\\n  `lock_key` varchar(128) DEFAULT NULL,\\n  `branch_type` varchar(8) DEFAULT NULL, `status` tinyint(8)\\n  `status` tinyint(4) DEFAULT NULL,\\n  `client_id` varchar(64) DEFAULT NULL, `application_data` tinyint(4)\\n  `application_data` varchar(2000) DEFAULT NULL, `gmt_create` tinyint(4) DEFAULT NULL, `gmt_create\\n  `gmt_create` datetime DEFAULT NULL,\\n  `gmt_modified` datetime DEFAULT NULL, `gmt_modified` datetime\\n  PRIMARY KEY (`branch_id`),\\n  KEY `idx_xid` (`xid`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\\n\\n -- ----------------------------\\n\\n -- Records of branch_table\\n\\n -- ----------------------------\\n\\n -- ----------------------------\\n\\n -- Table structure for global_table\\n\\n -- ----------------------------\\n\\n DROP TABLE IF EXISTS `global_table`;\\n CREATE TABLE `global_table` (\\n  `xid` varchar(128) NOT NULL, `transaction_id` varchar(128)\\n  `transaction_id` bigint(20) DEFAULT NULL, `status` tinyint(20)\\n  `status` tinyint(4) NOT NULL, `application_id` varchar(4)\\n  `application_id` varchar(32) DEFAULT NULL, `transaction_service` bigint(20)\\n  `transaction_service_group` varchar(32) DEFAULT NULL,\\n  `transaction_name` varchar(128) DEFAULT NULL, `timeout` int(11.0)\\n  `timeout` int(11) DEFAULT NULL, `begin_time` big\\n  `begin_time` bigint(20) DEFAULT NULL, `application_data` int(11)\\n  `application_data` varchar(2000) DEFAULT NULL, `gmt_create` bigint(20)\\n  `gmt_create` datetime DEFAULT NULL, `gmt_modify` datetime\\n  `gmt_modified` datetime DEFAULT NULL, `gmt_modified` datetime\\n  PRIMARY KEY (`xid`),\\n  KEY `idx_gmt_modified_status` (`gmt_modified`, `status`), KEY `idx_tmt_status\\n  KEY `idx_transaction_id` (`transaction_id`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\\n\\n -- ----------------------------\\n\\n -- Records of global_table\\n\\n -- ----------------------------\\n\\n -- ----------------------------\\n\\n -- Table structure for lock_table\\n\\n -- ----------------------------\\n\\n DROP TABLE IF EXISTS `lock_table`;\\n CREATE TABLE `lock_table` (\\n  `row_key` varchar(128) NOT NULL, `xid` varchar(128)\\n  `xid` varchar(96) DEFAULT NULL,\\n  `transaction_id` mediumtext, `branch_id` mediumtext, `transaction_id` mediumtext\\n  `branch_id` mediumtext,\\n  `resource_id` varchar(256) DEFAULT NULL, `table_name` varchar(256)\\n  `table_name` varchar(32) DEFAULT NULL,\\n  `pk` varchar(36) DEFAULT NULL, `gmt_create\\n  `gmt_create` datetime DEFAULT NULL,\\n  `gmt_modified` datetime DEFAULT NULL, `gmt_modified` datetime\\n  PRIMARY KEY (`row_key`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\\n\\n -- ----------------------------\\n\\n -- Records of lock_table\\n\\n -- ----------------------------\\n\\n -- ----------------------------\\n\\n -- Table structure for undo_log\\n\\n -- ----------------------------\\n\\n DROP TABLE IF EXISTS `undo_log`;\\n CREATE TABLE `undo_log` (\\n  `id` bigint(20) NOT NULL AUTO_INCREMENT, `branch_id` bigint(20)\\n  `branch_id` bigint(20) NOT NULL, `xid` varchar\\n  `xid` varchar(100) NOT NULL,\\n  `context` varchar(128) NOT NULL, `rollback_info` bigint(20)\\n  `rollback_info` longblob NOT NULL, `log_status` int\\n  `log_status` int(11) NOT NULL, `log_created` datasheet\\n  `log_created` datetime NOT NULL, `log_modified` longblob NOT NULL, `log_status` int(11)\\n  `log_modified` datetime NOT NULL,\\n  `ext` varchar(100) DEFAULT NULL,\\n  PRIMARY KEY (`id`),\\n  UNIQUE KEY `ux_undo_log` (`xid`,`branch_id`)\\n ) ENGINE=InnoDB DEFAULT CHARSET=utf8;\\n\\n -- ----------------------------\\n\\n -- Records of undo_log\\n ```\\n\\n2. After running the database needed for the above seata, we build the library we need to write the demo, create a database named test, and then execute the following sql code.\\n\\n ```mysql\\n /*\\n Navicat MySQL Data Transfer\\n Source Server : mysql\\n Source Server Version : 50721\\n Source Host : localhost:3306\\n Source Database : test\\n Target Server Type : MYSQL\\n Target Server Version : 50721\\n File Encoding : 65001\\n Date: 2019-11-23 22:03:24\\n */\\n\\n SET FOREIGN_KEY_CHECKS=0;\\n\\n -- ----------------------------\\n\\n -- Table structure for test\\n\\n -- ----------------------------\\n\\n DROP TABLE IF EXISTS `test`.\\n CREATE TABLE `test` (\\n  `id` int(11) NOT NULL AUTO_INCREMENT, `one` varchar(255) DEFATE TABLE (\\n  `one` varchar(255) DEFAULT NULL,\\n  `two` varchar(255) DEFAULT NULL, `createTime` datetime, `createTime` datetime, `createTime` datetime\\n  `createTime` datetime DEFAULT NULL, `two` varchar(255) DEFAULT NULL, `createTime` datetime\\n  PRIMARY KEY (`id`)\\n ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4;\\n\\n -- ----------------------------\\n\\n -- Records of test\\n\\n -- ----------------------------\\n\\n INSERT INTO `test` VALUES (\'1\', \'1\', \'2\', \'2019-11-23 16:07:34\');\\n\\n -- ----------------------------\\n\\n -- Table structure for undo_log\\n\\n -- ----------------------------\\n\\n DROP TABLE IF EXISTS `undo_log`;.\\n CREATE TABLE `undo_log` (\\n  `id` bigint(20) NOT NULL AUTO_INCREMENT, `branch_id` bigint(20)\\n  `branch_id` bigint(20) NOT NULL, `xid` varchar\\n  `xid` varchar(100) NOT NULL,\\n  `context` varchar(128) NOT NULL, `rollback_info` bigint(20)\\n  `rollback_info` longblob NOT NULL, `log_status` int\\n  `log_status` int(11) NOT NULL, `log_created` datasheet\\n  `log_created` datetime NOT NULL, `log_modified` longblob NOT NULL, `log_status` int(11)\\n  `log_modified` datetime NOT NULL,\\n  `ext` varchar(100) DEFAULT NULL,\\n  PRIMARY KEY (`id`),\\n  UNIQUE KEY `ux_undo_log` (`xid`,`branch_id`)\\n ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;\\n\\n -- ----------------------------\\n\\n -- Records of undo_log\\n ```\\n\\n3. we find the file inside the seata-server/conf folder and edit it:![20191129132933](/img/blog/20191129132933.png)\\n\\n4. again find the db configuration method block, change the method as follows:![](/img/blog/20191129133111.png)\\n\\nWell, you can go to the bin directory./seata-server.bat run to see the\\n\\n# Create a project\\n\\nfirst of all, we use eclipse, of course, you can also use idea and other tools, please run in detail according to the following steps\\n\\n1. create a new maven project, and delete the extra folder:![20191129133354](/img/blog/20191129133354.png)<img src=\\"/img/blog/20191129133441.png\\" alt=\\"20191129133441\\" style={{ zoom:\'150%\' }}  />\\n\\n2. Open the project\'s pom.xml and add the following dependency.\\n\\n```java\\n<properties\\n<webVersion>3.1</webVersion\\n<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding\\n<project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding\\n<maven.compiler.source>1.8</maven.compiler.source\\n<maven.compiler.target>1.8</maven.compiler.target\\n<HikariCP.version>3.2.0</HikariCP.version\\n<mybatis-plus-boot-starter.version>3.2.0</mybatis-plus-boot-starter.version>\\n</properties\\n<parent>\\n<groupId>org.springframework.boot</groupId>\\n<artifactId>spring-boot-starter-parent</artifactId>\\n<version>2.1.8.RELEASE</version>.\\n</parent\\n<dependencies\\n<dependency>\\n<groupId>org.apache.curator</groupId>\\n<artifactId>curator-framework</artifactId\\n<version>4.2.0</version>\\n</dependency\\n<dependency>\\n<groupId>org.apache.curator</groupId>\\n<artifactId>curator-recipes</artifactId>\\n<version>4.2.0</version>.\\n</dependency\\n<dependency>\\n<groupId>org.apache.dubbo</groupId>\\n<artifactId>dubbo-spring-boot-starter</artifactId>\\n<version>2.7.4.1</version>\\n</dependency\\n<dependency>\\n<groupId>org.apache.commons</groupId>\\n<artifactId>commons-lang3</artifactId>\\n</dependency\\n<dependency>\\n<groupId>com.alibaba</groupId\\n<artifactId>fastjson</artifactId>\\n<version>1.2.60</version>\\n</dependency\\n<! -- <dependency> <groupId>javax</groupId> <artifactId>javaee-api</artifactId>\\n<version>7.0</version> <scope>provided</scope> </dependency> --\x3e\\n<dependency>\\n<groupId>io.springfox</groupId>\\n<artifactId>springfox-swagger2</artifactId>\\n<version>2.9.2</version>.\\n</dependency\\n<dependency>\\n<groupId>io.springfox</groupId>\\n<artifactId>springfox-swagger-ui</artifactId>\\n<version>2.9.2</version>.\\n</dependency\\n\\n       <! -- mybatis-plus begin --\x3e\\n       <dependency>\\n          <groupId>com.baomidou</groupId>\\n          <artifactId>mybatis-plus-boot-starter</artifactId>\\n          <version>${mybatis-plus-boot-starter.version}</version>\\n       </dependency\\n       <! -- mybatis-plus end --\x3e\\n       <! -- https://mvnrepository.com/artifact/org.projectlombok/lombok --\x3e\\n       <dependency>\\n          <groupId>org.projectlombok</groupId>\\n          <artifactId>lombok</artifactId>\\n          <scope>provided</scope>\\n       </dependency\\n       <dependency>\\n          <groupId>io.seata</groupId>\\n          <artifactId>seata-all</artifactId>\\n          <version>0.9.0.1</version>\\n       </dependency\\n       <! -- Zookeeper --\x3e\\n       <dependency>\\n          <groupId>org.apache.zookeeper</groupId>\\n          <artifactId>zookeeper</artifactId>\\n          <version>3.4.9</version>\\n          <exclusions\\n             <exclusion\\n                <groupId>org.slf4j</groupId>\\n                <artifactId>slf4j-log4j12</artifactId>\\n             </exclusion\\n          </exclusions\\n       </dependency\\n       <! -- <dependency> <groupId>com.baomidou</groupId> <artifactId>dynamic-datasource-spring-boot-starter</ artifactId>\\n          <version>2.5.4</version> </dependency> --\x3e\\n\\n       <! -- <dependency> <groupId>com.baomidou</groupId> <artifactId>mybatis-plus-generator</artifactId>\\n          <version>3.1.0</version> </dependency> --\x3e\\n       <! -- https://mvnrepository.com/artifact/org.freemarker/freemarker --\x3e\\n       <dependency>\\n          <groupId>org.freemarker</groupId>\\n          <artifactId>freemarker</artifactId>\\n       </dependency\\n       <! -- https://mvnrepository.com/artifact/com.alibaba/druid-spring-boot-starter --\x3e\\n       <dependency>\\n          <groupId>com.alibaba</groupId>\\n          <artifactId>druid-spring-boot-starter</artifactId>\\n          <version>1.1.20</version>\\n       </dependency\\n       <! -- Add this to recognise the log4j2.yml file --\x3e\\n       <dependency>\\n          <groupId>com.fasterxml.jackson.dataformat</groupId>\\n          <artifactId>jackson-dataformat-yaml</artifactId>\\n       </dependency\\n       <dependency> <! -- Introducing the log4j2 dependency --\x3e\\n          <groupId>org.springframework.boot</groupId>\\n          <artifactId>spring-boot-starter-log4j2</artifactId>\\n       </dependency\\n       <! -- https://mvnrepository.com/artifact/mysql/mysql-connector-java --\x3e\\n       <dependency>\\n          <groupId>mysql</groupId>\\n          <artifactId>mysql-connector-java</artifactId>\\n       </dependency\\n       <dependency>\\n          <groupId>org.springframework.boot</groupId>\\n          <artifactId>spring-boot-starter-web</artifactId\\n          <exclusions\\n             <exclusion>\\n                <groupId>org.springframework.boot</groupId\\n                <artifactId>spring-boot-starter-logging</artifactId>\\n             </exclusion\\n             <exclusion\\n                <groupId>org.slf4j</groupId\\n                <artifactId>slf4j-log4j12</artifactId>\\n             </exclusion\\n          </exclusions\\n       </dependency\\n       <dependency>\\n          <groupId>org.springframework.boot</groupId>\\n          <artifactId>spring-boot-starter-aop</artifactId>\\n       </dependency\\n       <dependency>\\n          <groupId>org.springframework.boot</groupId>\\n          <artifactId>spring-boot-starter-test</artifactId\\n          <scope>test</scope\\n       </dependency\\n       <! -- <dependency> <groupId>org.scala-lang</groupId> <artifactId>scala-library</artifactId>\\n          <version>2.11.0</version> </dependency> --\x3e\\n       <dependency>\\n          <groupId>org.springframework.boot</groupId>\\n          <artifactId>spring-boot-configuration-processor</artifactId>\\n          <optional>true</optional\\n       </dependency\\n    </dependencies\\n\\n<optional>true</optional> </dependencies>\\n```\\n\\n3. and then switch the parent project for pom mode, or pom file, switch to overview , do as shown in the operation:![20191129134127](/img/blog/20191129134127.png)\\n\\n4. create our demo sub-project, test-service:![20191129135935](/img/blog/20191129135935.png)\\n\\nThe directory is as follows.\\n\\n<img src=\\"/img/blog/20191129140048.png\\" alt=\\"20191129140048\\" style={{ zoom:\'200%\' }} />\\n\\n    Create EmbeddedZooKeeper.java file, along with ProviderApplication.java, with the following code.\\n\\n```java\\npackage org.test;\\n\\nimport java.io.File;\\nimport java.lang.reflect.Method;\\nimport java.util.Properties;\\nimport java.util.UUID;\\n\\nimport org.apache.zookeeper.server.ServerConfig;\\nimport org.apache.zookeeper.server.ZooKeeperServerMain;\\nimport org.apache.zookeeper.server.quorum.QuorumPeerConfig;\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport org.springframework.context.SmartLifecycle;\\nimport org.springframework.util.ErrorHandler;\\nimport org.springframework.util.SocketUtils;\\n\\n/**\\n * from:\\n * https://github.com/spring-projects/spring-xd/blob/v1.3.1.RELEASE/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/zookeeper/ZooKeeperUtils.java\\n *\\n * Helper class to start an embedded instance of standalone (non clustered) ZooKeeper.\\n *\\n * NOTE: at least an external standalone server (if not an ensemble) are recommended, even for\\n * {@link org.springframework.xd.dirt.server.singlenode.SingleNodeApplication}\\n *\\n * @author Patrick Peralta\\n * @author Mark Fisher\\n * @author David Turanski\\n */\\npublic class EmbeddedZooKeeper implements SmartLifecycle {\\n\\n  /**\\n   * Logger.\\n   */\\n  private static final Logger logger = LoggerFactory.getLogger(EmbeddedZooKeeper.class);\\n\\n  /**\\n   * ZooKeeper client port. This will be determined dynamically upon startup.\\n   */\\n  private final int clientPort;\\n\\n  /**\\n   * Whether to auto-start. Default is true.\\n   */\\n  private boolean autoStartup = true;\\n\\n  /**\\n   * Lifecycle phase. Default is 0.\\n   */\\n  private int phase = 0;\\n\\n  /**\\n   * Thread for running the ZooKeeper server.\\n   */\\n  private volatile Thread zkServerThread;\\n\\n  /**\\n   * ZooKeeper server.\\n   */\\n  private volatile ZooKeeperServerMain zkServer;\\n\\n  /**\\n   * {@link ErrorHandler} to be invoked if an Exception is thrown from the ZooKeeper server thread.\\n   */\\n  private ErrorHandler errorHandler;\\n\\n  private boolean daemon = true;\\n\\n  /**\\n   * Construct an EmbeddedZooKeeper with a random port.\\n   */\\n  public EmbeddedZooKeeper() {\\n    clientPort = SocketUtils.findAvailableTcpPort();\\n  }\\n\\n  /**\\n   * Construct an EmbeddedZooKeeper with the provided port.\\n   *\\n   * @param clientPort\\n   *            port for ZooKeeper server to bind to\\n   */\\n  public EmbeddedZooKeeper(int clientPort, boolean daemon) {\\n    this.clientPort = clientPort;\\n    this.daemon = daemon;\\n  }\\n\\n  /**\\n   * Returns the port that clients should use to connect to this embedded server.\\n   *\\n   * @return dynamically determined client port\\n   */\\n  public int getClientPort() {\\n    return this.clientPort;\\n  }\\n\\n  /**\\n   * Specify whether to start automatically. Default is true.\\n   *\\n   * @param autoStartup\\n   *            whether to start automatically\\n   */\\n  public void setAutoStartup(boolean autoStartup) {\\n    this.autoStartup = autoStartup;\\n  }\\n\\n  /**\\n   * {@inheritDoc}\\n   */\\n  public boolean isAutoStartup() {\\n    return this.autoStartup;\\n  }\\n\\n  /**\\n   * Specify the lifecycle phase for the embedded server.\\n   *\\n   * @param phase\\n   *            the lifecycle phase\\n   */\\n  public void setPhase(int phase) {\\n    this.phase = phase;\\n  }\\n\\n  /**\\n   * {@inheritDoc}\\n   */\\n  public int getPhase() {\\n    return this.phase;\\n  }\\n\\n  /**\\n   * {@inheritDoc}\\n   */\\n  public boolean isRunning() {\\n    return (zkServerThread != null);\\n  }\\n\\n  /**\\n   * Start the ZooKeeper server in a background thread.\\n   * <p>\\n   * Register an error handler via {@link #setErrorHandler} in order to handle any exceptions thrown during startup or\\n   * execution.\\n   */\\n  public synchronized void start() {\\n    if (zkServerThread == null) {\\n      zkServerThread = new Thread(new ServerRunnable(), \\"ZooKeeper Server Starter\\");\\n      zkServerThread.setDaemon(daemon);\\n      zkServerThread.start();\\n    }\\n  }\\n\\n  /**\\n   * Shutdown the ZooKeeper server.\\n   */\\n  public synchronized void stop() {\\n    if (zkServerThread != null) {\\n      // The shutdown method is protected...thus this hack to invoke it.\\n      // This will log an exception on shutdown; see\\n      // https://issues.apache.org/jira/browse/ZOOKEEPER-1873 for details.\\n      try {\\n        Method shutdown = ZooKeeperServerMain.class.getDeclaredMethod(\\"shutdown\\");\\n        shutdown.setAccessible(true);\\n        shutdown.invoke(zkServer);\\n      }\\n\\n      catch (Exception e) {\\n        throw new RuntimeException(e);\\n      }\\n\\n      // It is expected that the thread will exit after\\n      // the server is shutdown; this will block until\\n      // the shutdown is complete.\\n      try {\\n        zkServerThread.join(5000);\\n        zkServerThread = null;\\n      } catch (InterruptedException e) {\\n        Thread.currentThread().interrupt();\\n        logger.warn(\\"Interrupted while waiting for embedded ZooKeeper to exit\\");\\n        // abandoning zk thread\\n        zkServerThread = null;\\n      }\\n    }\\n  }\\n\\n  /**\\n   * Stop the server if running and invoke the callback when complete.\\n   */\\n  public void stop(Runnable callback) {\\n    stop();\\n    callback.run();\\n  }\\n\\n  /**\\n   * Provide an {@link ErrorHandler} to be invoked if an Exception is thrown from the ZooKeeper server thread. If none\\n   * is provided, only error-level logging will occur.\\n   *\\n   * @param errorHandler\\n   *            the {@link ErrorHandler} to be invoked\\n   */\\n  public void setErrorHandler(ErrorHandler errorHandler) {\\n    this.errorHandler = errorHandler;\\n  }\\n\\n  /**\\n   * Runnable implementation that starts the ZooKeeper server.\\n   */\\n  private class ServerRunnable implements Runnable {\\n\\n    public void run() {\\n      try {\\n        Properties properties = new Properties();\\n        File file = new File(System.getProperty(\\"java.io.tmpdir\\") + File.separator + UUID.randomUUID());\\n        file.deleteOnExit();\\n        properties.setProperty(\\"dataDir\\", file.getAbsolutePath());\\n        properties.setProperty(\\"clientPort\\", String.valueOf(clientPort));\\n\\n        QuorumPeerConfig quorumPeerConfig = new QuorumPeerConfig();\\n        quorumPeerConfig.parseProperties(properties);\\n\\n        zkServer = new ZooKeeperServerMain();\\n        ServerConfig configuration = new ServerConfig();\\n        configuration.readFrom(quorumPeerConfig);\\n\\n        zkServer.runFromConfig(configuration);\\n      } catch (Exception e) {\\n        if (errorHandler != null) {\\n          errorHandler.handleError(e);\\n        } else {\\n          logger.error(\\"Exception running embedded ZooKeeper\\", e);\\n        }\\n      }\\n    }\\n  }\\n\\n}\\n ```\\n\\n ```java\\n package org.test;\\n\\n import org.apache.dubbo.config.spring.context.annotation.DubboComponentScan; import org.apache.dubbo.config.spring.context.annotation.\\n import org.springframework.boot.SpringApplication; import org.springframework.boot.\\n import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.boot.autoconfigure.\\n import org.springframework.context.annotation.ComponentScan; import org.springframework.context.annotation.\\n import org.springframework.transaction.annotation.EnableTransactionManagement; import org.springframework.transaction.annotation.\\n\\n /**\\n * @author cjbc.annotation.EnableTransactionManagement; /**\\n * @author cjb\\n * @date 2019/10/24\\n */\\n @EnableTransactionManagement.\\n @ComponentScan(basePackages = {\\"org.test.config\\", \\"org.test.service.impl\\"})\\n @DubboComponentScan(basePackages = \\"org.test.service.impl\\")\\n @SpringBootApplication\\n public class ProviderApplication {\\n\\n    public static void main(String[] args) {\\n        new EmbeddedZooKeeper(2181, false).start();\\n        SpringApplication app = new SpringApplication(ProviderApplication.class);\\n        app.run(args);\\n    }\\n\\n }\\n\\n ```\\n\\n    create entity package org.test.entity and the creation of entity class Test used to lombok, details of Baidu, eclipse installed lombok plug-in\\n\\n```java\\npackage org.test.entity;\\n\\nimport java.io.Serializable;\\nimport java.time.LocalDateTime;\\n\\nimport com.baomidou.mybatisplus.annotation.IdType;\\nimport com.baomidou.mybatisplus.annotation.TableField;\\nimport com.baomidou.mybatisplus.annotation.TableId;\\n\\nimport io.swagger.annotations.ApiModel;\\nimport io.swagger.annotations.ApiModelProperty;\\nimport lombok.Data;\\nimport lombok.EqualsAndHashCode;\\nimport lombok.experimental.Accessors;\\n\\n/**\\n* <p>\\n* Functions\\n* </p\\n*\\n* @author Funkye\\n* @since 2019-04-23\\n  */\\n@Data\\n@EqualsAndHashCode(callSuper = false)\\n@Accessors(chain = true)\\n@ApiModel(value = \\"test\u5bf9\u8c61\\", description = \\"\u529f\u80fd\\")\\npublic class Test implements Serializable {\\n\\n  private static final long serialVersionUID = 1L;\\n\\n  @ApiModelProperty(value = \\"\u4e3b\u952e\\")\\n  @TableId(value = \\"id\\", type = IdType.AUTO)\\n  private Integer id;\\n\\n  @ApiModelProperty(value = \\"one\\")\\n  @TableField(\\"one\\")\\n  private String one;\\n\\n  @ApiModelProperty(value = \\"two\\")\\n  @TableField(\\"two\\")\\n  private String two;\\n\\n  @ApiModelProperty(value = \\"createTime\\")\\n  @TableField(\\"createTime\\")\\n  private LocalDateTime createTime;\\n\\n}\\n\\n```\\n\\nCreate service, service.impl, mapper and other packages, in turn create ITestservice, and the implementation class, mapper.\\n\\n```java\\npackage org.test.service;\\n\\nimport org.test.entity.Test;\\n\\nimport com.baomidou.mybatisplus.extension.service.IService;\\n\\n/**\\n* <p>\\n* Function Service class\\n* </p\\n*\\n* @author Funkye\\n* @since 2019-04-10\\n  */\\n  public interface ITestService extends IService<Test> {\\n\\n}\\n\\n ```\\n\\n ```java\\nimport org.apache.dubbo.config.annotation.Service;\\nimport org.test.entity.Test;\\nimport org.test.mapper.TestMapper;\\nimport org.test.service.ITestService;\\n\\nimport com.baomidou.mybatisplus.extension.service.impl.ServiceImpl;\\n\\n@Service(version = \\"1.0.0\\",interfaceClass =ITestService.class )\\npublic class TestServiceImpl extends ServiceImpl<TestMapper, Test> implements ITestService {\\n\\n}\\n\\n```\\n\\n```java\\n package org.test.mapper;\\n\\n import org.test.entity.Test;\\n\\n import com.baomidou.mybatisplus.core.mapper.BaseMapper;\\n\\n /**\\n * <p>\\n * Functional Mapper interface\\n * </p>\\n *\\n * @author Funkye\\n * @since 2019-04-10\\n */\\n public interface TestMapper extends BaseMapper<Test> {\\n\\n }\\n\\n ```\\n\\n    Create org.test.config package, create SeataAutoConfig.java, configuration information are here, the main role for the proxy data, connect to the transaction service grouping\\n\\n```java\\npackage org.test.config;\\n\\nimport javax.sql.DataSource;\\n\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport org.springframework.beans.factory.annotation.Autowired;\\nimport org.springframework.beans.factory.annotation.Qualifier;\\nimport org.springframework.boot.autoconfigure.jdbc.DataSourceProperties;\\nimport org.springframework.context.annotation.Bean;\\nimport org.springframework.context.annotation.Configuration;\\nimport org.springframework.context.annotation.Primary;\\n\\nimport com.alibaba.druid.pool.DruidDataSource;\\n\\nimport io.seata.rm.datasource.DataSourceProxy;\\nimport io.seata.spring.annotation.GlobalTransactionScanner;\\n\\n@Configuration\\npublic class SeataAutoConfig {\\n  @Autowired(required = true)\\n  private DataSourceProperties dataSourceProperties;\\n  private final static Logger logger = LoggerFactory.getLogger(SeataAutoConfig.class);\\n\\n  @Bean(name = \\"druidDataSource\\")\\n  public DataSource druidDataSource() {\\n    DruidDataSource druidDataSource = new DruidDataSource();\\n    logger.info(\\"dataSourceProperties.getUrl():{}\\", dataSourceProperties.getUrl());\\n    druidDataSource.setUrl(dataSourceProperties.getUrl());\\n    druidDataSource.setUsername(dataSourceProperties.getUsername());\\n    druidDataSource.setPassword(dataSourceProperties.getPassword());\\n    druidDataSource.setDriverClassName(dataSourceProperties.getDriverClassName());\\n    druidDataSource.setInitialSize(0);\\n    druidDataSource.setMaxActive(180);\\n    druidDataSource.setMaxWait(60000);\\n    druidDataSource.setMinIdle(0);\\n    druidDataSource.setValidationQuery(\\"Select 1 from DUAL\\");\\n    druidDataSource.setTestOnBorrow(false);\\n    druidDataSource.setTestOnReturn(false);\\n    druidDataSource.setTestWhileIdle(true);\\n    druidDataSource.setTimeBetweenEvictionRunsMillis(60000);\\n    druidDataSource.setMinEvictableIdleTimeMillis(25200000);\\n    druidDataSource.setRemoveAbandoned(true);\\n    druidDataSource.setRemoveAbandonedTimeout(1800);\\n    druidDataSource.setLogAbandoned(true);\\n    logger.info(\\"load dataSource........\\");\\n    return druidDataSource;\\n  }\\n\\n    /**\\n     * init datasource proxy\\n     * @Param: druidDataSource datasource bean instance\\n     * @Param: druidDataSource datasource bean instance\\n     * @Return: DataSourceProxy datasource proxy\\n     */\\n    @Bean(name = \\"dataSource\\")\\n    @Primary // In the same DataSource, first use the labelled DataSource\\n    public DataSourceProxy dataSourceProxy(@Qualifier(value = \\"druidDataSource\\") DruidDataSource druidDataSource) {\\n       logger.info(\\"Proxy dataSource ........\\") ;\\n       return new DataSourceProxy(druidDataSource);\\n    }\\n\\n    /**\\n     * init global transaction scanner\\n     * @Return: GlobalTransactionScanner\\n     * @Return: GlobalTransactionScanner\\n     */\\n    @Bean\\n    public GlobalTransactionScanner globalTransactionScanner() {\\n       logger.info(\\"Configuring seata........\\") ;\\n       return new GlobalTransactionScanner(\\"test-service\\", \\"test-group\\");\\n    }\\n}\\n ```\\n\\n    Then create the configuration file MybatisPlusConfig, which is required for mybatisplus.\\n\\n ```java\\npackage org.test.config;\\n\\nimport java.util.ArrayList;\\nimport java.util.List;\\n\\nimport org.mybatis.spring.mapper.MapperScannerConfigurer;\\nimport org.springframework.context.annotation.Bean;\\nimport org.springframework.context.annotation.Configuration;\\n\\nimport com.baomidou.mybatisplus.core.parser.ISqlParser;\\nimport com.baomidou.mybatisplus.extension.parsers.BlockAttackSqlParser;\\nimport com.baomidou.mybatisplus.extension.plugins.PaginationInterceptor;\\n\\n @Configuration\\n // @MapperScan(\\"com.baomidou.springboot.mapper*\\") // This annotation is equivalent to @Bean below.\\n // MapperScannerConfigurer, 2 configurations of a copy can be\\n public class MybatisPlusConfig {\\n\\n    /**\\n     * mybatis-plus paging plugin <br\\n     * Documentation: http://mp.baomidou.com<br>\\n     */\\n    @Bean\\n    public PaginationInterceptor paginationInterceptor() {\\n        PaginationInterceptor paginationInterceptor = new PaginationInterceptor();\\n        List<ISqlParser> sqlParserList = new ArrayList<ISqlParser>();\\n        // Attack the SQL blocking parser and join the parse chain.\\n        sqlParserList.add(new BlockAttackSqlParser());\\n        paginationInterceptor.setSqlParserList(sqlParserList);\\n        return paginationInterceptor;\\n    }\\n\\n    /**\\n     * Equivalent to the top: {@code @MapperScan(\\"com.baomidou.springboot.mapper*\\")} Here it can be extended, e.g., using a configuration file to configure the path to scan the Mapper\\n     */\\n\\n    @Bean\\n    public MapperScannerConfigurer mapperScannerConfigurer() {\\n        MapperScannerConfigurer scannerConfigurer = new MapperScannerConfigurer();\\n        scannerConfigurer.setBasePackage(\\"org.test.mapper\\");\\n        return scannerConfigurer;\\n    }\\n\\n }\\n\\n ```\\n\\nCreate the **resources directory, create the mapper folder, application.yml and other files**.\\n\\n ```yaml\\n server:\\n   port: 38888\\n spring:\\n   application:\\n     name: test-service\\n   datasource:\\n     type: com.alibaba.druid.pool.DruidDataSource\\n     url: jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&characterEncoding=UTF-8&serverTimezone=UTC\\n     driver-class-name: com.mysql.cj.jdbc.Driver\\n     username: root\\n     password: 123456\\n dubbo:\\n   protocol:\\n     loadbalance: leastactive\\n     threadpool: cached\\n   scan:\\n     base-packages: org\u3002test.service\\n   application:\\n     qos-enable: false\\n     name: testserver\\n   registry:\\n     id: my-registry\\n     address:  zookeeper://127.0.0.1:2181?client=curator\\n mybatis-plus:\\n   mapper-locations: classpath:/mapper/*Mapper.xml\\n   typeAliasesPackage: org.test.entity\\n   global-config:\\n     db-config:\\n       field-strategy: not-empty\\n       id-type: auto\\n       db-type: mysql\\n   configuration:\\n     map-underscore-to-camel-case: true\\n     cache-enabled: true\\n     auto-mapping-unknown-column-behavior: none\\n\\n ```\\n\\n create file.conf, here the service within the vgroup_mapping. your transaction grouping, for example, on the ** face SeataAutoConfig configured within the test-group, then here should also be changed to test-group **, and then the following ip port are seata running ip and port on the line!\\n\\n   ```java\\n   transport {\\n     type = \\"TCP\\"\\n     server = \\"NIO\\"\\n     heartbeat = true\\n     thread-factory {\\n     boss-thread-prefix = \\"NettyBoss\\"\\n     worker-thread-prefix = \\"NettyServerNIOWorker\\"\\n     server-executor-thread-prefix = \\"NettyServerBizHandler\\"\\n     share-boss-worker = false\\n     client-selector-thread-prefix = \\"NettyClientSelector\\"\\n     client-selector-thread-size = 1\\n     client-worker-thread-prefix = \\"NettyClientWorkerThread\\"\\n     boss-thread-size = 1\\n     worker-thread-size = 8\\n }\\n     shutdown {\\n     wait = 3\\n }\\n     serialization = \\"seata\\"\\n     compressor = \\"none\\"\\n }\\n     service {\\n     vgroup_mapping.test-group = \\"default\\"\\n     default.grouplist = \\"127.0.0.1:8091\\"\\n     enableDegrade = false\\n     disable = false\\n     max.commit.retry.timeout = \\"-1\\"\\n     max.rollback.retry.timeout = \\"-1\\"\\n }\\n\\n     client {\\n     async.commit.buffer.limit = 10000\\n     lock {\\n     retry.internal = 10\\n     retry.times = 30\\n }\\n     report.retry.count = 5\\n     tm.commit.retry.count = 1\\n     tm.rollback.retry.count = 1\\n     undo.log.table = \\"undo_log\\"\\n }\\n\\n     recovery {\\n     committing-retry-period = 1000\\n     asyn-committing-retry-period = 1000\\n     rollbacking-retry-period = 1000\\n     timeout-retry-period = 1000\\n }\\n\\n     transaction {\\n     undo.data.validation = true\\n     undo.log.serialization = \\"jackson\\"\\n     undo.log.save.days = 7\\n     undo.log.delete.period = 86400000\\n     undo.log.table = \\"undo_log\\"\\n }\\n\\n     metrics {\\n     enabled = false\\n     registry-type = \\"compact\\"\\n     exporter-list = \\"prometheus\\"\\n     exporter-prometheus-port = 9898\\n }\\n\\n     support {\\n     spring {\\n     datasource.autoproxy = false\\n }\\n }\\n\\n```\\n\\nCreate registry.conf to specify ip ports for file, zk and so on.\\n\\n```java\\nregistry {\\n  type = \\"file\\"\\n  file {\\n    name = \\"file.conf\\"\\n  }\\n}\\nconfig {\\n  type = \\"file\\"\\n  file {\\n    name = \\"file.conf\\"\\n  }\\n  zk {\\n    serverAddr = \\"127.0.0.1:2181\\"\\n    session.timeout = 6000\\n    connect.timeout = 2000\\n  }\\n}\\n\\n```\\n\\nGreat success, you can directly run it, this time to observe the seata-server![20191129142115](/img/blog/20191129142115.png)\\n\\nNext, we create test-client project, here will not repeat, with the above test-service the same way to create\\n\\nNext, we copy the test-service service and entities within the past, of course, you are too much trouble, you can get a separate sub-project to put a general service and entities, some tools and so on, I\'m here in order to quickly build this demo, the choice of copy and paste the way.\\n\\nDirectory structure:![](/img/blog/20191129142349.png)\\n\\n    Then we create ClientApplication.\\n\\n```java\\npackage org.test;\\n\\nimport java.util.TimeZone;\\nimport java.util.concurrent.Executor;\\n\\nimport org.apache.dubbo.config.spring.context.annotation.EnableDubbo;\\nimport org.springframework.boot.SpringApplication;\\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\\nimport org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration;\\nimport org.springframework.context.annotation.Bean;\\nimport org.springframework.context.annotation.ComponentScan;\\nimport org.springframework.context.annotation.Configuration;\\nimport org.springframework.scheduling.annotation.EnableAsync;\\nimport org.springframework.scheduling.annotation.EnableScheduling;\\nimport org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;\\n\\nimport com.baomidou.mybatisplus.autoconfigure.MybatisPlusAutoConfiguration;\\n\\n@SpringBootApplication(exclude = {DataSourceAutoConfiguration.class, MybatisPlusAutoConfiguration.class})\\n@EnableScheduling\\n@EnableAsync\\n@Configuration\\n@EnableDubbo(scanBasePackages = {\\"org.test.service\\"})\\n@ComponentScan(basePackages = {\\"org.test.service\\", \\"org.test.controller\\", \\"org.test.config\\"})\\npublic class ClientApplication {\\n    public static void main(String[] args) {\\n        TimeZone.setDefault(TimeZone.getTimeZone(\\"Asia/Shanghai\\"));\\n        SpringApplication app = new SpringApplication(ClientApplication.class);\\n        app.run(args);\\n    }\\n\\n    @Bean(name = \\"threadPoolTaskExecutor\\")\\n    public Executor threadPoolTaskExecutor() {\\n        return new ThreadPoolTaskExecutor();\\n    }\\n}\\n\\n```\\n\\n Then go to the config package and create SwaggerConfig :\\n\\n ```java\\npackage org.test.config;\\n\\nimport java.util.ArrayList;\\nimport java.util.List;\\n\\nimport org.springframework.context.annotation.Bean;\\nimport org.springframework.context.annotation.Configuration;\\n\\nimport springfox.documentation.builders.ApiInfoBuilder;\\nimport springfox.documentation.builders.PathSelectors;\\nimport springfox.documentation.builders.RequestHandlerSelectors;\\nimport springfox.documentation.service.ApiInfo;\\nimport springfox.documentation.service.Contact;\\nimport springfox.documentation.service.Parameter;\\nimport springfox.documentation.spi.DocumentationType;\\nimport springfox.documentation.spring.web.plugins.Docket;\\nimport springfox.documentation.swagger2.annotations.EnableSwagger2;\\n\\n @Configuration\\n public class SwaggerConfig {\\n    // swagger2 configuration file, here you can configure the swagger2 some basic content, such as scanning packages and so on\\n    @Bean\\n    public Docket createRestApi() {\\n        List<Parameter> pars = new ArrayList<Parameter>(); return new Docket(DocumentationText)\\n        return new Docket(DocumentationType.SWAGGER_2).apiInfo(apiInfo()).select()\\n            // Path to the current package\\n            .apis(RequestHandlerSelectors.basePackage(\\"org.test.controller\\")).paths(PathSelectors.any()).build()\\n            .globalOperationParameters(pars);\\n    }\\n\\n    // Build the api document\'s details function, noting which annotation is referenced here\\n    private ApiInfo apiInfo() {\\n        return new ApiInfoBuilder()\\n            // The title of the page\\n            .title(\\"Project Interface\\")\\n            // Creator\\n            .contact(new Contact(\\"FUNKYE\\", \\"\\", \\"\\"))\\n            // Version number\\n            .version(\\"1.0\\")\\n            // Description\\n            .description(\\"API description\\").build();\\n    }\\n }\\n\\n ```\\n\\nand then create SpringMvcConfigure, and then put inside the configuration of seata, I\'m lazy in order to directly integrated in the mvc configuration of the class, you can standardise the point can be created in addition to the configuration of a seata class, you can find the following is still a group name, I have two projects are assigned to a group to go, it seems that another take a also It\'s okay.\\n\\n ```java\\npackage org.test.config;\\n\\nimport java.nio.charset.Charset;\\nimport java.util.ArrayList;\\nimport java.util.List;\\nimport java.util.Map;\\n\\nimport org.apache.dubbo.config.annotation.Reference;\\nimport org.springframework.boot.web.servlet.FilterRegistrationBean;\\nimport org.springframework.context.annotation.Bean;\\nimport org.springframework.context.annotation.Configuration;\\nimport org.springframework.core.Ordered;\\nimport org.springframework.http.MediaType;\\nimport org.springframework.http.converter.HttpMessageConverter;\\nimport org.springframework.http.converter.StringHttpMessageConverter;\\nimport org.springframework.web.cors.CorsConfiguration;\\nimport org.springframework.web.cors.UrlBasedCorsConfigurationSource;\\nimport org.springframework.web.filter.CorsFilter;\\nimport org.springframework.web.servlet.HandlerInterceptor;\\nimport org.springframework.web.servlet.config.annotation.InterceptorRegistry;\\nimport org.springframework.web.servlet.config.annotation.WebMvcConfigurer;\\nimport org.springframework.web.servlet.view.InternalResourceViewResolver;\\n\\nimport com.alibaba.fastjson.serializer.SerializerFeature;\\nimport com.alibaba.fastjson.support.config.FastJsonConfig;\\nimport com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter;\\nimport com.google.common.collect.Maps;\\n\\nimport io.seata.spring.annotation.GlobalTransactionScanner;\\n\\n@Configuration\\n public class SpringMvcConfigure implements WebMvcConfigurer {\\n\\n    @Bean\\n    public FilterRegistrationBean corsFilter() {\\n        UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource();\\n        CorsConfiguration config = new CorsConfiguration();\\n        config.setAllowCredentials(true);\\n        config.addAllowedOrigin(\\"*\\");\\n        config.addAllowedHeader(CorsConfiguration.ALL); config.addAllowedHeader(CorsConfiguration.ALL);\\n        config.addAllowedMethod(CorsConfiguration.ALL); config.addAllowedMethod(CorsConfiguration.ALL);\\n        source.registerCorsConfiguration(\\"/**\\", config);\\n        FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(new CorsFilter(source));\\n        filterRegistrationBean.setOrder(Ordered.HIGHEST_PRECEDENCE);\\n        filterRegistrationBean.setOrder(1);\\n        filterRegistrationBean.setEnabled(true);\\n        filterRegistrationBean.addUrlPatterns(\\"/**\\");\\n        Map<String, String> initParameters = Maps.newHashMap();\\n        initParameters.put(\\"excludes\\", \\"/favicon.ico,/img/*,/js/*,/css/*\\");\\n        initParameters.put(\\"isIncludeRichText\\", \\"true\\");\\n        filterRegistrationBean.setInitParameters(initParameters); return filterRegistrationBean.\\n        return filterRegistrationBean; }\\n    }\\n\\n    @Bean\\n    public InternalResourceViewResolver viewResolver() {\\n        InternalResourceViewResolver viewResolver = new InternalResourceViewResolver(); viewResolver.setPrefix(\\"/WEB-INF\\")\\n        viewResolver.setPrefix(\\"/WEB-INF/jsp/\\");\\n        viewResolver.setSuffix(\\".jsp\\");\\n        // viewResolver.setViewClass(JstlView.class); // This property does not usually need to be configured manually.\\n        // This property does not usually need to be configured manually, as higher versions of Spring will automatically detect it.\\n        return viewResolver; // viewResolver.setViewClass(JstlView.class)\\n    }\\n\\n\\n\\n    /**\\n     * Replacing frame json with fastjson\\n     */\\n    @Override\\n    public void configureMessageConverters(List<HttpMessageConverter<? >> converters) {\\n      FastJsonHttpMessageConverter fastConverter = new FastJsonHttpMessageConverter();\\n      FastJsonConfig fastJsonConfig = new FastJsonConfig();\\n      fastJsonConfig.setSerializerFeatures(SerializerFeature.PrettyFormat, SerializerFeature.WriteMapNullValue,\\n        SerializerFeature.WriteNullStringAsEmpty, SerializerFeature.DisableCircularReferenceDetect);\\n        // Handle garbled Chinese characters\\n        List<MediaType> fastMediaTypes = new ArrayList<>();\\n        fastMediaTypes.add(MediaType.APPLICATION_JSON_UTF8);\\n        fastConverter.setSupportedMediaTypes(fastMediaTypes);\\n        fastConverter.setFastJsonConfig(fastJsonConfig);\\n        // Handle strings, avoiding quotes when returning strings directly.\\n        StringHttpMessageConverter smc = new StringHttpMessageConverter(Charset.forName(\\"UTF-8\\"));\\n        converters.add(smc);\\n        converters.add(fastConverter);\\n    }\\n\\n    @Bean\\n    public GlobalTransactionScanner globalTransactionScanner() {\\n        return new GlobalTransactionScanner(\\"test-client\\", \\"test-group\\"); }\\n    }\\n\\n }\\n\\n ```\\n\\nCreate the c**ontroller package, and then create the TestController** under the package.\\n\\n ```java\\npackage org.test.controller;\\n\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport org.springframework.beans.factory.annotation.Autowired;\\nimport org.springframework.context.annotation.Lazy;\\nimport org.springframework.web.bind.annotation.GetMapping;\\nimport org.springframework.web.bind.annotation.RequestMapping;\\nimport org.springframework.web.bind.annotation.RestController;\\nimport org.test.service.DemoService;\\n\\nimport io.swagger.annotations.Api;\\nimport io.swagger.annotations.ApiOperation;\\n\\n /**\\n * <p>\\n * Documentation table Front-end controller\\n * </p\\n *\\n * @author funkye\\n * @since 2019-03-20\\n */\\n @RestController\\n @RequestMapping(\\"/test\\")\\n @Api(tags = \\"test interface\\")\\n public class TestController {\\n\\n    private final static Logger logger = LoggerFactory.getLogger(TestController.class);\\n    @Autowired\\n    @Lazy\\n    DemoService demoService;\\n\\n    @GetMapping(value = \\"testSeataOne\\")\\n    @ApiOperation(value = \\"Test the manual rollback distributed transaction interface\\")\\n    public Object testSeataOne() {\\n        return demoService.One();\\n    }\\n\\n    @GetMapping(value = \\"testSeataTwo\\")\\n    @ApiOperation(value = \\"Test Exception Rollback Distributed Transaction Interface\\")\\n    public Object testSeataTwo() {\\n        return demoService.Two();\\n    }\\n\\n }\\n\\n ```\\n\\nThen go to service and create the demoService you need to depend on.\\n\\n ```java\\npackage org.test.service;\\n\\nimport java.time.LocalDateTime;\\n\\nimport org.apache.dubbo.config.annotation.Reference;\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport org.springframework.stereotype.Service;\\nimport org.test.controller.TestController;\\nimport org.test.entity.Test;\\n\\nimport io.seata.core.context.RootContext;\\nimport io.seata.core.exception.TransactionException;\\nimport io.seata.spring.annotation.GlobalTransactional;\\nimport io.seata.tm.api.GlobalTransactionContext;\\n\\n@Service\\npublic class DemoService {\\n  @Reference(version = \\"1.0.0\\", timeout = 60000)\\n  private ITestService testService;\\n  private final static Logger logger = LoggerFactory.getLogger(DemoService.class);\\n\\n  /**\\n   * manual rollback example\\n   *\\n   * @return\\n   */\\n  @GlobalTransactional\\n  public Object One() {\\n    logger.info(\\"seata distribute transaction Id:{}\\", RootContext.getXID());\\n    Test t = new Test();\\n    t.setOne(\\"1\\");\\n    t.setTwo(\\"2\\");\\n    t.setCreateTime(LocalDateTime.now());\\n    testService.save(t);\\n    try {\\n      int i = 1 / 0;\\n      return true;\\n    } catch (Exception e) {\\n      // TODO: handle exception\\n      try {\\n        logger.info(\\"load transaction id for rollback\\");\\n        GlobalTransactionContext.reload(RootContext.getXID()).rollback();\\n      } catch (TransactionException e1) {\\n        // TODO Auto-generated catch block\\n        e1.printStackTrace();\\n      }\\n    }\\n    return false;\\n  }\\n\\n  /**\\n   * throw exception and rollback\\n   *\\n   * @return\\n   */\\n  @GlobalTransactional\\n  public Object Two() {\\n    logger.info(\\"seata\u5206\u5e03\u5f0f\u4e8b\u52a1Id:{}\\", RootContext.getXID());\\n    logger.info(\\"seata distribute transaction Id:{}\\", RootContext.getXID());\\n    Test t = new Test();\\n    t.setOne(\\"1\\");\\n    t.setTwo(\\"2\\");\\n    t.setCreateTime(LocalDateTime.now());\\n    testService.save(t);\\n    try {\\n      int i = 1 / 0;\\n      return true;\\n    } catch (Exception e) {\\n      // TODO: handle exception\\n      throw new RuntimeException();\\n    }\\n  }\\n}\\n\\n ```\\n\\nCreate the resources folder as usual, starting with the common **application.yml**.\\n\\n```java\\nspring:\\n  application:\\n     name: test\\n  datasource:\\n     driver-class-name: com.mysql.cj.jdbc.Driver\\n     url: jdbc:mysql://127.0.0.1:3306/test?userSSL=true&useUnicode=true&characterEncoding=UTF8&serverTimezone=Asia/Shanghai\\n     username: root\\n     password: 123456\\n  mvc:\\n    servlet:\\n      load-on-startup: 1\\n  http:\\n    encoding:\\n            force: true\\n            charset: utf-8\\n            enabled: true\\n    multipart:\\n      max-file-size: 10MB\\n      max-request-size: 10MB\\ndubbo:\\n  registry:\\n    id: my-registry\\n    address:  zookeeper://127.0.0.1:2181?client=curator\\n#    address:  zookeeper://127.0.0.1:2181?client=curator\\n  application:\\n    name: dubbo-demo-client\\n    qos-enable: false\\nserver:\\n  port: 28888\\n  max-http-header-size: 8192\\n  address: 0.0.0.0\\n  tomcat:\\n    max-http-post-size: 104857600\\n\\n```\\n\\nCopy the service configuration file and registry file, if your client group name is changed in the configuration class, then the group name in the file file needs to be changed as well.\\n\\n![](/img/blog/20191129142851.png)\\n\\nThe complete directory structure as above, this time you can start test-service, then start test-client, to swagger test it!\\n\\n4. Visit 127.0.0.1:28888/swagger-ui.html to do the final finish ![](/img/blog/20191129143041.png)\\n\\n![20191129143124](/img/blog/20191129143124.png)\\n\\nHere\'s the data I\'ve saved a record, let\'s see if we\'ll successfully rollback:\\n\\n![20191129143252](/img/blog/20191129143252.png)\\n\\nRefresh the database, found that there is still only one data:\\n\\n![20191129143124](/img/blog/20191129143124.png)\\n\\nAnd then check the log.\\n\\n![20191129143407](/img/blog/20191129143407.png)\\n\\nIt shows that it has been rolled back, let\'s look at the log from seata-server again:\\n\\n<img src=\\"/img/blog/20191129143419.png\\" style={{ zoom:\'200%\' }} />\\n\\nDisplay rollback success, transaction id is also consistent, this is our distributed transaction on the run through, through the interruption point way, you can view the undo_log, you will find that before the transaction is committed, will be deposited into a transaction information data, if the rollback is successful, the information will be deleted.\\n\\n# Summary\\n\\nseata\'s integration is still relatively simple and easy to start, a little more attentive you must write better than me!\\n\\nWelcome to read more seata, dubbo and other source code, can solve the business encountered a lot of pit oh!"},{"id":"/seata-at-mode-start-rm-tm","metadata":{"permalink":"/blog/seata-at-mode-start-rm-tm","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-at-mode-start-rm-tm.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-at-mode-start-rm-tm.md","title":"Does Seata Client Need to Start RM and TM Simultaneously?","description":"A discussion point regarding future optimizations for Seata","date":"2019-11-28T00:00:00.000Z","formattedDate":"November 28, 2019","tags":[],"readingTime":2.735,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"Does Seata Client Need to Start RM and TM Simultaneously?","author":"chenghui.zhang","keywords":["Seata","distributed transaction","AT mode","RM","TM"],"description":"A discussion point regarding future optimizations for Seata","date":"2019/11/28"},"unlisted":false,"prevItem":{"title":"Integrating Seata Distributed Transaction with SpringBoot+Dubbo+MybatisPlus","permalink":"/blog/springboot-dubbo-mybatisplus-seata"},"nextItem":{"title":"Seata AT Mode Startup Source Code Analysis","permalink":"/blog/seata-at-mode-start"}},"content":"When analysing the source code of the startup section, I found that GlobalTransactionScanner will start both RM and TM client, but according to Seata\'s design, TM is responsible for global transaction operation, if a service does not need to open global transaction, then there is no need to start TM client, that is to say, if there is no global transaction annotation in the project, then there is no need to initialize TM client, because not every microservice needs GlobalTransactional, it just acts as an RM client. That is to say, if there is no global transaction annotation in the project, there is no need to initialise the TM client at this time, because not every microservice needs GlobalTransactional, and it is only used as an RM client at this time.\\n\\n\\nSo I proceeded to change the initialisation rules of GlobalTransactionScanner slightly, since previously GlobalTransactionScanner called the initialisation method in the afterPropertiesSet() method of InitializingBean, the afterPropertySet() method was used to initialise the TM client. AfterPropertySet() is only called after the current bean is initialised, there is no way to know if the Spring container has a global transaction annotation.\\n\\nTherefore, I removed the InitializingBean and implemented ApplicationListener instead, checking for GlobalTransactional annotations during bean instantiation, and then calling RM and TM client initialisation methods after the Spring container initialisation is complete. Finally, after the Spring container is initialised, the RM and TM client initialisation methods are called, and then you can decide whether to start the TM client or not, depending on whether the GlobalTransactional annotation is used in the project or not.\\n\\nHere is the PR address: [https://github.com/apache/incubator-seata/pull/1936](https://github.com/apache/incubator-seata/pull/1936)\\n\\nAs we discussed in pr, the current design of Seata is that only the TM at the initiator can initiate GlobalRollbackRequest, and the RM can only send BranchReport(false) to report the branch status to the TC server, and cannot send GlobalRollbackRequest directly to perform global rollback. operation. The interaction logic is as follows:\\n\\n! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191128094250.png)\\n\\nAccording to the above design model, the TM client can be started on demand.\\n\\nHowever, in the later optimisation iterations of Seata, there is one more point that needs to be considered:\\n\\nWhen an exception occurs in a participant, is it possible to initiate a global rollback directly from the participant\'s TM client? This also means that the cycle time of distributed transactions can be shortened, and global locks can be released as soon as possible so that other transactions with conflicting data can acquire locks and execute as soon as possible.\\n\\n! [](https://gitee.com/objcoding/md-picture/raw/master/img/20191127202606.png)\\n\\n\\nThat is to say, in a global transaction, as long as one RM client fails to execute a local transaction, the TM client of the current service will directly initiate a global transaction rollback, so there is no need to wait for the TM of the initiator to initiate a resolution rollback notification. To achieve this optimisation, each service needs to start both the TM client and the RM client.\\n\\n# Author Bio:\\n\\nZhang Chenghui, currently working in the Information Centre of China Communication Technology, Technology Platform Department, as a Java engineer, mainly responsible for the development of China Communication messaging platform and the whole link pressure test project, love to share technology, WeChat public number \\"back-end advanced\\" author, technology blog ([https://objcoding.com/](https://objcoding.com/)) Blogger, Seata Contributor, GitHub ID: objcoding."},{"id":"/seata-at-mode-start","metadata":{"permalink":"/blog/seata-at-mode-start","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-at-mode-start.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-at-mode-start.md","title":"Seata AT Mode Startup Source Code Analysis","description":"Seata Source Code Analysis Series","date":"2019-11-27T00:00:00.000Z","formattedDate":"November 27, 2019","tags":[],"readingTime":12.435,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"Seata AT Mode Startup Source Code Analysis","author":"chenghui.zhang","keywords":["Seata","distributed transaction","AT mode"],"description":"Seata Source Code Analysis Series","date":"2019/11/27"},"unlisted":false,"prevItem":{"title":"Does Seata Client Need to Start RM and TM Simultaneously?","permalink":"/blog/seata-at-mode-start-rm-tm"},"nextItem":{"title":"Designing More Flexible Financial Applications with Seata Saga","permalink":"/blog/design-more-flexable-application-by-saga"}},"content":"From the previous article \\"[Design Principles of Distributed Transaction Middleware Seata](https://mp.weixin.qq.com/s/Pypkm5C9aLPJHYwcM6tAtA)\\", we talked about some design principles of Seata AT pattern, from which we also know the three roles of AT pattern (RM, TM, TC). I will update the Seata source code analysis series. Today, we are going to analyse what Seata AT mode does at startup.\\n\\n\\n\\n# Client Startup Logic\\n\\nTM is responsible for the whole global transaction manager, so a global transaction is started by TM, TM has a global management class GlobalTransaction, the structure is as follows:\\n\\nio.seata.tm.api.GlobalTransaction\\n\\n```java\\npublic interface GlobalTransaction {\\n\\nvoid begin() throws TransactionException.\\n\\nvoid begin(int timeout) throws TransactionException.\\n\\nvoid begin(int timeout, String name) throws TransactionException; void commit() throws TransactionException.\\n\\nvoid commit() throws TransactionException.\\n\\nvoid rollback() throws TransactionException.\\n\\nGlobalStatus getStatus() throws TransactionException; // ...\\n\\n// ...\\n}\\n```\\n\\nIt is possible to create a GlobalTransaction via GlobalTransactionContext and then use the GlobalTransaction to open, commit, rollback, etc., a global transaction, so we\'re using the Seata AT mode directly in an API way:\\n\\n```java\\n//init seata; TMClient.init(application)\\nTMClient.init(applicationId, txServiceGroup); RMClient.init(applicationId, txServiceGroup)\\nRMClient.init(applicationId, txServiceGroup);\\n//trx\\nGlobalTransaction tx = GlobalTransactionContext.getCurrentOrCreate();\\ntry {\\n  tx.begin(60000, \\"testBiz\\");\\n  // Transaction\\n  // ...\\n  tx.commit(); } catch (Exception exx)\\n} catch (Exception exx) {\\n  tx.rollback(); } catch (Exception exx) { tx.rollback(); }\\n  throw exx; } catch (Exception exx) { tx.rollback(); throw exx; }\\n}\\n```\\n\\nIf you write this every time you use a global transaction, it will inevitably cause code redundancy, our projects are based on the Spring container, we can use the characteristics of Spring AOP, with template patterns to encapsulate this redundant code in the template, reference Mybatis-spring also does this thing, so let\'s analyse what a Spring-based So let\'s analyse what a Spring-based project does when it starts Seata and registers a global transaction.\\n\\nWe enable a global transaction by adding the `@GlobalTransactional` annotation to the method. Seata\'s Spring module has a GlobalTransactionScanner, which has the following inheritance relationship:\\n\\n```java\\npublic class GlobalTransactionScanner extends AbstractAutoProxyCreator implements InitialisingBean, ApplicationContextAware, DisposableBean {\\n// ...\\n}\\n```\\n\\nDuring the startup of a Spring-based project, the following initialisation process occurs for this class:\\n\\n! [image-20191124155455309](https://gitee.com/objcoding/md-picture/raw/master/img/image-20191124155455309.png)\\n\\nThe afterPropertiesSet() method of InitialisingBean calls the initClient() method:\\n\\nio.seata.spring.annotation.GlobalTransactionScanner#initClient\\n\\n```java\\nTMClient.init(applicationId, txServiceGroup);\\nRMClient.init(applicationId, txServiceGroup).\\n```\\n\\nInitialisation operations are done for TM and RM.\\n\\n- TM initialisation\\n\\nio.seata.tm.TMClient#init\\n\\n```java\\npublic static void init(String applicationId, String transactionServiceGroup) {\\n  // Get the TmRpcClient instance.\\n  TmRpcClient tmRpcClient = TmRpcClient.getInstance(applicationId, transactionServiceGroup); // Initialise the TM Client.\\n  // Initialise the TM Client\\n  tmRpcClient.init();\\n}\\n```\\n\\nCalling the TmRpcClient.getInstance() method acquires an instance of the TM Client, and during the acquisition process creates the Netty Client Profile object, as well as the messageExecutor thread pool, which is used to handle various message interactions with the server, and during the creation of the TmRpcClient instance, the Create a ClientBootstrap, which is used to manage the start and stop of the Netty service, and a ClientChannelManager, which is used to manage the Netty client object pool, which is used in conjunction with the Netty part of Seata, and which will be discussed later in the Analysing Networks module.\\n\\nio.seata.core.rpc.netty.AbstractRpcRemotingClient#init\\n\\n```java\\npublic void init() {\\n  clientBootstrap.start();\\n  // Timer to try to connect to the server\\n  timerExecutor.scheduleAtFixedRate(new Runnable() {\\n    @Override\\n    public void run() {\\n      clientChannelManager.reconnect(getTransactionServiceGroup());\\n    }\\n  }, SCHEDULE_INTERVAL_MILLS, SCHEDULE_INTERVAL_MILLS, TimeUnit.SECONDS);\\n  mergeSendExecutorService = new ThreadPoolExecutor(MAX_MERGE_SEND_THREAD,\\n                                                    MAX_MERGE_SEND_THREAD, MAX_MERGE_SEND_THREAD, KEEP_ALIVE_TIME\\n                                                    KEEP_ALIVE_TIME, TimeUnit.\\n                                                    new LinkedBlockingQueue<>(),\\n                                                    new NamedThreadFactory(getThreadPrefix(), MAX_MERGE_SEND_THREAD));\\n  mergeSendExecutorService.submit(new MergedSendRunnable());\\n  super.init();\\n}\\n```\\n\\nCalling the TM client init() method will eventually start the netty client (it\'s not really started yet, it will be started when the object pool is called); a timed task is started to resend the RegisterTMRequest (the RM client sends the RegisterRMRequest) request to try to connect to the server, the logic for this is The logic is that the client channel is cached in channels in the NettyClientChannelManager, so if the channels don\'t exist and are out of date, then it will try to connect to the server in order to fetch the channel again and cache it in channels; a separate thread is started to handle asynchronous request sending. This is a very clever use of the network module, which will be analysed later in the analysis.\\n\\nio.seata.core.rpc.netty.AbstractRpcRemoting#init\\n\\n\\n```java\\npublic void init() {\\ntimerExecutor.scheduleAtFixedRate(new Runnable() {\\npublic void run() { scheduleAtFixedRate(new Runnable() {\\npublic void run() {\\nfor (Map.Entry<Integer, MessageFuture> entry : futures.entrySet()) {\\nif (entry.getValue().isTimeout()) {\\nfutures.remove(entry.getKey());\\nentry.getValue().setResultMessage(null);\\nif (LOGGER.isDebugEnabled()) {\\nLOGGER.debug(\\"timeout clear future: {}\\", entry.getValue().getRequestMessage().getBody());\\n}\\n}\\n}\\n\\n      nowMills = System.currentTimeMillis();\\n    }\\n}, TIMEOUT_CHECK_INTERNAL, TIMEOUT_CHECK_INTERNAL, TimeUnit.MILLISECONDS);\\n}\\n```\\n\\nIn the init method of AbstractRpcRemoting, it opens a timer task, which is mainly used to clear the expired futrue of futures. futures is a future object that saves the results of sending requests, and this object has a timeout period, after which an exception will be thrown. Therefore, you need to clear the expired futures regularly.\\n\\n- RM Initialisation\\n\\n```java\\nio.seata.rm.RMClient#init\\npublic static void init(String applicationId, String transactionServiceGroup) {\\n  RmRpcClient rmRpcClient = RmRpcClient.getInstance(applicationId, transactionServiceGroup);\\n  rmRpcClient.setResourceManager(DefaultResourceManager.get());\\n  rmRpcClient.setClientMessageListener(new RmMessageListener(DefaultRMHandler.get());\\n  rmRpcClient.init();\\n}\\n```\\n\\nRmRpcClient.getInstance handles the same logic as the TM; ResourceManager is the RM resource manager responsible for branch transaction registration, commit, report, and rollback operations, as well as global lock querying operations, and DefaultResourceManager will hold all current RM resource managers. The DefaultResourceManager holds all current RM resource managers. DefaultResourceManager will hold all the current RM resource managers for unified call processing, and get() method is mainly to load the current resource manager, mainly using a mechanism similar to SPI, for flexible loading, as shown in the following figure, Seata will scan the META- INF/services/ directory for configuration classes and load them dynamically.\\n\\nClientMessageListener is a RM message listener, which is responsible for processing commands sent from TC and performing branch commit, branch rollback, and undo log deletion operations on the branch; finally, the init method follows the same logic as the TM; DefaultRMHandler encapsulates some of the specific operation logic of RM branching transactions. logic.\\n\\nLet\'s take a look at what the wrapIfNecessary method does.\\n\\nio.seata.spring.annotation.GlobalTransactionScanner#wrapIfNecessary\\n\\n```java\\nprotected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) { // Determine if there is a global transaction scanner turned on?\\n// Determine if global transactions are enabled\\nif (disableGlobalTransaction) {\\nReturns the bean;\\n}\\ntry {\\nsynchronized (PROXYED_SET) {\\nif (PROXYED_SET.contains(beanName)) {\\nReturn the bean;\\n}\\nInterceptor = null;\\n// Check the TCC proxy\\nif (TCCBeanParserUtils.isTccAutoProxy(bean, beanName, applicationContext)) { //TCC interceptor, proxy bean for sofa:reference/dubbo.\\n//TCC interceptor, proxy bean for sofa:reference/dubbo:reference and LocalTCC.\\ninterceptor = new TccActionInterceptor(TCCBeanParserUtils.getRemotingDesc(beanName));\\n} else {\\nClass<? > serviceInterface = SpringProxyUtils.findTargetClass(bean);\\nClass<? >[] interfacesIfJdk = SpringProxyUtils.findInterfaces(bean);\\n\\n        // Determine if the bean has the GlobalTransactional and GlobalLock annotations.\\n        if (!existsAnnotation(new Class[]{serviceInterface}))\\n            && !existsAnnotation(interfacesIfJdk)) {\\n          Return the bean;\\n        }\\n\\n        if (interceptor == null) { // create the proxy class\\n          // Create the proxy class\\n          interceptor = new GlobalTransactionalInterceptor(failureHandlerHook);\\n        }\\n      }\\n\\n      LOGGER.info(\\"Bean [{}] with name [{}] would use interceptor [{}]\\",\\n                  bean.getClass().getName(),beanName,interceptor.getClass().getName());\\n      if (!AopUtils.isAopProxy(bean)) {\\n        bean = super.wrapIfNecessary(bean, beanName, cacheKey);\\n      } else {\\n        AdvisedSupport advised = SpringProxyUtils.getAdvisedSupport(bean);\\n        // Perform wrapping the target object to the proxy object\\n        Advisor[] advisor = super.buildAdvisors(beanName, getAdvicesAndAdvisorsForBean(null, null, null));\\n        for (Advisor avr : advisor) {\\n          advised.addAdvisor(0, avr);\\n        }\\n      }\\n      PROXYED_SET.add(beanName);\\n      Returns the bean;\\n    }\\n} catch (Exception exx) {\\nthrow new RuntimeException(exx);\\n}\\n}\\n```\\n\\n\\nGlobalTransactionScanner inherits AbstractAutoProxyCreator for Spring AOP support, and as you can see from the code, GlobalTransactionalInterceptor is used instead of the methods annotated with GlobalTransactional and GlobalLock annotated methods.\\n\\nGlobalTransactionalInterceptor implements MethodInterceptor: method interceptor.\\n\\n```java\\n\\nio.seata.spring.annotation.GlobalTransactionalInterceptor#invoke\\npublic Object invoke(final MethodInvocation methodInvocation) throws Throwable {\\n  Class<? > targetClass = methodInvocation.getThis() ! = null ? AopUtils.getTargetClass(methodInvocation.getThis()) : null;\\n  Method specificMethod = ClassUtils.getMostSpecificMethod(methodInvocation.getMethod(), targetClass);\\n  final Method method = BridgeMethodResolver.findBridgedMethod(specificMethod);\\n\\n  final GlobalTransactional globalTransactionalAnnotation = getAnnotation(method, GlobalTransactional.class);\\n  final GlobalLock globalLockAnnotation = getAnnotation(method, GlobalLock.class);\\n  if (globalTransactionalAnnotation ! = null) { // globalTransactionalAnnotation !\\n    // globalTransactionalAnnotation\\n    return handleGlobalTransaction(methodInvocation, globalTransactionalAnnotation);\\n  } else if (globalLockAnnotation ! = null) { // globalLockAnnotation !\\n    // global lock annotation\\n    return handleGlobalLock(methodInvocation);\\n  } else {\\n    return methodInvocation.proceed();\\n  }\\n}\\n```\\n\\nThe above is the logic executed by the proxy method, where the handleGlobalTransaction() method calls the TransactionalTemplate template inside: io.seata.spring.annotation.GlobalTransactionalInterceptor #handleGlobalTransaction()\\n\\nio.seata.spring.annotation.GlobalTransactionalInterceptor#handleGlobalTransaction\\n\\n```java\\nprivate Object handleGlobalTransaction(final MethodInvocation methodInvocation,\\n                                       final GlobalTransactional globalTrxAnno) throws Throwable {\\n  try {\\n    return transactionalTemplate.execute(new TransactionalExecutor() {\\n      @Override\\n      public Object execute() throws Throwable {\\n        return methodInvocation.proceed();\\n      }\\n      @Override\\n      public TransactionInfo getTransactionInfo() {\\n        // ...\\n      }\\n    });\\n  } catch (TransactionalExecutor.ExecutionException e) {\\n    // ...\\n  }\\n}\\n```\\nThe handleGlobalTransaction() method executes the execute method of the TransactionalTemplate template class:\\n\\nio.seata.tm.api.TransactionalTemplate#execute\\n\\n```java\\npublic Object execute(TransactionalExecutor business) throws Throwable {\\n// 1. get or create a transaction\\nGlobalTransaction tx = GlobalTransactionContext.getCurrentOrCreate(); // 1.1 get transactionInfo = GlobalTransactionContext.\\n\\n// 1.1 get transactionInfo\\nTransactionInfo txInfo = business.getTransactionInfo(); if (txInfo = txInfo.getCurrentOrCreate())\\nif (txInfo == null) {\\nthrow new ShouldNeverHappenException(\\"transactionInfo does not exist\\"); }\\n}\\ntry {\\n\\n    // 2. begin transaction\\n\\n\\n    Object rs = null; } try { // 2.\\n    try {\\n\\n      // Do Your Business\\n      rs = business.execute(); } catch (Throwable ex) { // Do Your Business.\\n\\n    } catch (Throwable ex) {\\n\\n      // 3. the needed business exception to rollback.\\n      completeTransactionAfterThrowing(txInfo,tx,ex); } throw ex; }\\n      throw ex; } catch (Throwable ex) { // 3.\\n    }\\n\\n    // 4. everything is fine, commit.\\n    commitTransaction(tx); return rs; }\\n\\n    commitTransaction(tx); return rs.\\n} finally {\\n} finally { // 5. clear\\ntriggerAfterCompletion(); cleanUp(); }\\ncleanUp();\\n}\\n}\\n```\\n\\nDoesn\'t the above give you a sense of d\xe9j\xe0 vu? That\'s right, the above is often written when we use the API redundant code, now Spring through the proxy model, the redundant code are encapsulated with the template inside it, it will be those redundant code is encapsulated in a unified process processing, and do not need to show you write out, interested can also go to look at the source code of the Mybatis-spring, is also written very exciting.\\n\\n\\n\\n# server-side processing logic\\n\\nThe server receives the client\'s connection, that is, of course, the channel is also cached up, also said that the client will send RegisterRMRequest/RegisterTMRequest request to the server, the server receives the ServerMessageListener listener will be called to deal with:\\n\\nio.seata.core.rpc.ServerMessageListener\\n\\n```java\\npublic interface ServerMessageListener {\\n  // Handles various transactions, such as branch registration, branch commit, branch report, branch rollback, etc.\\n  void onTrxMessage(RpcMessage request, ChannelHandlerContext ctx, ServerMessageSender sender); // Handle the registration of RM clients.\\n\\t// Handle the registration of the RM client\'s connection\\n  void onRegRmMessage(RpcMessage request, ChannelHandlerContext ctx, ServerMessageSender sender); // Handle the registration of the RM client.\\n                      ServerMessageSender sender, RegisterCheckAuthHandler checkAuthHandler); // Handle the registration of the TM client.\\n  // Handle the registration of the TM client\'s connection.\\n  void onRegTmMessage(RpcMessage request, ChannelHandlerContext ctx, ServerMessageSender sender, RegisterCheckAuthHandler checkAuthHandler)\\n                      ServerMessageSender sender, RegisterCheckAuthHandler checkAuthHandler); // Handle TM client\'s registered connection.\\n  // The server maintains a heartbeat with the client\\n  void onCheckMessage(RpcMessage request, ChannelHandlerContext ctx, ServerMessageSender sender)\\n\\n}\\n```\\n\\nChannelManager is the manager of the server channel, every time the server communicates with the client, it needs to get the corresponding channel of the client from the ChannelManager, which is used to save the cache structure of the TM and RM client channel as follows:\\n\\n```java\\n/**\\n * resourceId -> applicationId -> ip -> port -> RpcContext\\n */\\nprivate static final ConcurrentMap<String, ConcurrentMap<String, ConcurrentMap<String, ConcurrentMap<Integer.\\nRpcContext>>>>\\n  RM_CHANNELS = new ConcurrentHashMap<String, ConcurrentMap<String, ConcurrentMap<String, ConcurrentMap<Integer, RpcContext\\nRpcContext>>>>();\\n\\n/**\\n * ip+appname,port\\n */\\nprivate static final ConcurrentMap<String, ConcurrentMap<Integer, RpcContext>> TM_CHANNELS\\n  = new ConcurrentHashMap<String, ConcurrentMap<Integer, RpcContext>>();\\n```\\n\\nThe above Map structure is a bit complicated:\\n\\nRM_CHANNELS:\\n\\n1. resourceId refers to the database address of the RM client;\\n2. applicationId refers to the service Id of the RM client, for example, account-service in springboot\'s configuration spring.application.name=account-service is the applicationId. 3. ip refers to the service Id of the RM client, for example, account-service in spring.application.name=account-service is the applicationId;\\n3. ip refers to the RM client service address. 4. port refers to the RM client service address;\\n4. port refers to the RM client service address;\\n5. RpcContext saves the information of this registration request.\\n\\nTM_CHANNELS:\\n\\n1. ip+appname: the comment here should be written wrongly, it should be appname+ip, that is, the first key of the Map structure of TM_CHANNELS is appname+ip;\\n2. port: the port number of the client.\\n\\nThe following is the RM Client registration logic:\\n\\nio.seata.core.rpc.ChannelManager#registerRMChannel\\n\\n```java\\npublic static void registerRMChannel(RegisterRMRequest resourceManagerRequest, Channel channel)\\nthrows IncompatibleVersionException {\\nVersion.checkVersion(resourceManagerRequest.getVersion()); // Register the ResourceIds database.\\n// Put the ResourceIds database connection connection information into a set\\nSet<String> dbkeySet = dbKeytoSet(resourceManagerRequest.getResourceIds()); // put the ResourceIds database connection connection information into a set.\\nRpcContext rpcContext;\\n// Determine if the channel information is available from the cache\\nif (!IDENTIFIED_CHANNELS.containsKey(channel)) {\\n// Build the rpcContext based on the request registration information.\\nrpcContext = buildChannelHolder(NettyPoolKey.TransactionRole.RMROLE, resourceManagerRequest.getVersion(),\\nresourceManagerRequest.getApplicationId(), resourceManagerRequest.getTransactionServiceGroup(),\\nresourceManagerRequest.getTransactionServiceGroup(), resourceManagerRequest.getResourceIds(), channel);\\n// Put the rpcContext into the cache\\nrpcContext.holdInIdentifiedChannels(IDENTIFIED_CHANNELS); } else { rpcContext.\\n} else {\\nrpcContext = IDENTIFIED_CHANNELS.get(channel);\\nrpcContext.addResources(dbkeySet);\\n}\\nif (null == dbkeySet || dbkeySet.isEmpty()) { return; }\\nfor (String resourceId : dbkeySet) {\\nString clientIp; // Store the request information into RM_Request.\\n// Store the request information into RM_CHANNELS, using java8\'s computeIfAbsent method.\\nConcurrentMap<Integer, RpcContext> portMap = RM_CHANNELS.computeIfAbsent(resourceId, resourceIdKey -> new ConcurrentHashMap<>())\\n.computeIfAbsent(resourceManagerRequest.getApplicationId(), applicationId -> new ConcurrentHashMap<>())\\n.computeIfAbsent(clientIp = getClientIpFromChannel(channel), clientIpKey -> new ConcurrentHashMap<>());\\n// Put the current rpcContext into the portMap.\\nrpcContext.holdInResourceManagerChannels(resourceId, portMap);\\nupdateChannelsResource(resourceId, clientIp, resourceManagerRequest.getApplicationId()); }\\n}\\n}\\n```\\n\\nFrom the above code logic, we can see that the registration of RM client is mainly to put the registration request information into RM_CHANNELS cache, and at the same time, we will also judge from IDENTIFIED_CHANNELS whether the channel of this request has been verified or not, and the structure of IDENTIFIED_CHANNELS is as follows:\\n\\n```java\\nprivate static final ConcurrentMap<Channel, RpcContext> IDENTIFIED_CHANNELS\\n  = new ConcurrentHashMap<>();\\n```\\nIDENTIFIED_CHANNELS contains all TM and RM registered channels.\\n\\nThe following is the TM registration logic:\\n\\nio.seata.core.rpc.ChannelManager#registerTMChannel\\n\\n```java\\npublic static void registerTMChannel(RegisterTMRequest request, Channel channel)\\nthrows IncompatibleVersionException {\\nVersion.checkVersion(request.getVersion());\\n// Build the RpcContext based on the request registration information.\\nRpcContext rpcContext = buildChannelHolder(NettyPoolKey.TransactionRole.TMROLE, request.getVersion(),\\nrequest.getApplicationId(), request.getTransactionServiceHolder(NettyPoolKey.TransactionRole.\\nrequest.getApplicationId(), request.getTransactionServiceGroup(),\\nnull, channel);\\n// Put the RpcContext into the IDENTIFIED_CHANNELS cache.\\n\\n\\nrpcContext.holdInIdentifiedChannels(IDENTIFIED_CHANNELS); // put RpcContext into IDENTIFIED_CHANNELS cache; rpcContext.\\n// account-service:127.0.0.1:63353\\nString clientIdentified = rpcContext.getApplicationId() + Constants.CLIENT_ID_SPLIT_CHAR\\n+ getClientIpFromChannel(channel);\\n// Store the request information in the TM_CHANNELS cache.\\nTM_CHANNELS.putIfAbsent(clientIdentified, new ConcurrentHashMap<Integer, RpcContext>()); // put the request information into the TM_CHANNELS cache.\\n// Create the get from the previous step, and then put the rpcContext into the value of the map.\\nConcurrentMap<Integer, RpcContext> clientIdentifiedMap = TM_CHANNELS.get(clientIdentified);\\nrpcContext.holdInClientChannels(clientIdentifiedMap);\\n}\\n```\\n\\nThe registration of TM client is similar, the information registered is put into the corresponding cache, but the registration logic is simpler than that of RM client, mainly because RM client involves the information of branch transaction resources, and the information needed to be registered will be more than that of TM client.\\n\\nThe above source code analysis is based on version 0.9.0.\\n\\n\\n# About the Author\\n\\nZhang Chenghui, currently working in the Information Centre of China Communication Technology, Technology Platform Department, as a Java engineer, mainly responsible for the development of China Communication messaging platform and the whole link pressure test project, love to share technology, WeChat public number \\"back-end advanced\\" author, technology blog ([https://objcoding.com/](https://objcoding.com/)) Blogger, Seata Contributor, GitHub ID: objcoding."},{"id":"/design-more-flexable-application-by-saga","metadata":{"permalink":"/blog/design-more-flexable-application-by-saga","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/design-more-flexable-application-by-saga.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/design-more-flexable-application-by-saga.md","title":"Designing More Flexible Financial Applications with Seata Saga","description":"This article delves into the pain points of developing distributed financial applications, analyzing solutions from both theoretical and practical perspectives. It explains how to design more flexible financial applications using Seata Saga.","date":"2019-11-04T00:00:00.000Z","formattedDate":"November 4, 2019","tags":[],"readingTime":19.915,"hasTruncateMarker":false,"authors":[{"name":"long187"}],"frontMatter":{"title":"Designing More Flexible Financial Applications with Seata Saga","keywords":["Saga","Seata","Consistency","Financial","Flexibility","Distributed","Transaction"],"description":"This article delves into the pain points of developing distributed financial applications, analyzing solutions from both theoretical and practical perspectives. It explains how to design more flexible financial applications using Seata Saga.","author":"long187","date":"2019-11-04T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Seata AT Mode Startup Source Code Analysis","permalink":"/blog/seata-at-mode-start"},"nextItem":{"title":"Comprehensive Explanation of Distributed Transaction Seata and Its Three Modes","permalink":"/blog/seata-at-tcc-saga"}},"content":"Seata, short for Simple Extensible Autonomous Transaction Architecture, is an all-in-one distributed transaction solution. It provides AT, TCC, Saga, and XA transaction modes. This article provides a detailed explanation of the Saga mode within Seata, with the project hosted on [GitHub](https://github.com/apache/incubator-seata).\\n\\nAuthor: Yiyuan (Chen Long), Core Developer of Distributed Transactions at Ant Financial.\\n\\n<a name=\\"uTwja\\"></a>\\n\\n# Pain Points in Financial Distributed Application Development\\n\\nDistributed systems face a prominent challenge where a business process requires a composition of various services. This challenge becomes even more pronounced in a microservices architecture, as it necessitates consistency guarantees at the business level. In other words, if a step fails, it either needs to roll back to the previous service invocation or continuously retry to ensure the success of all steps. - From \\"Left Ear Wind - Resilient Design: Compensation Transaction\\"\\n\\nIn the domain of financial microservices architecture, business processes are often more complex. Processes are lengthy, such as a typical internet microloan business process involving calls to more than ten services. When combined with exception handling processes, the complexity increases further. Developers with experience in financial business development can relate to these challenges.\\n\\nDuring the development of financial distributed applications, we encounter several pain points:\\n\\n- **Difficulty Ensuring Business Consistency**<br />\\n\\n  In many of the systems we encounter (e.g., in channel layers, product layers, and integration layers), ensuring eventual business consistency often involves adopting a \\"compensation\\" approach. Without a coordinator to support this, the development difficulty is significant. Each step requires handling \\"rollback\\" operations in catch blocks, resulting in a code structure resembling an \\"arrow,\\" with poor readability and maintainability. Alternatively, retrying exceptional operations, if unsuccessful, might lead to asynchronous retries or even manual intervention. These challenges impose a significant burden on developers, reducing development efficiency and increasing the likelihood of errors.\\n\\n- **Difficulty Managing Business State**<br />\\n\\n  With numerous business entities and their corresponding states, developers often update the entity\'s state in the database after completing a business activity. Lack of a state machine to manage the entire state transition process results in a lack of intuitiveness, increases the likelihood of errors, and causes the business to enter an incorrect state.\\n\\n- **Difficulty Ensuring Idempotence**<br />\\n\\n  Idempotence of services is a fundamental requirement in a distributed environment. Ensuring the idempotence of services often requires developers to design each service individually, using unique keys in databases or distributed caches. There is no unified solution, creating a significant burden on developers and increasing the chances of oversight, leading to financial losses.\\n\\n- **Challenges in Business Monitoring and Operations; Lack of Unified Error Guardian Capability**<br />\\n\\n  Monitoring the execution of business operations is usually done by logging, and monitoring platforms are based on log analysis. While this is generally sufficient, in the case of business errors, these monitors lack immediate access to the business context and require additional database queries. Additionally, the reliance on developers for log printing makes it prone to omissions. For compensatory transactions, there is often a need for \\"error guardian triggering compensation\\" and \\"worker-triggered compensation\\" operations. The lack of a unified error guardian and processing standard requires developers to implement these individually, resulting in a heavy development burden.\\n\\n<a name=\\"hvEU6\\"></a>\\n\\n# Theoretical Foundation\\n\\nIn certain scenarios where strong consistency is required for data, we may adopt distributed transaction schemes like \\"Two-Phase Commit\\" at the business layer. However, in other scenarios, where such strong consistency is not necessary, ensuring eventual consistency is sufficient.\\n\\nFor example, Ant Financial currently employs the TCC (Try, Confirm, Cancel) pattern in its financial core systems. The characteristics of financial core systems include high consistency requirements (business isolation), short processes, and high concurrency.\\n\\nOn the other hand, in many business systems above the financial core (e.g., systems in the channel layer, product layer, and integration layer), the emphasis is on achieving eventual consistency. These systems typically have complex processes, long flows, and may need to call services from other companies (such as financial networks). Developing Try, Confirm, Cancel methods for each service in these scenarios incurs high costs. Additionally, when there are services from other companies in the transaction, it is impractical to require those services to follow the TCC development model. Long processes can negatively impact performance if transaction boundaries are too extensive.\\n\\nWhen it comes to transactions, we are familiar with ACID, and we are also acquainted with the CAP theorem, which states that at most two out of three\u2014Consistency (C), Availability (A), and Partition Tolerance (P)\u2014can be achieved simultaneously. To enhance performance, a variant of ACID known as BASE emerged. While ACID emphasizes consistency (C in CAP), BASE emphasizes availability (A in CAP). Achieving strong consistency (ACID) is often challenging, especially when dealing with multiple systems that are not provided by a single company. BASE systems are designed to create more resilient systems. In many situations, particularly when dealing with multiple systems and providers, BASE systems acknowledge the risk of data inconsistency in the short term. This allows new transactions to occur, with potentially problematic transactions addressed later through compensatory means to ensure eventual consistency.\\n\\nTherefore, in practical development, we make trade-offs. For many business systems above the financial core, compensatory transactions can be adopted. The concept of compensatory transactions has been proposed for about 30 years, with the Saga theory emerging as a solution for long transactions. With the recent rise of microservices, Saga has gradually gained attention in recent years. Currently, the industry generally recognizes Saga as a solution for handling long transactions.\\n\\n> [https://github.com/aphyr/dist-sagas/blob/master/sagas.pdf](https://github.com/aphyr/dist-sagas/blob/master/sagas.pdf)[1] > [http://microservices.io/patterns/data/saga.html](http://microservices.io/patterns/data/saga.html)[2]\\n\\n<a name=\\"k8kbY\\"></a>\\n\\n# Community and Industry Solutions\\n\\n<a name=\\"Oc5Er\\"></a>\\n\\n## Apache Camel Saga\\n\\nCamel is an open-source product that implements Enterprise Integration Patterns (EIP). It is based on an event-driven architecture and offers good performance and throughput. In version 2.21, Camel introduced the Saga EIP.\\n\\nThe Saga EIP provides a way to define a series of related actions through Camel routes. These actions either all succeed or all roll back. Saga can coordinate distributed services or local services using any communication protocol, achieving global eventual consistency. Saga does not require the entire process to be completed in a short time because it does not occupy any database locks. It can support requests that require long processing times, ranging from seconds to days. Camel\'s Saga EIP is based on [MicroProfile\'s LRA](https://github.com/eclipse/microprofile-sandbox/tree/master/proposals/0009-LRA)[3] (Long Running Action). It also supports the coordination of distributed services implemented in any language using any communication protocol.\\n\\nThe implementation of Saga does not lock data. Instead, it defines \\"compensating operations\\" for each operation. When an error occurs during the normal process execution, the \\"compensating operations\\" for the operations that have already been executed are triggered to roll back the process. \\"Compensating operations\\" can be defined on Camel routes using Java or XML DSL (Definition Specific Language).\\n\\nHere is an example of Java DSL:\\n\\n```java\\n// Java DSL example goes here\\n\\n// action\\nfrom(\\"direct:reserveCredit\\")\\n  .bean(idService, \\"generateCustomId\\") // generate a custom Id and set it in the body\\n  .to(\\"direct:creditReservation\\")\\n\\n// delegate action\\nfrom(\\"direct:creditReservation\\")\\n  .saga()\\n  .propagation(SagaPropagation.SUPPORTS)\\n  .option(\\"CreditId\\", body()) // mark the current body as needed in the compensating action\\n  .compensation(\\"direct:creditRefund\\")\\n    .bean(creditService, \\"reserveCredit\\")\\n    .log(\\"Credit ${header.amount} reserved. Custom Id used is ${body}\\");\\n\\n// called only if the saga is cancelled\\nfrom(\\"direct:creditRefund\\")\\n  .transform(header(\\"CreditId\\")) // retrieve the CreditId option from headers\\n  .bean(creditService, \\"refundCredit\\")\\n  .log(\\"Credit for Custom Id ${body} refunded\\");\\n```\\n\\nXML DSL sample:\\n\\n```xml\\n<route>\\n  <from uri=\\"direct:start\\"/>\\n  <saga>\\n    <compensation uri=\\"direct:compensation\\" />\\n    <completion uri=\\"direct:completion\\" />\\n    <option optionName=\\"myOptionKey\\">\\n      <constant>myOptionValue</constant>\\n    </option>\\n    <option optionName=\\"myOptionKey2\\">\\n      <constant>myOptionValue2</constant>\\n    </option>\\n  </saga>\\n  <to uri=\\"direct:action1\\" />\\n  <to uri=\\"direct:action2\\" />\\n</route>\\n```\\n\\n<a name=\\"pQWuF\\"></a>\\n\\n## Eventuate Tram Saga\\n\\n[Eventuate Tram Saga](https://github.com/eventuate-tram/eventuate-tram-sagas)[4]\xa0The framework is a Saga framework for Java microservices using JDBC/JPA. Similar to Camel Saga, it also adopts Java DSL to define compensating operations:\\n\\n```java\\npublic class CreateOrderSaga implements SimpleSaga<CreateOrderSagaData> {\\n\\n  private SagaDefinition<CreateOrderSagaData> sagaDefinition =\\n          step()\\n            .withCompensation(this::reject)\\n          .step()\\n            .invokeParticipant(this::reserveCredit)\\n          .step()\\n            .invokeParticipant(this::approve)\\n          .build();\\n\\n\\n  @Override\\n  public SagaDefinition<CreateOrderSagaData> getSagaDefinition() {\\n    return this.sagaDefinition;\\n  }\\n\\n\\n  private CommandWithDestination reserveCredit(CreateOrderSagaData data) {\\n    long orderId = data.getOrderId();\\n    Long customerId = data.getOrderDetails().getCustomerId();\\n    Money orderTotal = data.getOrderDetails().getOrderTotal();\\n    return send(new ReserveCreditCommand(customerId, orderId, orderTotal))\\n            .to(\\"customerService\\")\\n            .build();\\n\\n...\\n```\\n\\n<a name=\\"scN9h\\"></a>\\n\\n## Apache ServiceComb Saga\\n\\n[ServiceComb Saga](https://github.com/apache/incubator-servicecomb-saga)[5] is also a solution for achieving data eventual consistency in microservices applications. In contrast to [TCC](http://design.inf.usi.ch/sites/default/files/biblio/rest-tcc.pdf), Saga directly commits transactions in the try phase, and the subsequent rollback phase is completed through compensating operations in reverse. What sets it apart is the use of Java annotations and interceptors to define \\"compensating\\" services.<br />\\n\\n#### Architecture:\\n\\nSaga consists of **alpha** and **omega**, where:\\n\\n- Alpha acts as the coordinator, primarily responsible for managing and coordinating transactions;<br />\\n- Omega is an embedded agent in microservices, responsible for intercepting network requests and reporting transaction events to alpha;<br />\\n\\nThe diagram below illustrates the relationship between alpha, omega, and microservices:<br />\\n\\n![ServiceComb Saga](/img/saga/service-comb-saga.png?raw=true)\\n\\n<a name=\\"ggflbq\\"></a>\\n\\n#### sample\uff1a\\n\\n```java\\npublic class ServiceA extends AbsService implements IServiceA {\\n\\n  private static final Logger LOG = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\\n\\n  @Autowired\\n  private IServiceB serviceB;\\n\\n  @Autowired\\n  private IServiceC serviceC;\\n\\n  @Override\\n  public String getServiceName() {\\n    return \\"servicea\\";\\n  }\\n\\n  @Override\\n  public String getTableName() {\\n    return \\"testa\\";\\n  }\\n\\n  @Override\\n  @SagaStart\\n  @Compensable(compensationMethod = \\"cancelRun\\")\\n  @Transactional(rollbackFor = Exception.class)\\n  public Object run(InvokeContext invokeContext) throws Exception {\\n    LOG.info(\\"A.run called\\");\\n    doRunBusi();\\n    if (invokeContext.isInvokeB(getServiceName())) {\\n      serviceB.run(invokeContext);\\n    }\\n    if (invokeContext.isInvokeC(getServiceName())) {\\n      serviceC.run(invokeContext);\\n    }\\n    if (invokeContext.isException(getServiceName())) {\\n      LOG.info(\\"A.run exception\\");\\n      throw new Exception(\\"A.run exception\\");\\n    }\\n    return null;\\n  }\\n\\n  public void cancelRun(InvokeContext invokeContext) {\\n    LOG.info(\\"A.cancel called\\");\\n    doCancelBusi();\\n  }\\n```\\n\\n<a name=\\"CnD8r\\"></a>\\n\\n## Ant Financial\'s Practice\\n\\nAnt Financial extensively uses the TCC mode for distributed transactions, mainly in scenarios where high consistency and performance are required, such as in financial core systems. In upper-level business systems with complex and lengthy processes, developing TCC can be costly. In such cases, most businesses opt for the Saga mode to achieve eventual business consistency. Due to historical reasons, different business units have their own set of \\"compensating\\" transaction solutions, basically falling into two categories:\\n\\n1. When a service needs to \\"retry\\" or \\"compensate\\" in case of failure, a record is inserted into the database with the status before executing the service. When an exception occurs, a scheduled task queries the database record and performs \\"retry\\" or \\"compensation.\\" If the business process is successful, the record is deleted.\\n\\n2. Designing a state machine engine and a simple DSL to orchestrate business processes and record business states. The state machine engine can define \\"compensating services.\\" In case of an exception, the state machine engine invokes \\"compensating services\\" in reverse. There is also an \\"error guardian\\" platform that monitors failed or uncompensated business transactions and continuously performs \\"compensation\\" or \\"retry.\\"\\n\\n## Solution Comparison\\n\\nGenerally, there are two common solutions in the community and industry: one is based on a state machine or a process engine that orchestrates processes and defines compensation through DSL; the other is based on Java annotations and interceptors to implement compensation. What are the advantages and disadvantages of these two approaches?\\n\\n| Approach                      | Pros                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Cons                                                                                                                                                                                                                                                                                                                             |\\n| ----------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| State Machine + DSL           | <br />- Business processes can be defined using visual tools, standardized, readable, and can achieve service orchestration functionality<br />- Improves communication efficiency between business analysts and developers<br />- Business state management: Processes are essentially state machines, reflecting the flow of business states<br />- Enhances flexibility in exception handling: Can implement \\"forward retry\\" or \\"backward compensation\\" after recovery from a crash<br />- Naturally supports asynchronous processing engines such as Actor model or SEDA architecture, improving overall throughput<br /> | <br />- Business processes are composed of JAVA programs and DSL configurations, making development relatively cumbersome<br />- High intrusiveness into existing business if it is a transformation<br />- High implementation cost of the engine<br />                                                                         |\\n| Interceptor + Java Annotation | <br />- Programs and annotations are integrated, simple development, low learning curve<br />- Easy integration into existing businesses<br />- Low framework implementation cost                                                                                                                                                                                                                                                                                                                                                                                                                                             | <br />- The framework cannot provide asynchronous processing modes such as the Actor model or SEDA architecture to improve system throughput<br />- The framework cannot provide business state management<br />- Difficult to achieve \\"forward retry\\" after crash recovery due to the inability to restore thread context<br /> |\\n\\n## Seata Saga Approach\\n\\nThe introduction of Seata Saga can be found in [Seata Saga Official Documentation](http://seata.io/zh-cn/docs/user/saga.html)[6].\\n\\nSeata Saga adopts the state machine + DSL approach for the following reasons:\\n\\n- The state machine + DSL approach is more widely used in practical production scenarios.\\n- Can use asynchronous processing engines such as the Actor model or SEDA architecture to improve overall throughput.\\n- Typically, business systems above the core system have \\"service orchestration\\" requirements, and service orchestration has transactional eventual consistency requirements. These two are challenging to separate. The state machine + DSL approach can simultaneously meet these two requirements.\\n- Because Saga mode theoretically does not guarantee isolation, in extreme cases, it may not complete the rollback operation due to dirty writing. For example, in a distributed transaction, if you recharge user A first and then deduct the balance from user B, if A user consumes the balance before the transaction is committed, and the transaction is rolled back, there is no way to compensate. Some business scenarios may allow the business to eventually succeed, and in cases where rollback is impossible, it can continue to retry the subsequent process. The state machine + DSL approach can achieve the ability to \\"forward\\" recover context and continue execution, making the business eventually successful and achieving eventual consistency.\\n\\n> In cases where isolation is not guaranteed: When designing business processes, follow the principle of \\"prefer long \u6b3e, not short \u6b3e.\\" Long \u6b3e means fewer funds for customers and more funds for institutions. Institutions can refund customers based on their credibility. Conversely, short \u6b3e means less funding for institutions, and the funds may not be recovered. Therefore, in business process design, deduction should be done first.\\n\\n### State Definition Language (Seata State Language)\\n\\n1. Define the service call process through a state diagram and generate a JSON state language definition file.\\n2. In the state diagram, a node can be a service call, and the node can configure its compensating node.\\n3. The JSON state diagram is driven by the state machine engine. When an exception occurs, the state engine executes the compensating node corresponding to the successfully executed node to roll back the transaction.\\n\\n   > Note: Whether to compensate when an exception occurs can also be user-defined.\\n\\n4. It can meet service orchestration requirements, supporting one-way selection, concurrency, asynchronous, sub-state machine, parameter conversion, parameter mapping, service execution status judgment, exception capture, and other functions.\\n\\nAssuming a business process calls two services, deducting inventory (InventoryService) and deducting balance (BalanceService), to ensure that in a distributed scenario, either both succeed or both roll back. Both participant services have a `reduce` method for inventory deduction or balance deduction, and a `compensateReduce` method for compensating deduction operations. Let\'s take a look at the interface definition of InventoryService:\\n\\n```java\\npublic interface InventoryService {\\n\\n    /**\\n     * reduce\\n     * @param businessKey\\n     * @param amount\\n     * @param params\\n     * @return\\n     */\\n    boolean reduce(String businessKey, BigDecimal amount, Map<String, Object> params);\\n\\n    /**\\n     * compensateReduce\\n     * @param businessKey\\n     * @param params\\n     * @return\\n     */\\n    boolean compensateReduce(String businessKey, Map<String, Object> params);\\n}\\n```\\n\\n## This is the state diagram corresponding to the business process:\\n\\n![Example State Diagram](/img/saga/demo_statelang.png?raw=true)\\n<br />Corresponding JSON\\n\\n```json\\n{\\n  \\"Name\\": \\"reduceInventoryAndBalance\\",\\n  \\"Comment\\": \\"reduce inventory then reduce balance in a transaction\\",\\n  \\"StartState\\": \\"ReduceInventory\\",\\n  \\"Version\\": \\"0.0.1\\",\\n  \\"States\\": {\\n    \\"ReduceInventory\\": {\\n      \\"Type\\": \\"ServiceTask\\",\\n      \\"ServiceName\\": \\"inventoryAction\\",\\n      \\"ServiceMethod\\": \\"reduce\\",\\n      \\"CompensateState\\": \\"CompensateReduceInventory\\",\\n      \\"Next\\": \\"ChoiceState\\",\\n      \\"Input\\": [\\"$.[businessKey]\\", \\"$.[count]\\"],\\n      \\"Output\\": {\\n        \\"reduceInventoryResult\\": \\"$.#root\\"\\n      },\\n      \\"Status\\": {\\n        \\"#root == true\\": \\"SU\\",\\n        \\"#root == false\\": \\"FA\\",\\n        \\"$Exception{java.lang.Throwable}\\": \\"UN\\"\\n      }\\n    },\\n    \\"ChoiceState\\": {\\n      \\"Type\\": \\"Choice\\",\\n      \\"Choices\\": [\\n        {\\n          \\"Expression\\": \\"[reduceInventoryResult] == true\\",\\n          \\"Next\\": \\"ReduceBalance\\"\\n        }\\n      ],\\n      \\"Default\\": \\"Fail\\"\\n    },\\n    \\"ReduceBalance\\": {\\n      \\"Type\\": \\"ServiceTask\\",\\n      \\"ServiceName\\": \\"balanceAction\\",\\n      \\"ServiceMethod\\": \\"reduce\\",\\n      \\"CompensateState\\": \\"CompensateReduceBalance\\",\\n      \\"Input\\": [\\n        \\"$.[businessKey]\\",\\n        \\"$.[amount]\\",\\n        {\\n          \\"throwException\\": \\"$.[mockReduceBalanceFail]\\"\\n        }\\n      ],\\n      \\"Output\\": {\\n        \\"compensateReduceBalanceResult\\": \\"$.#root\\"\\n      },\\n      \\"Status\\": {\\n        \\"#root == true\\": \\"SU\\",\\n        \\"#root == false\\": \\"FA\\",\\n        \\"$Exception{java.lang.Throwable}\\": \\"UN\\"\\n      },\\n      \\"Catch\\": [\\n        {\\n          \\"Exceptions\\": [\\"java.lang.Throwable\\"],\\n          \\"Next\\": \\"CompensationTrigger\\"\\n        }\\n      ],\\n      \\"Next\\": \\"Succeed\\"\\n    },\\n    \\"CompensateReduceInventory\\": {\\n      \\"Type\\": \\"ServiceTask\\",\\n      \\"ServiceName\\": \\"inventoryAction\\",\\n      \\"ServiceMethod\\": \\"compensateReduce\\",\\n      \\"Input\\": [\\"$.[businessKey]\\"]\\n    },\\n    \\"CompensateReduceBalance\\": {\\n      \\"Type\\": \\"ServiceTask\\",\\n      \\"ServiceName\\": \\"balanceAction\\",\\n      \\"ServiceMethod\\": \\"compensateReduce\\",\\n      \\"Input\\": [\\"$.[businessKey]\\"]\\n    },\\n    \\"CompensationTrigger\\": {\\n      \\"Type\\": \\"CompensationTrigger\\",\\n      \\"Next\\": \\"Fail\\"\\n    },\\n    \\"Succeed\\": {\\n      \\"Type\\": \\"Succeed\\"\\n    },\\n    \\"Fail\\": {\\n      \\"Type\\": \\"Fail\\",\\n      \\"ErrorCode\\": \\"PURCHASE_FAILED\\",\\n      \\"Message\\": \\"purchase failed\\"\\n    }\\n  }\\n}\\n```\\n\\n## This is the state language to some extent referring to [AWS Step Functions](https://docs.aws.amazon.com/zh_cn/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html)[7].\\n\\n<a name=\\"2de9b28a\\"></a>\\n\\n#### Introduction to \\"State Machine\\" Attributes:\\n\\n- Name: Represents the name of the state machine, must be unique;\\n- Comment: Description of the state machine;\\n- Version: Version of the state machine definition;\\n- StartState: The first \\"state\\" to run when starting;\\n- States: List of states, a map structure, where the key is the name of the \\"state,\\" which must be unique within the state machine;\\n\\n<a name=\\"2b956670\\"></a>\\n\\n#### Introduction to \\"State\\" Attributes:\\n\\n- Type: The type of the \\"state,\\" such as:\\n  - ServiceTask: Executes the service task;\\n  - Choice: Single conditional choice route;\\n  - CompensationTrigger: Triggers the compensation process;\\n  - Succeed: Normal end of the state machine;\\n  - Fail: Exceptional end of the state machine;\\n  - SubStateMachine: Calls a sub-state machine;\\n- ServiceName: Service name, usually the beanId of the service;\\n- ServiceMethod: Service method name;\\n- CompensateState: Compensatory \\"state\\" for this state;\\n- Input: List of input parameters for the service call, an array corresponding to the parameter list of the service method, $. represents using an expression to retrieve parameters from the state machine context. The expression uses [SpringEL](https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html)[8], and if it is a constant, write the value directly;\\n- Output: Assigns the parameters returned by the service to the state machine context, a map structure, where the key is the key when placing it in the state machine context (the state machine context is also a map), and the value uses $. as a SpringEL expression, indicating the value is taken from the return parameters of the service, #root represents the entire return parameters of the service;\\n- Status: Mapping of the service execution status, the framework defines three statuses, SU success, FA failure, UN unknown. We need to map the execution status of the service into these three statuses, helping the framework judge the overall consistency of the transaction. It is a map structure, where the key is a condition expression, usually based on the return value of the service or the exception thrown for judgment. The default is a SpringEL expression to judge the return parameters of the service. Those starting with $Exception\\\\{ indicate judging the exception type, and the value is mapped to this value when this condition expression is true;\\n- Catch: Route after catching an exception;\\n- Next: The next \\"state\\" to execute after the service is completed;\\n- Choices: List of optional branches in the Choice type \\"state,\\" where Expression is a SpringEL expression, and Next is the next \\"state\\" to execute when the expression is true;\\n- ErrorCode: Error code for the Fail type \\"state\\";\\n- Message: Error message for the Fail type \\"state\\";\\n\\nFor more detailed explanations of the state language, please refer to [Seata Saga Official Documentation](http://seata.io/zh-cn/docs/user/saga.html)[6[http://seata.io/zh-cn/docs/user/saga.html](http://seata.io/zh-cn/docs/user/saga.html)].\\n\\n<a name=\\"209f0e37\\"></a>\\n\\n### State Machine Engine Principle:\\n\\n![State Machine Engine Principle](/img/saga/saga_engine_mechanism.png?raw=true)\\n\\n- The state diagram in the image first executes stateA, then executes stateB, and then executes stateC;\\n- The execution of \\"states\\" is based on an event-driven model. After stateA is executed, a routing message is generated and placed in the EventQueue. The event consumer takes the message from the EventQueue and executes stateB;\\n- When the entire state machine is started, Seata Server is called to start a distributed transaction, and the xid is generated. Then, the start event of the \\"state machine instance\\" is recorded in the local database;\\n- When a \\"state\\" is executed, Seata Server is called to register a branch transaction, and the branchId is generated. Then, the start event of the \\"state instance\\" is recorded in the local database;\\n- After a \\"state\\" is executed, the end event of the \\"state instance\\" is recorded in the local database, and Seata Server is called to report the status of the branch transaction;\\n- When the entire state machine is executed, the completion event of the \\"state machine instance\\" is recorded in the local database, and Seata Server is called to commit or roll back the distributed transaction;\\n\\n<a name=\\"808e95dc\\"></a>\\n\\n### Design of State Machine Engine:\\n\\n![Design of State Machine Engine](/img/saga/saga_engine.png?raw=true)\\n\\nThe design of the state machine engine is mainly divided into three layers, with the upper layer depending on the lower layer. From bottom to top, they are:\\n\\n- Eventing Layer:\\n\\n  - Implements an event-driven architecture that can push events and be consumed by a consumer. This layer does not care about what the event is or what the consumer executes; it is implemented by the upper layer.\\n\\n- ProcessController Layer:\\n\\n  - Driven by the above Eventing to execute a \\"empty\\" process. The behavior and routing of \\"states\\" are not implemented. It is implemented by the upper layer.\\n    > Based on the above two layers, theoretically, any \\"process\\" engine can be customly extended. The design of these two layers is based on the internal design of the financial network platform.\\n\\n- StateMachineEngine Layer:\\n  - Implements the behavior and routing logic of each type of state in the state machine engine;\\n  - Provides API and state machine language repository;\\n\\n<a name=\\"73a9fddd\\"></a>\\n\\n### Practical Experience in Service Design under Saga Mode\\n\\nBelow are some practical experiences summarized in the design of microservices under Saga mode. Of course, these are recommended practices, not necessarily to be followed 100%. There are \\"workaround\\" solutions even if not followed.\\n\\n> Good news: Seata Saga mode has no specific requirements for the interface parameters of microservices, making Saga mode suitable for integrating legacy systems or services from external institutions.\\n\\n<a name=\\"d64c5051\\"></a>\\n\\n#### Allow Empty Compensation\\n\\n- Empty Compensation: The original service was not executed, but the compensation service was executed;\\n- Reasons:\\n  - Timeout (packet loss) of the original service;\\n  - Saga transaction triggers a rollback;\\n  - The request of the original service is not received, but the compensation request is received first;\\n\\nTherefore, when designing services, it is necessary to allow empty compensation, that is, if the business primary key to be compensated is not found, return compensation success and record the original business primary key.\\n\\n<a name=\\"88a92b17\\"></a>\\n\\n#### Hang Prevention Control\\n\\n- Hang: Compensation service is executed before the original service;\\n- Reasons:\\n  - Timeout (congestion) of the original service;\\n  - Saga transaction rollback triggers a rollback;\\n  - Congested original service arrives;\\n\\nTherefore, check whether the current business primary key already exists in the business primary keys recorded by empty compensation. If it exists, reject the execution of the service.\\n\\n<a name=\\"ce766631\\"></a>\\n\\n#### Idempotent Control\\n\\n- Both the original service and the compensation service need to ensure idempotence. Due to possible network timeouts, a retry strategy can be set. When a retry occurs, idempotent control should be used to avoid duplicate updates to business data.\\n\\n<a name=\\"FO5YS\\"></a>\\n\\n# Summary\\n\\nMany times, we don\'t need to emphasize strong consistency. We design more resilient systems based on the BASE and Saga theories to achieve better performance and fault tolerance in distributed architecture. There is no silver bullet in distributed architecture, only solutions suitable for specific scenarios. In fact, Seata Saga is a product with the capabilities of \\"service orchestration\\" and \\"Saga distributed transactions.\\" Summarizing, its applicable scenarios are:\\n\\n- Suitable for handling \\"long transactions\\" in a microservices architecture;\\n- Suitable for \\"service orchestration\\" requirements in a microservices architecture;\\n- Suitable for business systems with a large number of composite services above the financial core system (such as systems in the channel layer, product layer, integration layer);\\n- Suitable for scenarios where integration with services provided by legacy systems or external institutions is required (these services are immutable and cannot be required to be modified).\\n\\n<a name=\\"3X7vO\\"></a>\\n\\n## Related Links Mentioned in the Article\\n\\n[1][https://github.com/aphyr/dist-sagas/blob/master/sagas.pdf](https://github.com/aphyr/dist-sagas/blob/master/sagas.pdf)<br />[2][http://microservices.io/patterns/data/saga.html](http://microservices.io/patterns/data/saga.html)<br />[3][Microprofile \u7684 LRA](https://github.com/eclipse/microprofile-sandbox/tree/master/proposals/0009-LRA)\uff1a[https://github.com/eclipse/microprofile-sandbox/tree/master/proposals/0009-LRA](https://github.com/eclipse/microprofile-sandbox/tree/master/proposals/0009-LRA)<br />[4][Eventuate Tram Saga](https://github.com/eventuate-tram/eventuate-tram-sagas)\uff1a[https://github.com/eventuate-tram/eventuate-tram-sagas](https://github.com/eventuate-tram/eventuate-tram-sagas)<br />[5][ServiceComb Saga](https://github.com/apache/incubator-servicecomb-saga)\uff1a[https://github.com/apache/servicecomb-pack](https://github.com/apache/servicecomb-pack)<br />[6][Seata Saga \u5b98\u7f51\u6587\u6863](http://seata.io/zh-cn/docs/user/saga.html)\uff1a[http://seata.io/zh-cn/docs/user/saga.html](http://seata.io/zh-cn/docs/user/saga.html)<br />[7][AWS Step Functions](https://docs.aws.amazon.com/zh_cn/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html)\uff1a[https://docs.aws.amazon.com/zh_cn/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html](https://docs.aws.amazon.com/zh_cn/step-functions/latest/dg/tutorial-creating-lambda-state-machine.html)<br />[8][SpringEL](https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html)\uff1a[https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html](https://docs.spring.io/spring/docs/4.3.10.RELEASE/spring-framework-reference/html/expressions.html)<br />"},{"id":"/seata-at-tcc-saga","metadata":{"permalink":"/blog/seata-at-tcc-saga","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-at-tcc-saga.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-at-tcc-saga.md","title":"Comprehensive Explanation of Distributed Transaction Seata and Its Three Modes","description":"This article focuses on sharing the background and theoretical foundation of distributed transactions, as well as the principles of Seata distributed transactions and the implementation of three modes (AT, TCC, Saga) of distributed transactions.","date":"2019-08-11T00:00:00.000Z","formattedDate":"August 11, 2019","tags":[],"readingTime":18.87,"hasTruncateMarker":false,"authors":[{"name":"long187"}],"frontMatter":{"title":"Comprehensive Explanation of Distributed Transaction Seata and Its Three Modes","keywords":["Saga","Seata","AT","TCC","consistency","finance","distributed","transaction"],"description":"This article focuses on sharing the background and theoretical foundation of distributed transactions, as well as the principles of Seata distributed transactions and the implementation of three modes (AT, TCC, Saga) of distributed transactions.","author":"long187","date":"2019-08-11T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Designing More Flexible Financial Applications with Seata Saga","permalink":"/blog/design-more-flexable-application-by-saga"},"nextItem":{"title":"Design Principles of Distributed Transaction Middleware Seata","permalink":"/blog/seata-at-mode-design"}},"content":"Author: Yi Yuan (Chen Long), Ant Gold Services distributed transaction framework core development.\\n<br />This article is based on the topic of \\"Distributed Transaction Seata and its Three Patterns\\" shared at SOFA Meetup#3 on 11 August in Guangzhou, focusing on the background and theoretical foundation of distributed transaction, as well as the principle of Seata distributed transaction and the implementation of distributed transaction in three patterns (AT, TCC, and Saga).\\n\\nThe video and PPT are at the end of this article.\\n\\n![3 Distributed Transaction Seata Three Modes Explained-Eiyuan.jpg](/img/saga/sofameetup3_img/1.jpeg)\\n\\n<a name=\\"Ad95d\\"></a>\\n## I. Background of the emergence of distributed transactions\\n\\n<a name=\\"Q2ayF\\"></a>\\n### 1.1 Distributed Architecture Evolution - Horizontal Splitting of Database\\n\\nAntGold\'s business database was initially a single database with a single table, but with the rapid development of the business data scale, the data volume is getting bigger and bigger, and the single database with a single table is gradually becoming a bottleneck. So we split the database horizontally, splitting the original single database and single table into database slices.\\n\\nAs shown in the figure below, after splitting the database and table, the original write operation that can be completed on a database may be across multiple databases, which gives rise to cross-database transaction problems.\\n\\n![image.png](/img/saga/sofameetup3_img/2.png)\\n\\n\\n<a name=\\"WBQbC\\"></a>\\n### 1.2 Distributed Architecture Evolution - Business Service Splitting\\n\\nIn the early stage of business development, the single business system architecture of \\"one piece of cake\\" can meet the basic business needs. However, with the rapid development of the business, the system\'s access and business complexity are growing rapidly, single-system architecture has gradually become the bottleneck of business development, to solve the problem of high coupling and scalability of the business system demand is becoming stronger and stronger.\\n\\nAs shown in the figure below, Ant Financial Services splits the single business system into multiple business systems in accordance with the design principles of Service Oriented Architecture (SOA), which reduces the coupling between the systems and enables different business systems to focus on their own business, which is more conducive to the development of the business and the scaling of the system capacity.\\n\\n![image.png](/img/saga/sofameetup3_img/3.png)\\n\\nAfter the business system is split according to services, a complete business often needs to call multiple services, how to ensure data consistency between multiple services becomes a difficult problem.\\n\\n\\n<a name=\\"3oIxE\\"></a>\\n## II. Theoretical foundation of distributed transaction\\n\\n<a name=\\"akRiW\\"></a>\\n### 2.1 Two-stage commit protocols\\n\\n![16_16_18__08_13_2019.jpg](/img/saga/sofameetup3_img/4.jpeg)\\n\\nTwo phase commit protocol: transaction manager coordinates resource manager in two phases, the first phase prepares resources, that is, reserve the resources needed for the transaction, if every resource manager resource reservation succeeds, the second phase resource commit is performed, otherwise the coordinated resource manager rolls back the resources.\\n\\n<a name=\\"8tfKI\\"></a>\\n### 2.2 TCC\\n\\n![16_16_51__08_13_2019.jpg](/img/saga/sofameetup3_img/5.jpeg)\\n\\nTCC (Try-Confirm-Cancel) is actually a two-phase commit protocol for servitisation, business developers need to implement these three service interfaces, the first phase of the service is choreographed by the business code to call the Try interface for resource reservation, the Try interface for all participants is successful, the transaction manager will commit the transaction and call the Confirm interface for each participant The transaction manager will commit the transaction and call the Confirm interface of each participant to actually commit the business operation, otherwise the Cancel interface of each participant will be called to rollback the transaction.\\n\\n<a name=\\"IXxpF\\"></a>\\n### 2.3 Saga\\n\\n![3 Distributed Transactions Seata Three Patterns Explained - Yi Yuan-9.jpg](/img/saga/sofameetup3_img/6.jpeg)\\n\\nSaga is a compensation protocol. In Saga mode, there are multiple participants within a distributed transaction, and each participant is an offsetting compensation service that requires the user to implement its forward and reverse rollback operations according to the business scenario.\\n\\nDuring the execution of a distributed transaction, the forward operations of each participant are executed sequentially, and if all forward operations are executed successfully, the distributed transaction commits. If any of the forward operations fails, the distributed transaction backs out and performs a reverse rollback on the previous participants, rolling back the committed participants and returning the distributed transaction to its initial state.\\n\\nSaga theory is from the paper Sagas published by Hector & Kenneth in 1987.<br />\\n<br />Saga Positive Service and Compensation Service also need to be implemented by business developers.\\n\\n<a name=\\"fZPaN\\"></a>\\n## III. Seata and its three patterns explained in detail\\n\\n<a name=\\"IgVM7\\"></a>\\n### 3.1 Distributed transaction Seata introduction\\n\\nSeata (Simple Extensible Autonomous Transaction Architecture) is a distributed transaction solution jointly open-sourced by Ant Financial Services and Alibaba in January 2019.Seata has been open-sourced for about half a year, and currently has more than 11,000 stars. Seata has been open source for about half a year, and now has more than 11,000 stars and a very active community. We warmly welcome you to participate in the Seata community construction, together will Seata become the open source distributed transaction benchmark product.\\n\\nSeata: [https://](https://github.com/apache/incubator-seata)[github.com/apache/incubator-seata](https://github.com/apache/incubator -seata)<br />\\n<br />![image.png](/img/saga/sofameetup3_img/7.png)\\n\\n<a name=\\"zyy0l\\"></a>\\n### 3.2 Distributed Transactions Seata Product Module\\n\\nAs shown in the figure below, there are three major modules in Seata, namely TM, RM and TC. TM and RM are integrated with the business system as clients of Seata, and TC is deployed independently as the server of Seata.\\n\\nTC is deployed independently as a Seata server. [image.png](/img/saga/sofameetup3_img/8.png)\\n\\nThe execution flow of a distributed transaction in Seata:\\n\\n- TM opens distributed transaction (TM registers global transaction record with TC);\\n- According to the business scenario, arrange the resources in the transaction such as database and service (RM reports the resource readiness status to TC);\\n- TM ends the distributed transaction, and the transaction phase ends (TM notifies TC to commit/rollback the distributed transaction);\\n- TC aggregates the transaction information and decides whether the distributed transaction should be committed or rolled back;\\n- TC notifies all RMs to commit/rollback resources, transaction phase 2 ends;\\n\\n<a name=\\"1QKqI\\"></a>\\n### 3.3 Distributed Transactions Seata Solution\\n\\nSeata has four distributed transaction solutions, AT mode, TCC mode, Saga mode and XA mode.\\n\\n![15_49_23__08_13_2019.jpg](/img/saga/sofameetup3_img/9.jpeg)<br />\\n\\n<a name=\\"784n4\\"></a>\\n#### 2.3.1 AT Mode\\n\\nIn January, Seata open sourced AT Mode, a non-intrusive distributed transaction solution. In AT mode, users only need to focus on their own \\"business SQL\\", the user\'s \\"business SQL\\" as a phase, Seata framework will automatically generate the transaction of the two-phase commit and rollback operations.\\n\\n![image.png](/img/saga/sofameetup3_img/10.png)<br />\\n\\n<a name=\\"Acfeo\\"></a>\\n##### How the AT model is non-intrusive to business :\\n\\n- Phase I:\\n\\nIn phase 1, Seata intercepts the \\"business SQL\\", first parses the semantics of the SQL, finds the business data to be updated by the \\"business SQL\\", and then saves it as a \\"before image\\" before updating the business data. Before the business data is updated, it will save it as \\"before image\\", then execute \\"business SQL\\" to update the business data, and after the business data is updated, it will save it as \\"after image\\", and finally generate row locks. The above operations are all done within a single database transaction, which ensures the atomicity of one phase of operation.\\n\\nThis ensures the atomicity of a phase of operations. [image3.png](/img/saga/sofameetup3_img/11.png)\\n\\n- Second-phase commit:\\n\\nIf the second phase is a commit, since the \\"business SQL\\" has already been committed to the database in the first phase, the Seata framework only needs to delete the snapshot data and row locks saved in the first phase to complete the data cleanup.\\n\\n![image 4.png](/img/saga/sofameetup3_img/12.png)\\n\\n- Phase 2 rollback:\\n\\nIf the second phase is a rollback, Seata needs to rollback the \\"business SQL\\" that has been executed in the first phase to restore the business data. The way to rollback is to use \\"before image\\" to restore the business data; however, before restoring, we must first verify the dirty writing, compare the \\"current business data in the database\\" and the \\"after image\\", if the two data are not in the same state, then we will use the \\"after image\\" to restore the business data. However, before restoring, we should first check the dirty writing, compare the \\"current business data in database\\" and \\"after image\\", if the two data are completely consistent, it means there is no dirty writing, and we can restore the business data, if it is inconsistent, it means there is dirty writing, and we need to transfer the dirty writing to manual processing.\\n\\n![image 5.png](/img/saga/sofameetup3_img/13.png)\\n\\nAT mode one phase, two phase commit and rollback are automatically generated by Seata framework, user only need to write \\"business SQL\\", then can easily access distributed transaction, AT mode is a kind of distributed transaction solution without any intrusion to business.\\n\\n<a name=\\"FnD1S\\"></a>\\n#### 2.3.2 TCC Mode\\n\\nIn March 2019, Seata open-sourced the TCC pattern, which is contributed by Ant Gold. the TCC pattern requires users to implement Try, Confirm and Cancel operations according to their business scenarios; the transaction initiator executes the Try method in the first stage, the Confirm method in the second-stage commit, and the Cancel method in the second-stage rollback.\\n\\nThe transaction initiator performs Try in the first stage, Confirm in the second stage, and Cancel in the second stage. [image 6.png](/img/saga/sofameetup3_img/14.png)\\n\\nTCC Three method descriptions:\\n\\n- Try: detection and reservation of resources;\\n- Confirm: the execution of the business operation submitted; require Try success Confirm must be successful;\\n- Cancel: the release of the reserved resources;\\n\\n**Ant Gold\'s practical experience in TCC**<br />**<br />![16_48_02__08_13_2019.jpg](/img/saga/sofameetup3_img/15.jpeg)\\n\\n**1 TCC Design - Business model is designed in 2 phases:**\\n\\nThe most important thing for users to consider when accessing TCC is how to split their business model into two phases.\\n\\nTake the \\"debit\\" scenario as an example, before accessing TCC, the debit of account A can be completed with a single SQL for updating the account balance; however, after accessing TCC, the user needs to consider how to split the original one-step debit operation into two phases and implement it into three methods, and to ensure that the first-phase Try will be successful and the second-phase Confirm will be successful if Try is successful. If Try succeeds in the first stage, Confirm will definitely succeed in the second stage.\\n\\n![image 7.png](/img/saga/sofameetup3_img/16.png)\\n\\nAs shown above, the\\n\\nTry method as a one-stage preparation method needs to do resource checking and reservation. In the deduction scenario, what Try has to do is to check whether the account balance is sufficient and reserve funds for transfer, and the way to reserve is to freeze the transfer funds of account A. After the execution of the Try method, although the balance of account A is still 100, but $30 of it has been frozen and cannot be used by other transactions.\\n\\nThe second stage, the Confirm method, performs the real debit operation; Confirm will use the funds frozen in the Try stage to perform the debit operation; after the Confirm method is executed, the $30 frozen in the first stage has been deducted from account A, and the balance of account A becomes $70.\\n\\nIf the second stage is a rollback, you need to release the $30 frozen in the first stage of Try in the Cancel method, so that account A is back to the initial state, and all $100 is available.\\n\\nThe most important thing for users to access TCC mode is to consider how to split the business model into 2 phases, implement it into 3 methods of TCC, and ensure that Try succeeds and Confirm succeeds. Compared to AT mode, TCC mode is somewhat intrusive to the business code, but TCC mode does not have the global line locks of AT mode, and the performance of TCC will be much higher than AT mode.\\n\\n**2 TCC Design - Allow Null Rollback:**<br />**<br />![16_51_44__08_13_2019.jpg](/img/saga/sofameetup3_img/17.jpeg)\\n\\nThe Cancel interface needs to be designed to allow null rollbacks. When the Try interface is not received due to packet loss, the transaction manager triggers a rollback, which triggers the Cancel interface, which needs to return to the success of the rollback when it finds that there is no corresponding transaction xid or primary key during the execution of Cancel. If the transaction service manager thinks it has been rolled back, otherwise it will keep retrying, and Cancel has no corresponding business data to roll back.\\n\\n**3 TCC Design - Anti-Suspension Control:**<br />**<br />![16_51_56__08_13_2019.jpg](/img/saga/sofameetup3_img/18.jpeg)\\n\\nThe implication of the suspension is that the Cancel is executed before the Try interface, which occurs because the Try times out due to network congestion, the transaction manager generates a rollback that triggers the Cancel interface, and the Try interface call is eventually received, but the Cancel arrives before the Try. According to the previous logic of allowing empty rollback, the rollback will return successfully, the transaction manager thinks the transaction has been rolled back successfully, then the Try interface should not be executed at this time, otherwise it will generate data inconsistency, so we record the transaction xid or business key before the Cancel empty rollback returns successfully, marking this record has been rolled back, the Try interface checks the transaction xid or business key first. The Try interface first checks the transaction xid or business key to identify that the record has been rolled back, and then does not perform the business operation of Try if it has already been marked as rolled back successfully.\\n\\n**4 TCC Design - Power Control:**<br />**<br />![16_52_07__08_13_2019.jpg](/img/saga/sofameetup3_img/19.jpeg)\\n\\nIdempotence means that for the same system, using the same conditions, a single request and repeated multiple requests have the same impact on system resources. Because network jitter or congestion may timeout, transaction manager will retry operation on resources, so it is very likely that a business operation will be called repeatedly, in order not to occupy resources many times because of repeated calls, it is necessary to control idempotency when designing the service, usually we can use the transaction xid or the business primary key to judge the weight to control.\\n\\n<a name=\\"dsMch\\"></a>\\n#### 2.3.3 Saga Patterns\\n\\nSaga mode is Seata\'s upcoming open source solution for long transactions, which will be mainly contributed by Ant Gold. In Saga mode, there are multiple participants within a distributed transaction, and each participant is an offsetting compensation service that requires users to implement its forward and reverse rollback operations according to business scenarios.\\n\\nDuring the execution of a distributed transaction, the forward operations of each participant are executed sequentially, and if all forward operations are executed successfully, the distributed transaction commits. If any of the forward operations fails, the distributed transaction will go back and execute the reverse rollback operations of the previous participants to roll back the committed participants and bring the distributed transaction back to the initial state.\\n\\n![image 8.png](/img/saga/sofameetup3_img/20.png)\\n\\n\\nSaga Pattern Distributed transactions are usually event-driven and executed asynchronously between the various participants, Saga Pattern is a long transaction solution.\\n\\n**1 Saga pattern usage scenario**<br />**<br />![16_44_58__08_13_2019.jpg](/img/saga/sofameetup3_img/21.jpeg)\\n\\nSaga pattern is suitable for business systems with long business processes and the need to ensure the final consistency of transactions. Saga pattern commits local transactions at one stage, and performance can be guaranteed in the case of lock-free and long processes.\\n\\nTransaction participants may be services from other companies or legacy systems that cannot be transformed and provide the interfaces required by TCC, and can use the Saga pattern.\\n\\nThe advantages of the Saga pattern are:\\n\\n- One-stage commit of local database transactions, lock-free, high performance;\\n- Participants can use transaction-driven asynchronous execution, high throughput;\\n- The compensation service is the \\"reverse\\" of the forward service, which is easy to understand and implement;\\n\\nDisadvantages: The Saga pattern does not guarantee isolation because the local database transaction has already been committed in the first phase and no \\"reservation\\" action has been performed. Later we will talk about the lack of isolation of the countermeasures. <br />**2 Saga implementation based on a state machine engine*** <br />**2 Saga implementation based on a state machine engine*** <br />**3\\n\\n![17_13_19__08_13_2019.jpg](/img/saga/sofameetup3_img/22.png)\\n\\nCurrently there are generally two types of Saga implementations, one is achieved through event-driven architecture, and the other is based on annotations plus interceptors to intercept the business of the positive service implementation.Seata is currently implemented using an event-driven mechanism, Seata implements a state machine, which can orchestrate the call flow of the service and the compensation service of the positive service, generating a state diagram defined by a json file, and the state machine The state machine engine is driven to the operation of this map, when an exception occurs, the state machine triggers a rollback and executes the compensation services one by one. Of course, it is up to the user to decide when to trigger the rollback. The state machine can achieve the needs of service orchestration, it supports single selection, concurrency, asynchrony, sub-state machine call, parameter conversion, parameter mapping, service execution state judgement, exception catching and other functions.\\n\\n**3 State Machine Engine Principles**<br />\\n\\n![16_45_32__08_13_2019.jpg](/img/saga/sofameetup3_img/23.png)\\n\\nThe basic principle of this state machine engine is that it is based on an event-driven architecture, where each step is executed asynchronously, and steps flow through an event queue between steps, <br />greatly improving system throughput. Transaction logs are recorded at the time of execution of each step for use when rolling back in the event of an exception. Transaction logs are recorded in the database where the business tables are located to improve performance.\\n\\n**4 State Machine Engine Design\\n\\n![16_45_46__08_13_2019.jpg](/img/saga/sofameetup3_img/24.jpeg)\\n\\nThe state machine engine is divided into a three-tier architecture design, the bottom layer is the \\"event-driven\\" layer, the implementation of the EventBus and the consumption of events in the thread pool, is a Pub-Sub architecture. The second layer is the \\"process controller\\" layer, which implements a minimalist process engine framework that drives an \\"empty\\" process execution. node does, it just executes the process method of each node and then executes the route method to flow to the next node. This is a generic framework, based on these two layers, developers can implement any process engine. The top layer is the \\"state machine engine\\" layer, which implements the \\"behaviour\\" and \\"route\\" logic code of each state node, provides APIs and statechart repositories, and has some other components, such as expression languages, logic languages, and so on. There are also a number of other components, such as expression languages, logic calculators, flow generators, interceptors, configuration management, transaction logging, and so on.\\n\\n**5 The Saga Service Design Experience**\\n\\nSimilar to TCC, Saga\'s forward and reverse services need to follow the following design principles:\\n\\n**1) Saga Service Design - Allow Null Compensation**<br />**<br />![16_52_22__08_13_2019.jpg](/img/saga/sofameetup3_img/25.jpeg)\\n\\n**2) Saga Service Design - Anti-Suspension Control**<br />**<br />![16_52_52__08_13_2019.jpg](/img/saga/sofameetup3_img/26.jpeg)\\n\\n**3) Saga Service Design - Power Control**<br />**<br />![3 Distributed Transactions Seata Three Patterns Explained - Yi Yuan-31.jpg](/img/saga/sofameetup3_img/27.jpeg)\\n\\n**4) Saga Design - Custom Transaction Recovery Strategies**<br />**<br />![16_53_07__08_13_2019.jpg](/img/saga/sofameetup3_img/28.jpeg)\\n\\nAs mentioned earlier, the Saga pattern does not guarantee transaction isolation, and dirty writes can occur in extreme cases. For example, in the case of a distributed transaction is not committed, the data of the previous service was modified, and the service behind the anomaly needs to be rolled back, may not be able to compensate for the operation due to the data of the previous service was modified. One way to deal with this situation is to \\"retry\\" and continue forward to complete the distributed transaction. Since the entire business process is arranged by the state machine, even after the recovery can continue to retry. So you can configure the transaction policy of the process according to the business characteristics, whether to give priority to \\"rollback\\" or \\"retry\\", when the transaction timeout, the Server side will continue to retry according to this policy.\\n\\nSince Saga does not guarantee isolation, we need to achieve the principle of \\"long money rather than short money\\" in business design. Long money refers to the situation when there is a mistake and the money is too much from our point of view, and the money is too little, because if the money is too long, we can refund the money to the customer, but if it is too short, the money may not be recovered, which means that in the business design, we must give priority to \\"rollback\\" or \\"retry\\". That is, when the business is designed, it must be deducted from the customer\'s account before crediting the account, and if the override update is caused by the isolation problem, there will not be a case of less money.\\n\\n**6 Annotation and Interceptor Based Saga Implementation**<br />**<br />![17_13_37__08_13_2019.jpg](/img/saga/sofameetup3_img/29.jpeg)\\n\\nThere is another implementation of Saga that is based on annotations + interceptors, which Seata does not currently implement. You can look at the pseudo-code above to understand it, the @SagaCompensable annotation is defined on the one method, and the compensation method used to define the one method is the compensateOne method. Then the @SagaTransactional annotation is defined on the processA method of the business process code, which starts a Saga distributed transaction, intercepts each forward method with an interceptor, and triggers a rollback operation when an exception occurs, calling the compensation method of the forward method.\\n\\n**7 Comparison of Advantages and Disadvantages of the Two Saga Implementations\\n\\nThe following table compares the advantages and disadvantages of the two Saga implementations:\\n\\n![17_13_49__08_13_2019.jpg](/img/saga/sofameetup3_img/30.jpeg)\\n\\nThe biggest advantage of the state machine engine is that it can be executed asynchronously through an event-driven approach to improve system throughput, service scheduling requirements can be achieved, and in the absence of isolation in the Saga model, there can be an additional \\"retry forward\\" strategy to recover from things. The biggest advantage of annotations and interceptors is that they are easy to develop and low cost to learn.\\n\\n<a name=\\"Gpkrf\\"></a>\\n## Summary\\n\\nThis article first reviewed the background and theoretical basis of distributed transactions, and then focused on the principles of Seata distributed transactions and three patterns (AT, TCC, Saga) of distributed transaction implementation.\\n\\nSeata\'s positioning is a full-scenario solution for distributed transactions, and in the future there will also be XA mode of distributed transaction implementation, each mode has its own application scenarios, AT mode is a non-intrusive distributed transaction solution for scenes that do not want to transform the business, with almost zero learning cost. TCC mode is a high-performance distributed transaction solution for core systems and other scenes that have a high demand for performance. Saga mode is a long transaction solution for business systems that have long business processes and need to ensure the ultimate consistency of transactions. Saga mode submits local transactions at one stage, with no locks, and can ensure performance in the case of long processes, and is mostly used in the channel layer and integration layer of business systems. Transaction participants may be services from other companies or legacy systems that can\'t be transformed to provide the interfaces required by TCC, Saga mode can also be used.\\n\\nThe video review and PPT of this sharing can be viewed at: [https://tech.antfin.com/community/activities/779/review](https://tech.antfin.com/community/activities/779/ review)"},{"id":"/seata-at-mode-design","metadata":{"permalink":"/blog/seata-at-mode-design","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-at-mode-design.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-at-mode-design.md","title":"Design Principles of Distributed Transaction Middleware Seata","description":"Design principles of AT mode","date":"2019-07-11T00:00:00.000Z","formattedDate":"July 11, 2019","tags":[],"readingTime":11.155,"hasTruncateMarker":false,"authors":[{"name":"chenghui.zhang"}],"frontMatter":{"title":"Design Principles of Distributed Transaction Middleware Seata","author":"chenghui.zhang","keywords":["Seata\u3001distributed transaction\u3001AT mode"],"description":"Design principles of AT mode","date":"2019/07/11"},"unlisted":false,"prevItem":{"title":"Comprehensive Explanation of Distributed Transaction Seata and Its Three Modes","permalink":"/blog/seata-at-tcc-saga"},"nextItem":{"title":"Seata Distributed Go Server Officially Open Source - Introduction to TaaS Design","permalink":"/blog/seata-analysis-go-server"}},"content":"Under the microservices architecture system, we can layered design according to business modules, deployed separately, reducing the pressure of service deployment, but also decoupled from the business coupling, to avoid the application gradually become a monster, so that it can be easily scaled up, and in the case of failure of some services will not affect the normal operation of other services. In short, microservices in the rapid development of business brings us more and more advantages, but microservices are not perfect, so we can not blindly over-abuse, it has a lot of shortcomings, and will bring a certain degree of complexity to the system, which is accompanied by distributed transactions, is a microservices architectural system is bound to need to deal with a pain point, but also the industry has always been concerned about a field, and therefore there is a Theories such as CAP and BASE have emerged.\\n\\nAt the beginning of this year, Ali open source a distributed transaction middleware, initially named Fescar, later renamed Seata, in the beginning of its open source, I know it must be fire, because this is an open source project to solve the pain points, Seata began to rush to the business of non-intrusive and high-performance direction to go, which is exactly the solution to the problem of distributed transactions of the urgent needs of us. Because several companies have stayed with the microservices architecture, but in solving the problem of distributed transactions are not very elegant, so I have been concerned about the development of Seata, today it is briefly about some of its design principles, followed by the various modules I will be in-depth analysis of the source code , interested in can continue to pay attention to my public number or blog, do not lose with.\\n\\n\\n\\n\\n# What are the solutions for distributed transaction resolution?\\n\\nCurrently distributed transaction solutions mainly have no invasion of business and invasive solutions, no invasive solutions are mainly based on the database XA protocol two-part submission (2PC) scheme, its advantage is no invasion of business code, but its shortcomings are also very obvious: the database must be required to support the XA protocol, and because of the characteristics of the XA protocol itself, it will result in a long period of time without the release of transactional resources, the locking cycle is long, and in the case of the XA protocol, it will cause a long period of time, but it will not be released. release, locking cycle is long, and in the application layer can not intervene, so it is very poor performance, its existence is equivalent to the fist of seven injuries as \\"hurt seven points, the loss of their own three points\\", so in the Internet project is not very popular this solution.\\n\\nIn order to make up for the low performance of this solution, the big boys have come up with a variety of solutions to solve the problem, but this invariably need to be done through the application layer, that is, invasion of the business approach, such as the well-known TCC programme, based on the TCC, there are also many mature frameworks, such as ByteTCC, tcc-transaction and so on. As well as based on the ultimate consistency of reliable messages to achieve, such as RocketMQ transaction messages.\\n\\nInvasive code solutions are based on the existing situation of \\"last resort\\" solution, in fact, they are very inelegant to implement, a transaction call is usually accompanied by a series of reverse operations on the transaction interface to add a series of, for example, TCC three-stage commit, the logic of the inevitable rollback of the logic of the logic of the logic of the commit, so that the code will make the project very bloated. code will make the project very bloated, high maintenance costs.\\n\\n\\n\\n# Relationships between Seata modules\\n\\nIn response to the above pain points of distributed transaction solutions, it is clear that our ideal distributed transaction solution must be good performance and no intrusion into the business, the business layer does not need to care about the constraints of the distributed transaction mechanism, Seata is precisely in this direction, so it is very much worth looking forward to, it will bring qualitative improvements to our microservices architecture.\\n\\nSo how does Seata do it? Here\'s how its modules relate to each other.\\n\\nSeata\'s design idea is that a distributed transaction can be understood as a global transaction, under which a number of branch transactions are hung, and a branch transaction is a local transaction that meets the ACID, so we can operate the distributed transaction as if it were a local transaction.\\n\\nSeata internally defines three modules to deal with the relationship and processing of global and branch transactions, these three components are:\\n\\n- Transaction Coordinator (TC): The transaction coordinator maintains the state of the global transaction and is responsible for coordinating and driving the commit or rollback of the global transaction.\\n- Transaction Manager (TM): Controls the boundaries of the global transaction and is responsible for opening a global transaction and ultimately initiating a global commit or global rollback resolution.\\n- Resource Manager (RM): Controls branch transactions and is responsible for branch registration, status reporting, and receiving instructions from the Transaction Coordinator to drive the commit and rollback of branch (local) transactions.\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/seata.png)\\n\\nBriefly describe the execution steps of the whole global transaction:\\n\\n1. TM requests TC to open a global transaction. TC creates the global transaction and returns a globally unique XID, which is propagated in the context of the global transaction;\\n2. the RM registers a branch transaction with the TC, which is attributed to the global transaction with the same XID;\\n3. the TM initiates a global commit or rollback to the TC;\\n4. TC schedules the branch transaction under the XID to complete the commit or rollback.\\n\\n\\n\\n# How is it different from the XA scheme?\\n\\nSeata\'s transaction commit method is basically the same as the XA protocol\'s two-stage commit in general, so what is the difference between them?\\n\\nWe all know that the XA protocol relies on the database level to ensure the consistency of transactions, that is, XA branch transactions are driven at the database level, because XA branch transactions need to have XA drivers, on the one hand, it will lead to the database and the XA driver coupling, on the other hand, it will lead to a long period of locking the resources of the transaction of the various branches, which is not popular in the Internet company! This is also an important factor that it is not popular in Internet companies.\\n\\nBased on the above problems of the XA protocol, Seata another way, since the dependence on the database layer will lead to so many problems, then I\'ll do from the application layer to do the trick, which also has to start from Seata\'s RM module, the previous also said that the main role of RM, in fact, RM in the database operation of the internal agent layer, as follows:\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/seata5.png)\\n\\nSeata in the data source to do a layer of proxy layer, so we use Seata, we use the data source is actually using Seata\'s own data source proxy DataSourceProxy, Seata in this layer of the proxy to add a lot of logic, mainly parsing SQL, business data before and after the update of the data mirror organised into a rollback log, and insert the undo log log into the undo_log table to ensure that every business sql that updates data has a corresponding rollback log.\\n\\nThe advantage of doing this is that after the local transaction is executed, the resources locked by the local transaction can be released immediately, and then the branch status can be reported to the TC. When the TM decides to commit globally, there is no need for synchronous coordination, the TC will asynchronously schedule each RM branch transaction to delete the corresponding undo log, which is a very fast step; when the TM decides to roll back globally, the RM receives a rollback request from the TC, and then finds the corresponding undo log through the XID, and then executes the log to complete the rollback operation. operation.\\n\\nRM will find the corresponding undo log through XID and execute the rollback log to complete the rollback operation. ![](https://gitee.com/objcoding/md-picture/raw/master/img/seata6.png)\\n\\nAs shown in the above figure, the RM of the XA scheme is placed in the database layer, and it relies on the XA driver of the database.\\n\\nThe XA scenario RM is placed at the database level as shown in the figure above.\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/Seata7.png)\\n\\nAs shown above, Seata\'s RM is actually placed in the application layer as middleware, and does not rely on the database for protocol support, completely stripping out the protocol support requirements of the database for distributed transaction scenarios.\\n\\n\\n# How are branching transactions committed and rolled back?\\n\\nHere is a detailed description of how branching transactions are committed and rolled back:\\n\\n- Stage one:\\n\\n\\nBranching transactions make use of the JDBC data source proxy in the RM module to join several processes, interpret business SQL, organise the data mirroring of business data before and after updates into a rollback log, generate an undo log log, check global transaction locks, and register branching transactions, etc., and make use of the ACID feature of the local transaction to write the business SQL and the undo log into the same The local transaction ACID feature is used to write the business SQL and undo log into the same thing and submit them to the database together to ensure that the corresponding rollback log must exist for the business SQL, and finally the branch transaction status is reported to the TC.\\n\\n![](https://gitee.com/objcoding/md-picture/raw/master/img/seata2.png)\\n\\n- Phase II:\\n\\n\\nTM resolution global commit:\\n\\nWhen the TM resolution is committed, there is no need for synchronous orchestration, the TC will asynchronously schedule each RM branch transaction to delete the corresponding undo logs, and this step can be completed very quickly. This mechanism is critical for performance improvement. We know that the success rate of transaction execution is very high during normal business operation, so it is possible to commit directly in the local transaction, which is a very significant step for performance improvement.\\n\\n\\n\\nThis step is very significant for performance improvement. ![](https://gitee.com/objcoding/md-picture/raw/master/img/seata3.png)\\n\\n\\n\\nTM resolution global rollback:\\n\\nWhen TM resolves to rollback, RM receives the rollback request from TC, RM finds the corresponding undo log through XID, then uses the ACID feature of the local transaction to execute the rollback log to complete the rollback operation and delete the undo log, and finally reports the rollback result to TC.\\n\\nThe last step is to report the rollback result to the TC. ![](https://gitee.com/objcoding/md-picture/raw/master/img/seata4.png)\\n\\nThe business is not aware of all the above processes, the business does not care about the specific global transaction commit and rollback, and the most important point is that Seata will be two-stage commit synchronisation coordination is decomposed into various branch transactions, branch transactions and ordinary local transactions are not any different, which means that after we use Seata, distributed transactions like the use of local transactions, the database layer of transaction coordination mechanism to the middleware layer. transaction coordination mechanism to the middleware layer Seata to do , so that although the transaction coordination moved to the application layer , but still can do zero intrusion into the business , thus stripping the distributed transaction scheme on the database in the protocol support requirements , and Seata in the branch transaction is completed directly after the release of resources , greatly reducing the branch transaction on the resources of the locking time , perfectly avoiding the XA protocol needs to be The problem of long resource locking time due to synchronous coordination of XA protocol is perfectly avoided.\\n\\n\\n\\n# Supplementation of other solutions\\n\\nThe above is actually the default mode of Seata, also known as AT mode, which is similar to the XA scheme of the two-stage submission scheme, and is non-intrusive on the business, but this mechanism still needs to rely on the database local transaction ACID characteristics, have you noticed that I have stressed in the above chart must be to support the ACID characteristics of relational databases, then the problem is, non-relational or databases that do not support ACID can not use Seata, do not panic, Seata is now prepared for us another mode, called MT mode, which is a business invasive solution, commit rollback and other operations need to be defined by us, the business logic needs to be broken down into Prepare/Commit/Rollback 3 parts, forming a MT branch The purpose of the MT model is to reach more scenarios for Seata by adding global transactions.\\n\\nThe point of this is to reach more scenarios for Seata. ![](https://gitee.com/objcoding/md-picture/raw/master/img/seata8.png)\\n\\nOnly, it is not Seata\'s \\"main\\" model, it exists only as a complementary solution, from the above official development vision can be seen, Seata\'s goal is to always be a non-invasive solution to the business.\\n\\n*Note: The design of the pictures in this article refers to the official Seata diagram*.\\n\\n# Author Bio:\\n\\nZhang Chenghui, currently working in the technology platform department of Zhongtong Technology Information Centre as a Java engineer, mainly responsible for the development of Zhongtong messaging platform and all-links pressure testing project, love to share technology, WeChat public number \\"back-end progression\\" author, technology blog ([https://objcoding.com/](https://objcoding.com/)) Blogger, Seata Contributor, GitHub ID: objcoding."},{"id":"/seata-analysis-go-server","metadata":{"permalink":"/blog/seata-analysis-go-server","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-go-server.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-go-server.md","title":"Seata Distributed Go Server Officially Open Source - Introduction to TaaS Design","description":"Preface","date":"2019-04-23T00:00:00.000Z","formattedDate":"April 23, 2019","tags":[],"readingTime":4.05,"hasTruncateMarker":false,"authors":[{"name":"fagongzi(zhangxu19830126@gmail.com)"}],"frontMatter":{"title":"Seata Distributed Go Server Officially Open Source - Introduction to TaaS Design","author":"fagongzi(zhangxu19830126@gmail.com)","date":"2019/04/23","keywords":["seata","distributed transaction","high availability"]},"unlisted":false,"prevItem":{"title":"Design Principles of Distributed Transaction Middleware Seata","permalink":"/blog/seata-at-mode-design"},"nextItem":{"title":"Fescar Integration with Spring Cloud In-Depth Analysis of Source Code","permalink":"/blog/how-to-support-spring-cloud"}},"content":"### Preface\\nTaaS is a high-availability implementation of the Seata server (TC, Transaction Coordinator), written in `Golang`. Taas has been contributed to the Seata open-source community by InfiniVision (http://infinivision.cn) and is now officially open source.\\n\\nBefore Seata was open-sourced, we began to reference GTS and some open-source projects to implement the distributed transaction solution TaaS (Transaction as a Service).\\n\\nAfter we completed the development of the TaaS server, Seata (then called Fescar) was open-sourced and attracted widespread attention from the open-source community. With Alibaba\'s platform influence and community activity, we believe that Seata will become the standard for open-source distributed transactions in the future. Therefore, we decided to make TaaS compatible with Seata.\\n\\nUpon discovering that Seata\'s server implementation was single-node and lacked high availability, we contacted the Seata community leaders and decided to open-source TaaS to contribute to the open-source community. We will also maintain it in the long term and keep it synchronized with Seata versions.\\n\\nCurrently, the official Java high-availability version of Seata is also under development. TaaS and this high-availability version have different design philosophies and will coexist in the future.\\n\\nTaaS has been open-sourced on GitHub (https://github.com/apache/incubator-seata-go-server). We welcome everyone to try it out.\\n\\n### Design Principles\\n1. High Performance: Performance scales linearly with the number of machines. Adding new machines to the cluster can improve performance.\\n2. High Availability: If a machine fails, the system can still provide services externally, or the service can be restored externally in a short time (the time it takes to switch leaders).\\n3. Auto-Rebalance: When new machines are added to the cluster or machines are offline, the system can automatically perform load balancing.\\n4. Strong Consistency: The system\'s metadata is stored consistently in multiple replicas.\\n\\n### Design\\n![TaaS Design](/img/blog/taas.png)\\n\\n#### High Performance\\nTaaS\'s performance scales linearly with the number of machines. To support this feature, TaaS handles the smallest unit of global transactions called a `Fragment`. The system sets the maximum concurrency of active global transactions supported by each Fragment upon startup. The system also samples each Fragment, and when it becomes overloaded, it generates new Fragments to handle more concurrency.\\n\\n#### High Availability\\nEach `Fragment` has multiple replicas and one leader to handle requests. When the leader fails, the system generates a new leader to handle requests. During the election process of the new leader, the Fragment does not provide services externally, typically for a few seconds.\\n\\n#### Strong Consistency\\nTaaS itself does not store the metadata of global transactions. The metadata is stored in Elasticell (https://github.com/deepfabric/elasticell), a distributed KV storage compatible with the Redis protocol. Elasticell ensures data consistency based on the Raft protocol.\\n\\n#### Auto-Rebalance\\nAs the system runs, there will be many `Fragments` and their replicas, resulting in uneven distribution of Fragments on each machine, especially when old machines are offline or new machines come online. When TaaS starts, it selects three nodes as schedulers, responsible for scheduling these `Fragments` to ensure that the number of Fragments and the number of leaders on each machine are roughly equal. It also ensures that the number of replicas for each Fragment remains at the specified number.\\n\\n##### Fragment Replication Creation\\n![Fragment Replication Creation](/img/blog/taas_add.png)\\n\\n1. At time t0, Fragment1 is created on machine Seata-TC1.\\n2. At time t1, a replica of Fragment1, Fragment1\', is created on machine Seata-TC2.\\n3. At time t2, another replica of Fragment1, Fragment1\\", is created on machine Seata-TC3.\\n\\nBy time t2, all three replicas of Fragment1 are created.\\n\\n##### Fragment Replication Migration\\n![Fragment Replication Migration](/img/blog/taas_move.png)\\n\\n1. At time t0, the system has four Fragments, each existing on machines Seata-TC1, Seata-TC2, and Seata-TC3.\\n2. At time t1, a new machine, Seata-TC4, is added.\\n3. At time t2, replicas of three Fragments are migrated to machine Seata-TC4.\\n\\n### Online Quick Experience\\nWe have set up an experience environment on the public network:\\n* Seata Server Address: 39.97.115.141:8091\\n* UI: http://39.97.115.141:8084/ui/index.html\\n\\n### Local Quick Experience\\nQuickly experience TaaS functionality using docker-compose.\\n```bash\\ngit clone https://github.com/seata/taas.git\\ndocker-compse up -d\\n```\\nDue to the many component dependencies, the docker-compose takes about 30 seconds to start and become available for external services.\\n\\n#### Seata Server Address\\nThe service listens on the default port 8091. Modify the Seata server address accordingly to experience.\\n\\n#### Seata UI \\nAccess the WEB UI at `http://127.0.0.1:8084/ui/index.html`\\n\\n### About InfiniVision\\nInfiniVision is a technology-driven enterprise service provider dedicated to assisting traditional enterprises in digital transformation and upgrading using technologies such as artificial intelligence, cloud computing, blockchain, big data, and IoT edge computing. InfiniVision actively embraces open source culture and open sources core algorithms and architectures. Notable open-source products include the facial recognition software InsightFace (https://github.com/deepinsight/insightface), which has repeatedly won large-scale facial recognition challenges, and the distributed storage engine Elasticell (https://github.com/deepfabric/elasticell).\\n\\n### About the Author\\nThe author, Zhang Xu, is the creator of the open-source Gateway (https://github.com/fagongzi/gateway) and currently works at InfiniVision, focusing on infrastructure-related development."},{"id":"/how-to-support-spring-cloud","metadata":{"permalink":"/blog/how-to-support-spring-cloud","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/how-to-support-spring-cloud.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/how-to-support-spring-cloud.md","title":"Fescar Integration with Spring Cloud In-Depth Analysis of Source Code","description":"Fescar","date":"2019-04-15T00:00:00.000Z","formattedDate":"April 15, 2019","tags":[],"readingTime":16.17,"hasTruncateMarker":false,"authors":[{"name":"shukang.guo min.ji"}],"frontMatter":{"title":"Fescar Integration with Spring Cloud In-Depth Analysis of Source Code","author":"shukang.guo min.ji","date":"2019/04/15","keywords":["fescar","seata","Distributed","transaction"]},"unlisted":false,"prevItem":{"title":"Seata Distributed Go Server Officially Open Source - Introduction to TaaS Design","permalink":"/blog/seata-analysis-go-server"},"nextItem":{"title":"Integrating Seata (formerly Fescar) Distributed Transaction with Spring Cloud","permalink":"/blog/integrate-seata-with-spring-cloud"}},"content":"### Fescar\\n\\nCommon distributed transaction approaches include XA based on 2PC (e.g., Atomikos), TCC (e.g., ByteTCC) focusing on the business layer, and transactional messaging (e.g., RocketMQ Half Message). XA is a protocol for distributed transactions that requires support from local databases. However, the resource locking at the database level can lead to poor performance. On the other hand, TCC, introduced by Alibaba as a preacher, requires a significant amount of business code to ensure transactional consistency, resulting in higher development and maintenance costs.\\n\\nDistributed transactions are a widely discussed topic in the industry, and this is one of the reasons why Fescar has gained 6k stars in a short period of time. The name \\"Fescar\\" stands for Fast & Easy Commit And Rollback. In simple terms, Fescar drives global transactions by coordinating local RDBMS branch transactions. It is a middleware that operates at the application layer. The main advantages of Fescar are better performance compared to XA, as it does not occupy connection resources for a long time, and lower development cost and business invasiveness compared to TCC.\\n\\nSimilar to XA, Fescar divides roles into TC (Transaction Coordinator), RM (Resource Manager), and TM (Transaction Manager). The overall transaction process model of Fescar is as follows:\\n\\n![Fescar\u4e8b\u52a1\u8fc7\u7a0b](/img/blog/fescar-microservices.png)\\n\\n```\\n1.The TM (Transaction Manager) requests the TC (Transaction Coordinator) to start a global transaction. The global transaction is successfully created, and a globally unique XID (Transaction ID) is generated.\\n2.The XID is propagated in the context of the microservice invocation chain.\\n3.The RM (Resource Manager) registers the branch transaction with the TC, bringing it under the jurisdiction of the global transaction corresponding to the XID.\\n4.The TM initiates a global commit or rollback resolution for the XID with the TC.\\n5.The TC schedules the completion of commit or rollback requests for all branch transactions under the jurisdiction of the XID.\\n```\\n\\nIn the current implementation version, the TC (Transaction Coordinator) is deployed as a separate process. It is responsible for maintaining the operation records and global lock records of the global transaction, as well as coordinating and driving the global transaction\'s commit or rollback. On the other hand, the TM (Transaction Manager) and RM (Resource Manager) work in the same application process as the application.\\n\\nThe RM manages the underlying database through proxying the JDBC data source. It uses syntax parsing to retain snapshots and generate undo logs during transaction execution. This ensures that the transaction can be rolled back to its previous state if needed.\\n\\nThis covers the general flow and model division of Fescar. Now, let\'s proceed with the analysis of Fescar\'s transaction propagation mechanism.\\n\\n### Fescar Transaction Propagation Mechanism\\n\\nThe transaction propagation in Fescar includes both nested transaction calls within an application and transaction propagation across different services. So, how does Fescar propagate transactions in a microservices call chain? Fescar provides a transaction API that allows users to manually bind a transaction\'s XID and join it to the global transaction. Therefore, depending on the specific service framework mechanism, we can propagate the XID in the call chain to achieve transaction propagation.\\n\\nThe RPC request process consists of two parts: the caller and the callee. We need to handle the XID during the request and response. The general process is as follows: the caller (or the requester) retrieves the XID from the current transaction context and passes it to the callee through the RPC protocol. The callee extracts the XID from the request and binds it to its own transaction context, thereby participating in the global transaction. Common microservices frameworks usually provide corresponding Filter and Interceptor mechanisms. Now, let\'s analyze the integration process of Spring Cloud and Fescar in more detail.\\n\\n### Partial Source Code Analysis of Fescar Integration with Spring Cloud Alibaba\\n\\nThis section of the source code is entirely from spring-cloud-alibaba-fescar. The source code analysis mainly includes three parts: AutoConfiguration, the microservice provider, and the microservice consumer. Regarding the microservice consumer, it can be further divided into two specific approaches: RestTemplate and Feign. For the Feign request approach, it is further categorized into usage patterns that integrate with Hystrix and Sentine.\\n\\n#### Fescar AutoConfiguration\\n\\nFor the AutoConfiguration analysis, this section will only cover the parts related to the startup of Fescar. The analysis of other parts will be interspersed in the \'Microservice Provider\' and \'Microservice Consumer\' sections.\\n\\nThe startup of Fescar requires the configuration of GlobalTransactionScanner. The GlobalTransactionScanner is responsible for initializing Fescar\'s RM client, TM client, and automatically proxying classes annotated with the GlobalTransactional annotation. The startup of the GlobalTransactionScanner bean is loaded and injected through GlobalTransactionAutoConfiguration, which also injects FescarProperties.\\n\\nFescarProperties contains important properties of Fescar, such as txServiceGroup. The value of this property can be read from the application.properties file using the key \'spring.cloud.alibaba.fescar.txServiceGroup\', with a default value of \'$\\\\{spring.application.name}-fescar-service-group\'. txServiceGroup represents the logical transaction group name in Fescar. This group name is obtained from the configuration center (currently supporting file and Apollo) to retrieve the TC cluster name corresponding to the logical transaction group name. The TC cluster\'s service name is then constructed based on the cluster name. The RM client, TM client, and TC interact through RPC by using the registry center (currently supporting Nacos, Redis, ZooKeeper, and Eureka) and the service name to find available TC service nodes.\\n\\n#### Microservice Provider\\n\\nSince the logic of the consumer is a bit more complex, let\'s first analyze the logic of the provider. For Spring Cloud projects, the default RPC transport protocol is HTTP, so the HandlerInterceptor mechanism is used to intercept HTTP requests.\\n\\nHandlerInterceptor is an interface provided by Spring, and it has three methods that can be overridden.\\n\\n```java\\n    /**\\n\\t * Intercept the execution of a handler. Called after HandlerMapping determined\\n\\t * an appropriate handler object, but before HandlerAdapter invokes the handler.\\n\\t */\\n\\tdefault boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler)\\n\\t\\t\\tthrows Exception {\\n\\n\\t\\treturn true;\\n\\t}\\n\\n\\t/**\\n\\t * Intercept the execution of a handler. Called after HandlerAdapter actually\\n\\t * invoked the handler, but before the DispatcherServlet renders the view.\\n\\t * Can expose additional model objects to the view via the given ModelAndView.\\n\\t */\\n\\tdefault void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler,\\n\\t\\t\\t@Nullable ModelAndView modelAndView) throws Exception {\\n\\t}\\n\\n\\t/**\\n\\t * Callback after completion of request processing, that is, after rendering\\n\\t * the view. Will be called on any outcome of handler execution, thus allows\\n\\t * for proper resource cleanup.\\n\\t */\\n\\tdefault void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler,\\n\\t\\t\\t@Nullable Exception ex) throws Exception {\\n\\t}\\n```\\n\\nAccording to the comments, we can clearly see the timing and common use cases of each method. For Fescar integration, it overrides the preHandle and afterCompletion methods as needed.\\n\\nThe purpose of FescarHandlerInterceptor is to bind the XID passed from the service chain to the transaction context of the service node and clean up related resources after the request is completed. FescarHandlerInterceptorConfiguration is responsible for configuring the interception of all URLs. This interceptor will be executed for all incoming requests to perform XID conversion and transaction binding.\\n\\n```java\\n/**\\n * @author xiaojing\\n *\\n * Fescar HandlerInterceptor, Convert Fescar information into\\n * @see com.alibaba.fescar.core.context.RootContext from http request\'s header in\\n * {@link org.springframework.web.servlet.HandlerInterceptor#preHandle(HttpServletRequest , HttpServletResponse , Object )},\\n * And clean up Fescar information after servlet method invocation in\\n * {@link org.springframework.web.servlet.HandlerInterceptor#afterCompletion(HttpServletRequest, HttpServletResponse, Object, Exception)}\\n */\\npublic class FescarHandlerInterceptor implements HandlerInterceptor {\\n\\n\\tprivate static final Logger log = LoggerFactory\\n\\t\\t\\t.getLogger(FescarHandlerInterceptor.class);\\n\\n\\t@Override\\n\\tpublic boolean preHandle(HttpServletRequest request, HttpServletResponse response,\\n\\t\\t\\tObject handler) throws Exception {\\n\\n\\t\\tString xid = RootContext.getXID();\\n\\t\\tString rpcXid = request.getHeader(RootContext.KEY_XID);\\n\\t\\tif (log.isDebugEnabled()) {\\n\\t\\t\\tlog.debug(\\"xid in RootContext {} xid in RpcContext {}\\", xid, rpcXid);\\n\\t\\t}\\n\\n\\t\\tif (xid == null && rpcXid != null) {\\n\\t\\t\\tRootContext.bind(rpcXid);\\n\\t\\t\\tif (log.isDebugEnabled()) {\\n\\t\\t\\t\\tlog.debug(\\"bind {} to RootContext\\", rpcXid);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\treturn true;\\n\\t}\\n\\n\\t@Override\\n\\tpublic void afterCompletion(HttpServletRequest request, HttpServletResponse response,\\n\\t\\t\\tObject handler, Exception e) throws Exception {\\n\\n\\t\\tString rpcXid = request.getHeader(RootContext.KEY_XID);\\n\\n\\t\\tif (StringUtils.isEmpty(rpcXid)) {\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\n\\t\\tString unbindXid = RootContext.unbind();\\n\\t\\tif (log.isDebugEnabled()) {\\n\\t\\t\\tlog.debug(\\"unbind {} from RootContext\\", unbindXid);\\n\\t\\t}\\n\\t\\tif (!rpcXid.equalsIgnoreCase(unbindXid)) {\\n\\t\\t\\tlog.warn(\\"xid in change during RPC from {} to {}\\", rpcXid, unbindXid);\\n\\t\\t\\tif (unbindXid != null) {\\n\\t\\t\\t\\tRootContext.bind(unbindXid);\\n\\t\\t\\t\\tlog.warn(\\"bind {} back to RootContext\\", unbindXid);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n}\\n\\n```\\n\\nThe preHandle method is called before the request is executed. The xid parameter represents the unique identifier of the global transaction already bound to the current transaction context, while rpcXid represents the global transaction identifier that needs to be bound to the request and is passed through the HTTP header. In the preHandle method, it checks if there is no XID in the current transaction context and if rpcXid is not empty. If so, it binds rpcXid to the current transaction context.\\n\\nThe afterCompletion method is called after the request is completed and is used to perform resource cleanup actions. Fescar uses the RootContext.unbind() method to unbind the XID involved in the transaction context. The logic in the if statement is for code robustness. If rpcXid and unbindXid are not equal, it rebinds unbindXid.\\n\\nFor Spring Cloud, the default RPC method is HTTP. Therefore, for the provider, there is no need to differentiate the request interception method. It only needs to extract the XID from the header and bind it to its own transaction context. However, for the consumer, due to the variety of request components, including circuit breakers and isolation mechanisms, different situations need to be distinguished and handled. We will analyze this in more detail later.\\n\\n#### Microservice Consumer\\n\\nFescar categorizes the request methods into RestTemplate, Feign, Feign+Hystrix, and Feign+Sentinel. Different components are automatically configured through Spring Boot\'s Auto Configuration. The specific configuration classes can be found in the spring.factories file, and we will also discuss the relevant configuration classes later in this document.\\n\\n##### RestTemplate\\n\\nLet\'s take a look at how Fescar passes XID if the consumer is using RestTemplate for requests.\\n\\n```java\\npublic class FescarRestTemplateInterceptor implements ClientHttpRequestInterceptor {\\n\\t@Override\\n\\tpublic ClientHttpResponse intercept(HttpRequest httpRequest, byte[] bytes,\\n\\t\\t\\tClientHttpRequestExecution clientHttpRequestExecution) throws IOException {\\n\\t\\tHttpRequestWrapper requestWrapper = new HttpRequestWrapper(httpRequest);\\n\\n\\t\\tString xid = RootContext.getXID();\\n\\n\\t\\tif (!StringUtils.isEmpty(xid)) {\\n\\t\\t\\trequestWrapper.getHeaders().add(RootContext.KEY_XID, xid);\\n\\t\\t}\\n\\t\\treturn clientHttpRequestExecution.execute(requestWrapper, bytes);\\n\\t}\\n}\\n```\\n\\nThe FescarRestTemplateInterceptor implements the intercept method of the ClientHttpRequestInterceptor interface. It wraps the outgoing request and, if there is an existing Fescar transaction context XID, retrieves it and adds it to the HTTP headers of the request.\\n\\nFescarRestTemplateInterceptor is configured in RestTemplate through FescarRestTemplateAutoConfiguration.\\n\\n```java\\n@Configuration\\npublic class FescarRestTemplateAutoConfiguration {\\n\\n\\t@Bean\\n\\tpublic FescarRestTemplateInterceptor fescarRestTemplateInterceptor() {\\n\\t\\treturn new FescarRestTemplateInterceptor();\\n\\t}\\n\\n\\t@Autowired(required = false)\\n\\tprivate Collection<RestTemplate> restTemplates;\\n\\n\\t@Autowired\\n\\tprivate FescarRestTemplateInterceptor fescarRestTemplateInterceptor;\\n\\n\\t@PostConstruct\\n\\tpublic void init() {\\n\\t\\tif (this.restTemplates != null) {\\n\\t\\t\\tfor (RestTemplate restTemplate : restTemplates) {\\n\\t\\t\\t\\tList<ClientHttpRequestInterceptor> interceptors = new ArrayList<ClientHttpRequestInterceptor>(\\n\\t\\t\\t\\t\\t\\trestTemplate.getInterceptors());\\n\\t\\t\\t\\tinterceptors.add(this.fescarRestTemplateInterceptor);\\n\\t\\t\\t\\trestTemplate.setInterceptors(interceptors);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n}\\n```\\n\\nThe init method iterates through all the RestTemplate instances, retrieves the original interceptors from each RestTemplate, adds the fescarRestTemplateInterceptor, and then reorders the interceptors.\\n\\n##### Feign\\n\\n![Feign \u7c7b\u5173\u7cfb\u56fe](/img/blog/20190305184812.png)\\n\\nNext, let\'s take a look at the code related to Feign. There are quite a few classes in this package, so let\'s start with its AutoConfiguration.\\n\\n```java\\n@Configuration\\n@ConditionalOnClass(Client.class)\\n@AutoConfigureBefore(FeignAutoConfiguration.class)\\npublic class FescarFeignClientAutoConfiguration {\\n\\n\\t@Bean\\n\\t@Scope(\\"prototype\\")\\n\\t@ConditionalOnClass(name = \\"com.netflix.hystrix.HystrixCommand\\")\\n\\t@ConditionalOnProperty(name = \\"feign.hystrix.enabled\\", havingValue = \\"true\\")\\n\\tFeign.Builder feignHystrixBuilder(BeanFactory beanFactory) {\\n\\t\\treturn FescarHystrixFeignBuilder.builder(beanFactory);\\n\\t}\\n\\n\\t@Bean\\n\\t@Scope(\\"prototype\\")\\n\\t@ConditionalOnClass(name = \\"com.alibaba.csp.sentinel.SphU\\")\\n\\t@ConditionalOnProperty(name = \\"feign.sentinel.enabled\\", havingValue = \\"true\\")\\n\\tFeign.Builder feignSentinelBuilder(BeanFactory beanFactory) {\\n\\t\\treturn FescarSentinelFeignBuilder.builder(beanFactory);\\n\\t}\\n\\n\\t@Bean\\n\\t@ConditionalOnMissingBean\\n\\t@Scope(\\"prototype\\")\\n\\tFeign.Builder feignBuilder(BeanFactory beanFactory) {\\n\\t\\treturn FescarFeignBuilder.builder(beanFactory);\\n\\t}\\n\\n\\t@Configuration\\n\\tprotected static class FeignBeanPostProcessorConfiguration {\\n\\n\\t\\t@Bean\\n\\t\\tFescarBeanPostProcessor fescarBeanPostProcessor(\\n\\t\\t\\t\\tFescarFeignObjectWrapper fescarFeignObjectWrapper) {\\n\\t\\t\\treturn new FescarBeanPostProcessor(fescarFeignObjectWrapper);\\n\\t\\t}\\n\\n\\t\\t@Bean\\n\\t\\tFescarContextBeanPostProcessor fescarContextBeanPostProcessor(\\n\\t\\t\\t\\tBeanFactory beanFactory) {\\n\\t\\t\\treturn new FescarContextBeanPostProcessor(beanFactory);\\n\\t\\t}\\n\\n\\t\\t@Bean\\n\\t\\tFescarFeignObjectWrapper fescarFeignObjectWrapper(BeanFactory beanFactory) {\\n\\t\\t\\treturn new FescarFeignObjectWrapper(beanFactory);\\n\\t\\t}\\n\\t}\\n\\n}\\n```\\n\\nThe FescarFeignClientAutoConfiguration is enabled when the Client.class exists and requires it to be applied before FeignAutoConfiguration. Since FeignClientsConfiguration is responsible for generating the FeignContext and is enabled by FeignAutoConfiguration, based on the dependency relationship, FescarFeignClientAutoConfiguration is also applied before FeignClientsConfiguration.\\n\\nFescarFeignClientAutoConfiguration customizes the Feign.Builder and adapts it for feign.sentinel, feign.hystrix, and regular feign cases. The purpose is to customize the actual implementation of the Client in Feign to be FescarFeignClient.\\n\\n```java\\nHystrixFeign.builder().retryer(Retryer.NEVER_RETRY)\\n      .client(new FescarFeignClient(beanFactory))\\n```\\n\\n```java\\nSentinelFeign.builder().retryer(Retryer.NEVER_RETRY)\\n\\t\\t\\t\\t.client(new FescarFeignClient(beanFactory));\\n```\\n\\n```java\\nFeign.builder().client(new FescarFeignClient(beanFactory));\\n```\\n\\nFescarFeignClient is an enhancement of the original Feign client proxy.\\n\\n```java\\npublic class FescarFeignClient implements Client {\\n\\n\\tprivate final Client delegate;\\n\\tprivate final BeanFactory beanFactory;\\n\\n\\tFescarFeignClient(BeanFactory beanFactory) {\\n\\t\\tthis.beanFactory = beanFactory;\\n\\t\\tthis.delegate = new Client.Default(null, null);\\n\\t}\\n\\n\\tFescarFeignClient(BeanFactory beanFactory, Client delegate) {\\n\\t\\tthis.delegate = delegate;\\n\\t\\tthis.beanFactory = beanFactory;\\n\\t}\\n\\n\\t@Override\\n\\tpublic Response execute(Request request, Request.Options options) throws IOException {\\n\\n\\t\\tRequest modifiedRequest = getModifyRequest(request);\\n\\n\\t\\ttry {\\n\\t\\t\\treturn this.delegate.execute(modifiedRequest, options);\\n\\t\\t}\\n\\t\\tfinally {\\n\\n\\t\\t}\\n\\t}\\n\\n\\tprivate Request getModifyRequest(Request request) {\\n\\n\\t\\tString xid = RootContext.getXID();\\n\\n\\t\\tif (StringUtils.isEmpty(xid)) {\\n\\t\\t\\treturn request;\\n\\t\\t}\\n\\n\\t\\tMap<String, Collection<String>> headers = new HashMap<>();\\n\\t\\theaders.putAll(request.headers());\\n\\n\\t\\tList<String> fescarXid = new ArrayList<>();\\n\\t\\tfescarXid.add(xid);\\n\\t\\theaders.put(RootContext.KEY_XID, fescarXid);\\n\\n\\t\\treturn Request.create(request.method(), request.url(), headers, request.body(),\\n\\t\\t\\t\\trequest.charset());\\n\\t}\\n\\n```\\n\\nIn the above process, we can see that FescarFeignClient modifies the original Request. It first retrieves the XID from the current transaction context and, if the XID is not empty, adds it to the request\'s header.\\n\\nFeignBeanPostProcessorConfiguration defines three beans: FescarContextBeanPostProcessor, FescarBeanPostProcessor, and FescarFeignObjectWrapper. FescarContextBeanPostProcessor and FescarBeanPostProcessor both implement the Spring BeanPostProcessor interface.\\n\\nHere is the implementation of FescarContextBeanPostProcessor\\n\\n```java\\n    @Override\\n\\tpublic Object postProcessBeforeInitialization(Object bean, String beanName)\\n\\t\\t\\tthrows BeansException {\\n\\t\\tif (bean instanceof FeignContext && !(bean instanceof FescarFeignContext)) {\\n\\t\\t\\treturn new FescarFeignContext(getFescarFeignObjectWrapper(),\\n\\t\\t\\t\\t\\t(FeignContext) bean);\\n\\t\\t}\\n\\t\\treturn bean;\\n\\t}\\n\\n\\t@Override\\n\\tpublic Object postProcessAfterInitialization(Object bean, String beanName)\\n\\t\\t\\tthrows BeansException {\\n\\t\\treturn bean;\\n\\t}\\n```\\n\\nThe two methods in BeanPostProcessor allow for pre- and post-processing of beans in the Spring container. The postProcessBeforeInitialization method is called before initialization, while the postProcessAfterInitialization method is called after initialization. The return value of these methods can be the original bean instance or a wrapped instance using a wrapper.\\n\\nFescarContextBeanPostProcessor wraps FeignContext into FescarFeignContext. FescarBeanPostProcessor wraps FeignClient into FescarLoadBalancerFeignClient and FescarFeignClient, depending on whether it inherits from LoadBalancerFeignClient.\\n\\nIn FeignAutoConfiguration, the FeignContext does not have any ConditionalOnXXX conditions. Therefore, Fescar uses a pre-processing approach to wrap FeignContext into FescarFeignContext.\\n\\n```java\\n    @Bean\\n\\tpublic FeignContext feignContext() {\\n\\t\\tFeignContext context = new FeignContext();\\n\\t\\tcontext.setConfigurations(this.configurations);\\n\\t\\treturn context;\\n\\t}\\n```\\n\\nFor Feign Clients, the FeignClientFactoryBean retrieves an instance of FeignContext. For custom Feign Client objects configured by developers using the @Configuration annotation, they are configured into the builder, which causes the enhanced FescarFeignClient in FescarFeignBuilder to become ineffective. The key code in FeignClientFactoryBean is as follows\\n\\n```java\\n\\t/**\\n\\t * @param <T> the target type of the Feign client\\n\\t * @return a {@link Feign} client created with the specified data and the context information\\n\\t */\\n\\t<T> T getTarget() {\\n\\t\\tFeignContext context = applicationContext.getBean(FeignContext.class);\\n\\t\\tFeign.Builder builder = feign(context);\\n\\n\\t\\tif (!StringUtils.hasText(this.url)) {\\n\\t\\t\\tif (!this.name.startsWith(\\"http\\")) {\\n\\t\\t\\t\\turl = \\"http://\\" + this.name;\\n\\t\\t\\t}\\n\\t\\t\\telse {\\n\\t\\t\\t\\turl = this.name;\\n\\t\\t\\t}\\n\\t\\t\\turl += cleanPath();\\n\\t\\t\\treturn (T) loadBalance(builder, context, new HardCodedTarget<>(this.type,\\n\\t\\t\\t\\t\\tthis.name, url));\\n\\t\\t}\\n\\t\\tif (StringUtils.hasText(this.url) && !this.url.startsWith(\\"http\\")) {\\n\\t\\t\\tthis.url = \\"http://\\" + this.url;\\n\\t\\t}\\n\\t\\tString url = this.url + cleanPath();\\n\\t\\tClient client = getOptional(context, Client.class);\\n\\t\\tif (client != null) {\\n\\t\\t\\tif (client instanceof LoadBalancerFeignClient) {\\n\\t\\t\\t\\t// not load balancing because we have a url,\\n\\t\\t\\t\\t// but ribbon is on the classpath, so unwrap\\n\\t\\t\\t\\tclient = ((LoadBalancerFeignClient)client).getDelegate();\\n\\t\\t\\t}\\n\\t\\t\\tbuilder.client(client);\\n\\t\\t}\\n\\t\\tTargeter targeter = get(context, Targeter.class);\\n\\t\\treturn (T) targeter.target(this, builder, context, new HardCodedTarget<>(\\n\\t\\t\\t\\tthis.type, this.name, url));\\n\\t}\\n```\\n\\nThe above code determines whether to make a direct call to the specified URL or use load balancing based on whether the URL parameter is specified in the annotation. The targeter.target method creates the object through dynamic proxy. The general process is as follows: the parsed Feign methods are stored in a map, and then passed as a parameter to generate the InvocationHandler, which in turn generates the dynamic proxy object.\\n\\nThe presence of FescarContextBeanPostProcessor ensures that even if developers customize operations on FeignClient, the enhancement of global transactions required by Fescar can still be achieved.\\n\\nAs for FescarFeignObjectWrapper, let\'s focus on the Wrapper method:\\n\\n```java\\n\\tObject wrap(Object bean) {\\n\\t\\tif (bean instanceof Client && !(bean instanceof FescarFeignClient)) {\\n\\t\\t\\tif (bean instanceof LoadBalancerFeignClient) {\\n\\t\\t\\t\\tLoadBalancerFeignClient client = ((LoadBalancerFeignClient) bean);\\n\\t\\t\\t\\treturn new FescarLoadBalancerFeignClient(client.getDelegate(), factory(),\\n\\t\\t\\t\\t\\t\\tclientFactory(), this.beanFactory);\\n\\t\\t\\t}\\n\\t\\t\\treturn new FescarFeignClient(this.beanFactory, (Client) bean);\\n\\t\\t}\\n\\t\\treturn bean;\\n\\t}\\n```\\n\\nIn the wrap method, if the bean is an instance of LoadBalancerFeignClient, it first retrieves the actual Client object that the LoadBalancerFeignClient proxies using the client.getDelegate() method. It then wraps the Client object into FescarFeignClient and generates a subclass of LoadBalancerFeignClient called FescarLoadBalancerFeignClient. If the bean is an instance of Client and not FescarFeignClient or LoadBalancerFeignClient, it is directly wrapped and transformed into FescarFeignClient.\\n\\nThe above process design is quite clever. It controls the order of configuration based on Spring Boot\'s Auto Configuration and customizes the Feign Builder bean to ensure that all Clients are enhanced with FescarFeignClient. It also wraps the beans in the Spring container using BeanPostProcessor, ensuring that all beans in the container are enhanced with FescarFeignClient, thus avoiding the replacement action in the getTarget method of FeignClientFactoryBean.\\n\\n##### Hystrix Isolation\\n\\nNow let\'s take a look at the Hystrix part. Why do we separate Hystrix and implement a separate strategy class in Fescar? Currently, the default implementation of the transaction context RootContext is based on ThreadLocal, which means the context is bound to the thread. Hystrix itself has two isolation modes: semaphore-based isolation and thread pool-based isolation. Hystrix officially recommends using thread pool isolation for better separation, which is the commonly used mode:\\n\\n```\\nThread or Semaphore\\nThe default, and the recommended setting, is to run HystrixCommands using thread isolation (THREAD) and HystrixObservableCommands using semaphore isolation (SEMAPHORE).\\n\\nCommands executed in threads have an extra layer of protection against latencies beyond what network timeouts can offer.\\n\\nGenerally the only time you should use semaphore isolation for HystrixCommands is when the call is so high volume (hundreds per second, per instance) that the overhead of separate threads is too high; this typically only applies to non-network calls.\\n```\\n\\nYou are correct that the service layer\'s business code and the thread that sends the request are not the same. Therefore, the ThreadLocal approach cannot pass the XID to the Hystrix thread and subsequently to the callee. To address this issue, Hystrix provides a mechanism for developers to customize the concurrency strategy. This can be done by extending the HystrixConcurrencyStrategy class and overriding the wrapCallable method:\\n\\n```java\\npublic class FescarHystrixConcurrencyStrategy extends HystrixConcurrencyStrategy {\\n\\n\\tprivate HystrixConcurrencyStrategy delegate;\\n\\n\\tpublic FescarHystrixConcurrencyStrategy() {\\n\\t\\tthis.delegate = HystrixPlugins.getInstance().getConcurrencyStrategy();\\n\\t\\tHystrixPlugins.reset();\\n\\t\\tHystrixPlugins.getInstance().registerConcurrencyStrategy(this);\\n\\t}\\n\\n\\t@Override\\n\\tpublic <K> Callable<K> wrapCallable(Callable<K> c) {\\n\\t\\tif (c instanceof FescarContextCallable) {\\n\\t\\t\\treturn c;\\n\\t\\t}\\n\\n\\t\\tCallable<K> wrappedCallable;\\n\\t\\tif (this.delegate != null) {\\n\\t\\t\\twrappedCallable = this.delegate.wrapCallable(c);\\n\\t\\t}\\n\\t\\telse {\\n\\t\\t\\twrappedCallable = c;\\n\\t\\t}\\n\\t\\tif (wrappedCallable instanceof FescarContextCallable) {\\n\\t\\t\\treturn wrappedCallable;\\n\\t\\t}\\n\\n\\t\\treturn new FescarContextCallable<>(wrappedCallable);\\n\\t}\\n\\n\\tprivate static class FescarContextCallable<K> implements Callable<K> {\\n\\n\\t\\tprivate final Callable<K> actual;\\n\\t\\tprivate final String xid;\\n\\n\\t\\tFescarContextCallable(Callable<K> actual) {\\n\\t\\t\\tthis.actual = actual;\\n\\t\\t\\tthis.xid = RootContext.getXID();\\n\\t\\t}\\n\\n\\t\\t@Override\\n\\t\\tpublic K call() throws Exception {\\n\\t\\t\\ttry {\\n\\t\\t\\t\\tRootContext.bind(xid);\\n\\t\\t\\t\\treturn actual.call();\\n\\t\\t\\t}\\n\\t\\t\\tfinally {\\n\\t\\t\\t\\tRootContext.unbind();\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t}\\n}\\n```\\n\\nFescar also provides a FescarHystrixAutoConfiguration, which generates the FescarHystrixConcurrencyStrategy when HystrixCommand is present.\\n\\n```java\\n@Configuration\\n@ConditionalOnClass(HystrixCommand.class)\\npublic class FescarHystrixAutoConfiguration {\\n\\n\\t@Bean\\n\\tFescarHystrixConcurrencyStrategy fescarHystrixConcurrencyStrategy() {\\n\\t\\treturn new FescarHystrixConcurrencyStrategy();\\n\\t}\\n\\n}\\n```\\n\\n### reference\\n\\n- Fescar: https://github.com/alibaba/fescar\\n\\n- Spring Cloud Alibaba: https://github.com/spring-cloud-incubator/spring-cloud-alibaba\\n\\n- spring-cloud-openfeign: https://github.com/spring-cloud/spring-cloud-openfeign\\n\\n### author\\n\\nkangshu.guo\uff0cCommunity nickname ywind, formerly employed at Huawei Terminal Cloud, currently a Java engineer at Sohu Intelligent Media Center. Mainly responsible for development related to Sohu accounts. Has a strong interest in distributed transactions, distributed systems, and microservices architecture.\\nmin.ji(qinming)\uff0cCommunity nickname slievrly, Fescar project leader, core developer of Alibaba middleware TXC/GTS. Engaged in core research and development work in distributed middleware for a long time. Has extensive technical expertise in the field of distributed transactions."},{"id":"/integrate-seata-with-spring-cloud","metadata":{"permalink":"/blog/integrate-seata-with-spring-cloud","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/integrate-seata-with-spring-cloud.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/integrate-seata-with-spring-cloud.md","title":"Integrating Seata (formerly Fescar) Distributed Transaction with Spring Cloud","description":"Many developers are already familiar with Fescar. However, Fescar has now transformed into Seata. If you\'re not aware of Seata, please check the following link.","date":"2019-04-15T00:00:00.000Z","formattedDate":"April 15, 2019","tags":[],"readingTime":5.995,"hasTruncateMarker":false,"authors":[{"name":"dafei.Fei"}],"frontMatter":{"title":"Integrating Seata (formerly Fescar) Distributed Transaction with Spring Cloud","author":"dafei.Fei","date":"2019/04/15","keywords":["fescar","seata","distributed transaction"]},"unlisted":false,"prevItem":{"title":"Fescar Integration with Spring Cloud In-Depth Analysis of Source Code","permalink":"/blog/how-to-support-spring-cloud"},"nextItem":{"title":"Detailed Explanation of Seata-Client Principles and Processes in Distributed Transactions","permalink":"/blog/seata-analysis-java-client"}},"content":"Many developers are already familiar with Fescar. However, Fescar has now transformed into Seata. If you\'re not aware of Seata, please check the following link.\\n\\nSEATA GITHUB: [https://github.com/apache/incubator-seata]\\n\\nWe extend our sincere thanks and greetings to the Alibaba team for their contributions in bringing numerous open-source software to developers.\\n\\nToday, I will share my insights on integrating Seata with Spring Cloud, aiming to help more developers avoid common pitfalls and streamline their setup process.\\n\\n# 2. Project Overview\\n\\nThe setup process is as follows: client -> gateway -> service consumer -> service provider.\\n\\n```\\nTechnical Framework: spring cloud gateway\\nspring cloud fegin\\nnacos1.0.RC2\\nfescar-server0.4.1 (Seata)\\n```\\n\\nFor instructions on starting Nacos, please refer to: [Nacos Startup Guide](https://nacos.io/zh-cn/docs/quick-start.html)\\n\\nSeata supports various service registration methods. In the `fescar-server-0.4.1\\\\conf` directory, you will find:\\n\\n```\\nfile.conf\\nlogback.xml\\nnacos-config.sh\\nnacos-config.text\\nregistry.conf\\n```\\n\\nThere are a total of five files. Among them, `file.conf` and `registry.conf` are needed in the code segments for both service consumers and providers. Note: `file.conf` and `registry.conf` must be included in the current applications in use, i.e., both service consumer and provider applications must include them. If you are using a configuration center like Nacos or ZK, `file.cnf` can be ignored. However, if `type=\\"file\\"` is specified, then `file.cnf` must be used.\\n\\nBelow is the configuration information in the `registry.conf` file. The `registry` section is for the service registration center configuration, and the `config` section is for the configuration center.\\n\\nAs shown below, Seata currently supports nacos, file, eureka, redis, zookeeper, etc., for registration and configuration. The default downloaded configuration type is `file`. The choice of method depends on your project\u2019s actual requirements. Here, I chose nacos, but eureka can also be used. Both versions have been tested and work fine.\\n\\nNote: If you are integrating with eureka, please use the latest official version.\\n\\n# 3. Core Configuration\\n\\n```java\\nregistry {\\n  # file, nacos, eureka, redis, zk\\n  type = \\"nacos\\"\\n\\n  nacos {\\n    serverAddr = \\"localhost\\"\\n    namespace = \\"public\\"\\n    cluster = \\"default\\"\\n  }\\n  eureka {\\n    serviceUrl = \\"http://localhost:1001/eureka\\"\\n    application = \\"default\\"\\n    weight = \\"1\\"\\n  }\\n  redis {\\n    serverAddr = \\"localhost:6379\\"\\n    db = \\"0\\"\\n  }\\n  zk {\\n    cluster = \\"default\\"\\n    serverAddr = \\"127.0.0.1:2181\\"\\n    session.timeout = 6000\\n    connect.timeout = 2000\\n  }\\n  file {\\n    name = \\"file.conf\\"\\n  }\\n}\\n\\nconfig {\\n  # file, nacos, apollo, zk\\n  type = \\"nacos\\"\\n\\n  nacos {\\n    serverAddr = \\"localhost\\"\\n    namespace = \\"public\\"\\n    cluster = \\"default\\"\\n  }\\n  apollo {\\n    app.id = \\"fescar-server\\"\\n    apollo.meta = \\"http://192.168.1.204:8801\\"\\n  }\\n  zk {\\n    serverAddr = \\"127.0.0.1:2181\\"\\n    session.timeout = 6000\\n    connect.timeout = 2000\\n  }\\n  file {\\n    name = \\"file.conf\\"\\n  }\\n}\\n```\\n\\nNote that `nacos-config.sh` is a script that needs to be executed if using Nacos as the configuration center. It initializes some default settings for Nacos.\\n\\nRefer to the official guide for SEATA startup: Note that the official startup command separates parameters with spaces, so be careful. The IP is an optional parameter. Due to DNS resolution, sometimes when registering with Nacos, Fescar might obtain the address using the computer name, requiring you to specify the IP or configure the host to point to the IP. This issue has been fixed in the latest SEATA version.\\n\\n```shell\\nsh fescar-server.sh 8091 /home/admin/fescar/data/ IP (optional)\\n```\\n\\nAs mentioned earlier, `file.conf` and `registry.conf` are needed in our code. The focus here is on `file.conf`. It is only loaded if `registry` is configured with `file`. If using ZK, Nacos, or other configuration centers, it can be ignored. However, `service.localRgroup.grouplist` and `service.vgroupMapping` need to be specified in the configuration center so that your client can automatically obtain the corresponding SEATA service and address from the configuration center upon startup. Failure to configure this will result in an error due to the inability to connect to the server. If using eureka, the config section should specify `type=\\"file\\"`. SEATA config currently does not support eureka.\\n\\n```java\\ntransport {\\n  # tcp, udt, unix-domain-socket\\n  type = \\"TCP\\"\\n  # NIO, NATIVE\\n  server = \\"NIO\\"\\n  # enable heartbeat\\n  heartbeat = true\\n  # thread factory for netty\\n  thread-factory {\\n    boss-thread-prefix = \\"NettyBoss\\"\\n    worker-thread-prefix = \\"NettyServerNIOWorker\\"\\n    server-executor-thread-prefix = \\"NettyServerBizHandler\\"\\n    share-boss-worker = false\\n    client-selector-thread-prefix = \\"NettyClientSelector\\"\\n    client-selector-thread-size = 1\\n    client-worker-thread-prefix = \\"NettyClientWorkerThread\\"\\n    # netty boss thread size, will not be used for UDT\\n    boss-thread-size = 1\\n    # auto default pin or 8\\n    worker-thread-size = 8\\n  }\\n}\\nservice {\\n  # vgroup -> rgroup\\n  vgroup_mapping.service-provider-fescar-service-group = \\"default\\"\\n  # only support single node\\n  localRgroup.grouplist = \\"127.0.0.1:8091\\"\\n  # degrade current not support\\n  enableDegrade = false\\n  # disable\\n  disable = false\\n}\\n\\nclient {\\n  async.commit.buffer.limit = 10000\\n  lock {\\n    retry.internal = 10\\n    retry.times = 30\\n  }\\n}\\n```\\n\\n# 4. Service Details\\nTwo key points need attention:\\n\\n```java\\ngrouplist IP: This is the IP and port of the current Fescar server.\\nvgroup_mapping configuration.\\n```\\n\\n`vgroup_mapping.service-provider-fescar-service-group`: The service name here is actually the application name configured in the `application.properties` of your consumer or provider, e.g., `spring.application.name=service-provider`. In the source code, the application name is concatenated with `fescar-service-group` to form the key. Similarly, the value is the name of the current Fescar service. `cluster = \\"default\\" / application = \\"default\\"`\\n\\n```java\\nvgroup_mapping.service-provider-fescar-service-group = \\"default\\"\\n# only support single node\\nlocalRgroup.grouplist = \\"127.0.0.1:8091\\"\\n```\\n\\nBoth provider and consumer need to configure these two files.\\n\\nIf you use Nacos as the configuration center, you need to add the configuration in Nacos by adding the configuration manually.\\n\\n# 5. Transaction Usage\\nIn my code, the request is load-balanced through the gateway to the consumer. The consumer then requests the provider through Feign. The official example uses Feign, but here, the request is forwarded directly through the gateway, so the global transaction is handled in the controller layer, similar to the official demo.\\n\\n```java\\n@RestController\\npublic class DemoController {\\n\\t@Autowired\\n\\tprivate DemoFeignClient demoFeignClient;\\n\\n\\t@Autowired\\n\\tprivate DemoFeignClient2 demoFeignClient2;\\n\\t@GlobalTransactional(timeoutMills = 300000, name = \\"spring-cloud-demo-tx\\")\\n\\t@GetMapping(\\"/getdemo\\")\\n\\tpublic String demo() {\\n\\n\\t\\t// Call service A and simply save\\n\\t\\tResponseData<Integer> result = demoFeignClient.insertService(\\"test\\", 1);\\n\\t\\tif(result.getStatus() == 400) {\\n\\t\\t\\tSystem.out.println(result + \\"+++++++++++++++++++++++++++++++++++++++\\");\\n\\t\\t\\tthrow new RuntimeException(\\"this is error1\\");\\n\\t\\t}\\n\\n\\t\\t// Call service B and test rollback of service A upon error\\n\\t\\tResponseData<Integer> result2 = demoFeignClient2.saveService();\\n\\n\\t\\tif(result2.getStatus() == 400) {\\n\\t\\t\\tSystem.out.println(result2 + \\"+++++++++++++++++++++++++++++++++++++++\\");\\n\\t\\t\\tthrow new RuntimeException(\\"this is error2\\");\\n\\t\\t}\\n\\n\\t\\treturn \\"SUCCESS\\";\\n\\t}\\n}\\n```\\n\\nThis concludes the core integration of transactions. Here, service A and B are both providers. When service B encounters an error, the global transaction rolls back. Each transaction can handle its local transactions independently.\\n\\nSEATA uses a global XID to uniformly identify transactions. I will not list the database tables needed for SEATA here. For details, refer to: [spring-cloud-fescar official DEMO](https://github.com/spring-cloud-incubator/spring-cloud-alibaba/tree/master/spring-cloud-alibaba-examples/fescar-example)\\n\\n# 5.Data Proxy\\n\\nAnother important point is that in a distributed database service, each database needs an `undo_log` table to handle XID storage.\\n\\nAdditionally, each service project needs a database connection pool proxy. Currently, only Druid connection pool is supported. More will be supported in the future.\\n\\n```java\\n@Configuration\\npublic class DatabaseConfiguration {\\n\\n\\t@Bean(destroyMethod = \\"close\\", initMethod = \\"init\\")\\n\\t@ConfigurationProperties(prefix=\\"spring.datasource\\")\\n\\tpublic DruidDataSource druidDataSource() {\\n\\t\\treturn new DruidDataSource();\\n\\t}\\n\\n\\t@Bean\\n\\tpublic DataSourceProxy dataSourceProxy(DruidDataSource druidDataSource) {\\n\\t\\treturn new DataSourceProxy(druidDataSource);\\n\\t}\\n\\n    @Bean\\n    public SqlSessionFactory sqlSessionFactory(DataSourceProxy dataSourceProxy) throws Exception {\\n        SqlSessionFactoryBean factoryBean = new SqlSessionFactoryBean();\\n        factoryBean.setDataSource(dataSourceProxy);\\n        return factoryBean.getObject();\\n    }\\n}\\n```\\n\\nPay attention to the configuration file and data proxy. Without a data source proxy, `undo_log` will have no data, making XID management impossible.\\n\\nAuthor: Da Fei"},{"id":"/seata-analysis-java-client","metadata":{"permalink":"/blog/seata-analysis-java-client","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-java-client.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-java-client.md","title":"Detailed Explanation of Seata-Client Principles and Processes in Distributed Transactions","description":"Preface","date":"2019-04-15T00:00:00.000Z","formattedDate":"April 15, 2019","tags":[],"readingTime":21.72,"hasTruncateMarker":false,"authors":[{"name":"fangliangsheng"}],"frontMatter":{"title":"Detailed Explanation of Seata-Client Principles and Processes in Distributed Transactions","author":"fangliangsheng","date":"2019/04/15","keywords":["fescar","seata","distributed transaction"]},"unlisted":false,"prevItem":{"title":"Integrating Seata (formerly Fescar) Distributed Transaction with Spring Cloud","permalink":"/blog/integrate-seata-with-spring-cloud"},"nextItem":{"title":"In-Depth Analysis of One-Stop Distributed Transaction Solution Seata-Server","permalink":"/blog/seata-analysis-java-server"}},"content":"## Preface\\n\\nIn distributed systems, distributed transactions are a problem that must be solved. Currently, the most commonly used solution is eventual consistency. Since Alibaba open-sourced Fescar (renamed Seata in early April) earlier this year, the project has received great attention, currently nearing 8000 stars. [Seata](https://github.com/apache/incubator-seata) aims to solve the problem of distributed transactions in the microservices field with high performance and zero intrusion. It is currently undergoing rapid iteration, with a near-term goal of producing a production-ready MySQL version. For a comprehensive introduction to Seata, you can check the [official WIKI](https://github.com/apache/incubator-seata/wiki/%E6%A6%82%E8%A7%88) for more detailed information.\\n\\nThis article is mainly based on the structure of spring cloud+spring jpa+spring cloud alibaba fescar+mysql+seata, building a distributed system demo, and analyzing and explaining its working process and principles from the perspective of the client (RM, TM) through seata\'s debug logs and source code.\\nThe code in the article is based on fescar-0.4.1. Since the project was just renamed to seata not long ago, some package names, class names, and jar package names are still named fescar, so the term fescar is still used in the following text. Sample project: [https://github.com/fescar-group/fescar-samples/tree/master/springcloud-jpa-seata](https://github.com/fescar-group/fescar-samples/tree/master/springcloud-jpa-seata)\\n\\n## Related Concepts\\n\\n- **XID**: The unique identifier of a global transaction, composed of ip:port:sequence\\n- **Transaction Coordinator (TC)**: Maintains the running state of global transactions, responsible for coordinating and driving the commit or rollback of global transactions\\n- **Transaction Manager (TM)**: Controls the boundary of global transactions, responsible for starting a global transaction and finally initiating the decision of global commit or rollback\\n- **Resource Manager (RM)**: Controls branch transactions, responsible for branch registration, status reporting, and receiving instructions from the transaction coordinator to drive the commit and rollback of branch (local) transactions\\n\\n## Distributed Framework Support\\n\\nFescar uses XID to represent a distributed transaction. XID needs to be transmitted in the systems involved in a distributed transaction request, to send the processing status of branch transactions to the feacar-server, and to receive commit and rollback instructions from the feacar-server. Fescar officially supports all versions of the dubbo protocol, and the community also provides corresponding implementations for spring cloud (spring-boot) distributed projects.\\n\\n```xml\\n<dependency>\\n    <groupId>org.springframework.cloud</groupId>\\n    <artifactId>spring-cloud-alibaba-fescar</artifactId>\\n    <version>2.1.0.BUILD-SNAPSHOT</version>\\n</dependency>\\n```\\n\\nThis component implements the XID transmission function based on RestTemplate and Feign communication.\\n\\n## Business Logic\\n\\nThe business logic is the classic process of placing an order, deducting the balance, and reducing inventory. According to the module division, it is divided into three independent services, each connected to the corresponding database:\\n\\n- Order: order-server\\n- Account: account-server\\n- Inventory: storage-server\\n\\nThere is also a business system that initiates distributed transactions:\\n\\n- Business: business-server\\n\\nThe project structure is as shown in the figure below:\\n![Insert image description here](/img/blog/20190410114411366.png)\\n\\n**Normal Business**\\n\\n1. The business initiates a purchase request\\n2. Storage deducts inventory\\n3. Order creates an order\\n4. Account deducts balance\\n\\n**Abnormal Business**\\n\\n1. The business initiates a purchase request\\n2. Storage deducts inventory\\n3. Order creates an order\\n4. Account `deduct balance exception`\\n\\nIn the normal process, the data of steps 2, 3, and 4 is normally updated globally commit. In the abnormal process, the data is globally rolled back due to the exception error in step 4.\\n\\n## Configuration Files\\n\\nThe configuration entry file for fescar is [registry.conf](https://github.com/apache/incubator-seata/blob/develop/config/src/main/resources/registry.conf). Check the code [ConfigurationFactory](https://github.com/apache/incubator-seata/blob/develop/config/src/main/java/com/alibaba/fescar/config/ConfigurationFactory.java) to find that currently, the configuration file name can only be registry.conf and cannot be specified otherwise.\\n\\n```java\\nprivate static final String REGISTRY_CONF = \\"registry.conf\\";\\npublic static final Configuration FILE_INSTANCE = new FileConfiguration(REGISTRY_CONF);\\n```\\n\\nIn `registry`, you can specify the specific configuration form, the default is to use the file type. In file.conf, there are 3 parts of the configuration content:\\n\\n1. **transport**\\n   The transport part configuration corresponds to the [NettyServerConfig](https://github.com/apache/incubator-seata/blob/develop/core/src/main/java/com/alibaba/fescar/core/rpc/netty/NettyServerConfig.java) class, used to define Netty-related parameters. TM and RM communicate with fescar-server using Netty.\\n\\n2. **service**\\n\\n```js\\nservice {\\n    #vgroup->rgroup\\n    vgroup_mapping.my_test_tx_group = \\"default\\"\\n    #Configure the address for the Client to connect to TC\\n    default.grouplist = \\"127.0.0.1:8091\\"\\n    #degrade current not support\\n    enableDegrade = false\\n    #disable\\n    Whether to enable seata distributed transaction\\n    disableGlobalTransaction = false\\n}\\n```\\n\\n3. **client**\\n\\n```js\\nclient {\\n    #RM receives the upper limit of buffer after TC\'s commit notification\\n    async.commit.buffer.limit = 10000\\n    lock {\\n      retry.internal = 10\\n      retry.times = 30\\n    }\\n}\\n```\\n\\n## Data Source Proxy\\n\\nIn addition to the previous configuration files, fescar in AT mode has a bit of code volume, which is for the proxy of the data source, and currently can only be based on the proxy of `DruidDataSource`. Note: In the latest release of version 0.4.2, it has supported any data source type.\\n\\n```java\\n@Bean\\n@ConfigurationProperties(prefix = \\"spring.datasource\\")\\npublic DruidDataSource druidDataSource() {\\n    DruidDataSource druidDataSource = new DruidDataSource();\\n    return druidDataSource;\\n}\\n@Primary\\n@Bean(\\"dataSource\\")\\npublic DataSourceProxy dataSource(DruidDataSource druidDataSource) {\\n    return new DataSourceProxy(druidDataSource);\\n}\\n```\\n\\nThe purpose of using `DataSourceProxy` is to introduce `ConnectionProxy`. One aspect of fescar\'s non-intrusiveness is reflected in the implementation of `ConnectionProxy`. The cut-in point for branch transactions to join global transactions is at the local transaction\'s `commit` stage. This design ensures that business data and `undo_log` are in a local transaction. The `undo_log` table needs to be created in the business library, and fescar depends on this table to record the status of each branch transaction and the rollback data of the second stage. There is no need to worry about the table\'s data volume becoming a single point problem. In the case of a global transaction commit, the transaction\'s corresponding `undo_log` will be asynchronously deleted.\\n\\n```sql\\nCREATE TABLE `undo_log` (\\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\\n  `branch_id` bigint(20) NOT NULL,\\n  `xid` varchar(100) NOT NULL,\\n  `rollback_info` longblob NOT NULL,\\n  `log_status` int(11) NOT NULL,\\n  `log_created` datetime NOT NULL,\\n  `log_modified` datetime NOT NULL,\\n  `ext` varchar(100) DEFAULT NULL,\\n  PRIMARY KEY (`id`),\\n  UNIQUE KEY `ux_undo_log` (`xid`, `branch_id`)\\n) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;\\n```\\n\\n## Start Server\\n\\nGo to [https://github.com/apache/incubator-seata/releases](https://github.com/apache/incubator-seata/releases) to download the fescar-server version corresponding to the Client version to avoid protocol inconsistencies caused by different versions. Enter the bin directory after decompression and execute\\n\\n```shell\\n./fescar-server.sh 8091 ../data\\n```\\n\\nIf the startup is successful, the following output will be shown\\n\\n```shell\\n2019-04-09 20:27:24.637 INFO [main] c.a.fescar.core.rpc.netty.AbstractRpcRemotingServer.start:152 -Server started ...\\n```\\n\\n## Start Client\\n\\nThe loading entry class of fescar is located in [GlobalTransactionAutoConfiguration](https://github.com/spring-cloud-incubator/spring-cloud-alibaba/blob/finchley/spring-cloud-alibaba-fescar/src/main/java/org/springframework/cloud/alibaba/fescar/GlobalTransactionAutoConfiguration.java), which can be automatically loaded for spring boot-based projects, and of course, it can also be instantiated by other means.\\n\\n```java\\n@Configuration\\n@EnableConfigurationProperties({FescarProperties.class})\\npublic class GlobalTransactionAutoConfiguration {\\n    private final ApplicationContext applicationContext;\\n    private final FescarProperties fescarProperties;\\n    public GlobalTransactionAutoConfiguration(ApplicationContext applicationContext, FescarProperties fescarProperties) {\\n\\n\\n        this.applicationContext = applicationContext;\\n        this.fescarProperties = fescarProperties;\\n    }\\n    /**\\n     * Instantiate GlobalTransactionScanner\\n     * scanner is the initialization initiator for the client\\n     */\\n    @Bean\\n    public GlobalTransactionScanner globalTransactionScanner() {\\n        String applicationName = this.applicationContext.getEnvironment().getProperty(\\"spring.application.name\\");\\n        String txServiceGroup = this.fescarProperties.getTxServiceGroup();\\n        if (StringUtils.isEmpty(txServiceGroup)) {\\n            txServiceGroup = applicationName + \\"-fescar-service-group\\";\\n            this.fescarProperties.setTxServiceGroup(txServiceGroup);\\n        }\\n        return new GlobalTransactionScanner(applicationName, txServiceGroup);\\n    }\\n}\\n```\\n\\nYou can see that it supports a configuration item FescarProperties, used to configure the transaction group name\\n\\n```json\\nspring.cloud.alibaba.fescar.tx-service-group=my_test_tx_group\\n```\\n\\nIf the service group is not specified, the default name generated is spring.application.name + \\"-fescar-service-group\\". Therefore, if spring.application.name is not specified, it will report an error when starting.\\n\\n```java\\n@ConfigurationProperties(\\"spring.cloud.alibaba.fescar\\")\\npublic class FescarProperties {\\n    private String txServiceGroup;\\n    public FescarProperties() {}\\n    public String getTxServiceGroup() {\\n        return this.txServiceGroup;\\n    }\\n    public void setTxServiceGroup(String txServiceGroup) {\\n        this.txServiceGroup = txServiceGroup;\\n    }\\n}\\n```\\n\\nAfter obtaining the applicationId and txServiceGroup, create a [GlobalTransactionScanner](https://github.com/apache/incubator-seata/blob/develop/spring/src/main/java/com/alibaba/fescar/spring/annotation/GlobalTransactionScanner.java) object, mainly seeing the initClient method in the class.\\n\\n```java\\nprivate void initClient() {\\n    if (StringUtils.isNullOrEmpty(applicationId) || StringUtils.isNullOrEmpty(txServiceGroup)) {\\n        throw new IllegalArgumentException(\\"applicationId: \\" + applicationId + \\" txServiceGroup: \\" + txServiceGroup);\\n    }\\n    // init TM\\n    TMClient.init(applicationId, txServiceGroup);\\n    // init RM\\n    RMClient.init(applicationId, txServiceGroup);\\n}\\n```\\n\\nIn the method, you can see that `TMClient` and `RMClient` are initialized. For a service, it can be both TM and RM roles. When it is TM or RM depends on whether the `@GlobalTransactional` annotation is marked in a global transaction. The result of the Client creation is a Netty connection with TC, so you can see two Netty Channels in the startup log, indicating that the transactionRole is `TMROLE` and `RMROLE`.\\n\\n```java\\n2019-04-09 13:42:57.417 INFO 93715 --- [imeoutChecker_1] c.a.f.c.rpc.netty.NettyPoolableFactory : NettyPool create channel to {\\"address\\":\\"127.0.0.1:8091\\", \\"message\\":{\\"applicationId\\":\\"business-service\\", \\"byteBuffer\\":{\\"char\\":\\"\\\\u0000\\",\\"direct\\":false,\\"double\\":0.0,\\"float\\":0.0,\\"int\\":0,\\"long\\":0,\\"readOnly\\":false,\\"short\\":0},\\"transactionServiceGroup\\":\\"my_test_tx_group\\",\\"typeCode\\":101,\\"version\\":\\"0.4.1\\"},\\"transactionRole\\":\\"TMROLE\\"}\\n2019-04-09 13:42:57.505 INFO 93715 --- [imeoutChecker_1] c.a.f.c.rpc.netty.NettyPoolableFactory : NettyPool create channel to {\\"address\\":\\"127.0.0.1:8091\\", \\"message\\":{\\"applicationId\\":\\"business-service\\", \\"byteBuffer\\":{\\"char\\":\\"\\\\u0000\\",\\"direct\\":false,\\"double\\":0.0,\\"float\\":0.0,\\"int\\":0,\\"long\\":0,\\"readOnly\\":false,\\"short\\":0},\\"transactionServiceGroup\\":\\"my_test_tx_group\\",\\"typeCode\\":103,\\"version\\":\\"0.4.1\\"},\\"transactionRole\\":\\"RMROLE\\"}\\n2019-04-09 13:42:57.629 DEBUG 93715 --- [lector_TMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Send:RegisterTMRequest{applicationId=\'business-service\', transactionServiceGroup=\'my_test_tx_group\'}\\n2019-04-09 13:42:57.629 DEBUG 93715 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Send:RegisterRMRequest{resourceIds=\'null\', applicationId=\'business-service\', transactionServiceGroup=\'my_test_tx_group\'}\\n2019-04-09 13:42:57.699 DEBUG 93715 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Receive:version=0.4.1,extraData=null,identified=true,resultCode=null,msg=null,messageId:1\\n2019-04-09 13:42:57.699 DEBUG 93715 --- [lector_TMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Receive:version=0.4.1,extraData=null,identified=true,resultCode=null,msg=null,messageId:2\\n2019-04-09 13:42:57.701 DEBUG 93715 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.AbstractRpcRemoting : com.alibaba.fescar.core.rpc.netty.RmRpcClient@3b06d101 msgId:1 future :com.alibaba.fescar.core.protocol.MessageFuture@28bb1abd body:version=0.4.1,extraData=null,identified=true,resultCode=null,msg=null\\n2019-04-09 13:42:57.701 DEBUG 93715 --- [lector_TMROLE_1] c.a.f.c.rpc.netty.AbstractRpcRemoting : com.alibaba.fescar.core.rpc.netty.TmRpcClient@65fc3fb7 msgId:2 future :com.alibaba.fescar.core.protocol.MessageFuture@9a1e3df body:version=0.4.1,extraData=null,identified=true,resultCode=null,msg=null\\n2019-04-09 13:42:57.710 INFO 93715 --- [imeoutChecker_1] c.a.fescar.core.rpc.netty.RmRpcClient : register RM success. server version:0.4.1 channel:[id: 0xe6468995 L:/127.0.0.1:57397 - R:/127.0.0.1:8091]\\n2019-04-09 13:42:57.710 INFO 93715 --- [imeoutChecker_1] c.a.f.c.rpc.netty.NettyPoolableFactory : register success cost 114 ms version:0.4.1 role:TMROLE channel:[id: 0xd22fe0c5 L:/127.0.0.1:57398 - R:/127.0.0.1:8091]\\n2019-04-09 13:42:57.711 INFO 93715 --- [imeoutChecker_1] c.a.f.c.rpc.netty.NettyPoolableFactory : register success cost 125 ms version:0.4.1 role:RMROLE channel:[id: 0xe6468995 L:/127.0.0.1:57397 - R:/127.0.0.1:8091]\\n```\\n\\nThe log shows\\n\\n1. Create Netty connection\\n2. Send registration request\\n3. Receive response result\\n4. `RmRpcClient`, `TmRpcClient` successfully instantiated\\n\\n## TM Process Flow\\n\\nIn this example, the TM role is business-service. The purchase method of BusinessService is marked with the `@GlobalTransactional` annotation.\\n\\n```java\\n@Service\\npublic class BusinessService {\\n    @Autowired\\n    private StorageFeignClient storageFeignClient;\\n    @Autowired\\n    private OrderFeignClient orderFeignClient;\\n    @GlobalTransactional\\n    public void purchase(String userId, String commodityCode, int orderCount) {\\n        storageFeignClient.deduct(commodityCode, orderCount);\\n        orderFeignClient.create(userId, commodityCode, orderCount);\\n    }\\n}\\n```\\n\\nAfter the method is called, a global transaction will be created. First, pay attention to the function of the `@GlobalTransactional` annotation, which is intercepted and processed in the [GlobalTransactionalInterceptor](https://github.com/apache/incubator-seata/blob/develop/spring/src/main/java/com/alibaba/fescar/spring/annotation/GlobalTransactionalInterceptor.java).\\n\\n```java\\n/**\\n * AOP intercepts method calls\\n */\\n@Override\\npublic Object invoke(final MethodInvocation methodInvocation) throws Throwable {\\n    Class<?> targetClass = (methodInvocation.getThis() != null ? AopUtils.getTargetClass(methodInvocation.getThis()) : null);\\n    Method specificMethod = ClassUtils.getMostSpecificMethod(methodInvocation.getMethod(), targetClass);\\n    final Method method = BridgeMethodResolver.findBridgedMethod(specificMethod);\\n    // Get the GlobalTransactional annotation of the method\\n    final GlobalTransactional globalTransactionalAnnotation = getAnnotation(method, GlobalTransactional.class);\\n    final GlobalLock globalLockAnnotation = getAnnotation(method, GlobalLock.class);\\n    // If the method has the GlobalTransactional annotation, intercept the corresponding method processing\\n    if (globalTransactionalAnnotation != null) {\\n        return handleGlobalTransaction(methodInvocation, globalTransactionalAnnotation);\\n    } else if (globalLockAnnotation != null) {\\n        return handleGlobalLock(methodInvocation);\\n    } else {\\n        return methodInvocation.proceed();\\n    }\\n}\\n```\\n\\nThe `handleGlobalTransaction` method calls the execute method of the [TransactionalTemplate](https://github.com/apache/incubator-seata/blob/develop/tm/src/main/java\\n\\n/com/alibaba/fescar/tm/api/TransactionalTemplate.java). As the class name suggests, this is a standard template method that defines the standard steps for TM to handle global transactions. The comments are already quite clear.\\n\\n```java\\npublic Object execute(TransactionalExecutor business) throws TransactionalExecutor.ExecutionException {\\n    // 1. get or create a transaction\\n    GlobalTransaction tx = GlobalTransactionContext.getCurrentOrCreate();\\n    try {\\n        // 2. begin transaction\\n        try {\\n            triggerBeforeBegin();\\n            tx.begin(business.timeout(), business.name());\\n            triggerAfterBegin();\\n        } catch (TransactionException txe) {\\n            throw new TransactionalExecutor.ExecutionException(tx, txe, TransactionalExecutor.Code.BeginFailure);\\n        }\\n        Object rs = null;\\n        try {\\n            // Do Your Business\\n            rs = business.execute();\\n        } catch (Throwable ex) {\\n            // 3. any business exception rollback.\\n            try {\\n                triggerBeforeRollback();\\n                tx.rollback();\\n                triggerAfterRollback();\\n                // 3.1 Successfully rolled back\\n                throw new TransactionalExecutor.ExecutionException(tx, TransactionalExecutor.Code.RollbackDone, ex);\\n            } catch (TransactionException txe) {\\n                // 3.2 Failed to rollback\\n                throw new TransactionalExecutor.ExecutionException(tx, txe, TransactionalExecutor.Code.RollbackFailure, ex);\\n            }\\n        }\\n        // 4. everything is fine commit.\\n        try {\\n            triggerBeforeCommit();\\n            tx.commit();\\n            triggerAfterCommit();\\n        } catch (TransactionException txe) {\\n            // 4.1 Failed to commit\\n            throw new TransactionalExecutor.ExecutionException(tx, txe, TransactionalExecutor.Code.CommitFailure);\\n        }\\n        return rs;\\n    } finally {\\n        // 5. clear\\n        triggerAfterCompletion();\\n        cleanUp();\\n    }\\n}\\n```\\n\\nThe begin method of [DefaultGlobalTransaction](https://github.com/apache/incubator-seata/blob/develop/tm/src/main/java/com/alibaba/fescar/tm/api/DefaultGlobalTransaction.java) is called to start a global transaction.\\n\\n```java\\npublic void begin(int timeout, String name) throws TransactionException {\\n    if (role != GlobalTransactionRole.Launcher) {\\n        check();\\n        if (LOGGER.isDebugEnabled()) {\\n            LOGGER.debug(\\"Ignore Begin(): just involved in global transaction [\\" + xid + \\"]\\");\\n        }\\n        return;\\n    }\\n    if (xid != null) {\\n        throw new IllegalStateException();\\n    }\\n    if (RootContext.getXID() != null) {\\n        throw new IllegalStateException();\\n    }\\n    // Specific method to start the transaction, get the XID returned by TC\\n    xid = transactionManager.begin(null, null, name, timeout);\\n    status = GlobalStatus.Begin;\\n    RootContext.bind(xid);\\n    if (LOGGER.isDebugEnabled()) {\\n        LOGGER.debug(\\"Begin a NEW global transaction [\\" + xid + \\"]\\");\\n    }\\n}\\n```\\n\\nAt the beginning of the method, `if (role != GlobalTransactionRole.Launcher)` checks the role to determine whether the current role is the initiator (Launcher) or the participant (Participant) of the global transaction. If the `@GlobalTransactional` annotation is also added to the downstream system methods in a distributed transaction, its role is Participant, and the subsequent begin will be ignored and directly returned. The determination of whether it is a Launcher or Participant is based on whether the current context already has an XID. The one without an XID is the Launcher, and the one with an XID is the Participant. Therefore, the creation of a global transaction can only be executed by the Launcher, and only one Launcher exists in a distributed transaction.\\n\\nThe [DefaultTransactionManager](https://github.com/apache/incubator-seata/blob/develop/tm/src/main/java/com/alibaba/fescar/tm/DefaultTransactionManager.java) is responsible for TM and TC communication, sending begin, commit, rollback instructions.\\n\\n```java\\n@Override\\npublic String begin(String applicationId, String transactionServiceGroup, String name, int timeout) throws TransactionException {\\n    GlobalBeginRequest request = new GlobalBeginRequest();\\n    request.setTransactionName(name);\\n    request.setTimeout(timeout);\\n    GlobalBeginResponse response = (GlobalBeginResponse) syncCall(request);\\n    return response.getXid();\\n}\\n```\\n\\nAt this point, the XID returned by fescar-server indicates that a global transaction has been successfully created. The log also reflects the above process.\\n\\n```java\\n2019-04-09 13:46:57.417 DEBUG 31326 --- [nio-8084-exec-1] c.a.f.c.rpc.netty.AbstractRpcRemoting : offer message: timeout=60000, transactionName=purchase(java.lang.String, java.lang.String, int)\\n2019-04-09 13:46:57.417 DEBUG 31326 --- [geSend_TMROLE_1] c.a.f.c.rpc.netty.AbstractRpcRemoting : write message:FescarMergeMessage, timeout=60000, transactionName=purchase(java.lang.String, java.lang.String, int) channel:[id: 0xa148545e L:/127.0.0.1:56120 - R:/127.0.0.1:8091] active?true, writable?true, isopen?true\\n2019-04-09 13:46:57.418 DEBUG 31326 --- [lector_TMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Send:FescarMergeMessage, timeout=60000, transactionName=purchase(java.lang.String, java.lang.String, int)\\n2019-04-09 13:46:57.421 DEBUG 31326 --- [lector_TMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Receive:MergeResultMessage, com.alibaba.fescar.core.protocol.transaction.GlobalBeginResponse@2dc480dc, messageId:1\\n2019-04-09 13:46:57.421 DEBUG 31326 --- [nio-8084-exec-1] c.a.fescar.core.context.RootContext : bind 192.168.224.93:8091:2008502699\\n2019-04-09 13:46:57.421 DEBUG 31326 --- [nio-8084-exec-1] c.a.f.tm.api.DefaultGlobalTransaction : Begin a NEW global transaction [192.168.224.93:8091:2008502699]\\n```\\n\\nAfter the global transaction is created, business.execute() is executed, which is the business code `storageFeignClient.deduct(commodityCode, orderCount)` that enters the RM processing flow. The business logic here is to call the inventory service\'s deduct inventory interface.\\n\\n## RM Processing Flow\\n\\n```java\\n@GetMapping(path = \\"/deduct\\")\\npublic Boolean deduct(String commodityCode, Integer count) {\\n    storageService.deduct(commodityCode, count);\\n    return true;\\n}\\n@Transactional\\npublic void deduct(String commodityCode, int count) {\\n    Storage storage = storageDAO.findByCommodityCode(commodityCode);\\n    storage.setCount(storage.getCount() - count);\\n    storageDAO.save(storage);\\n}\\n```\\n\\nThe interface and service method of storage do not appear to have fescar-related code and annotations, reflecting fescar\'s non-intrusiveness. How does it join this global transaction? The answer is in the [ConnectionProxy](https://github.com/apache/incubator-seata/blob/develop/rm-datasource/src/main/java/com/alibaba/fescar/rm/datasource/ConnectionProxy.java), which is why the `DataSourceProxy` must be used. Through DataSourceProxy, fescar can register branch transactions and send RM\'s processing results to TC at the cut-in point of local transaction submission in business code. Since the local transaction submission of business code is implemented by the proxy of `ConnectionProxy`, the commit method of ConnectionProxy is actually executed during local transaction submission.\\n\\n```java\\npublic void commit() throws SQLException {\\n    // If it is a global transaction, perform global transaction submission\\n    // Determine if it is a global transaction, just check if there is an XID in the current context\\n    if (context.inGlobalTransaction()) {\\n        processGlobalTransactionCommit();\\n    } else if (context.isGlobalLockRequire()) {\\n        processLocalCommitWithGlobalLocks();\\n    } else {\\n        targetConnection.commit();\\n    }\\n}\\n\\nprivate void processGlobalTransactionCommit() throws SQLException {\\n    try {\\n        // First register RM with TC and get the branchId assigned by TC\\n        register();\\n    } catch (TransactionException e) {\\n        recognizeLockKeyConflictException(e);\\n    }\\n    try {\\n        if (context.hasUndoLog()) {\\n            // Write undo log\\n            UndoLogManager.flushUndoLogs(this);\\n        }\\n        // Commit local transaction, write undo_log and business data in a local transaction\\n        targetConnection.commit();\\n    } catch (Throwable ex) {\\n        // Notify TC that RM\'s transaction processing failed\\n        report(false);\\n        if (ex instanceof SQLException) {\\n            throw new SQLException(ex);\\n        }\\n    }\\n    // Notify TC that RM\'s transaction processing succeeded\\n    report(true);\\n    context.reset();\\n}\\n\\nprivate void register() throws TransactionException {\\n    // Register RM, build request to send registration instructions to TC via netty\\n    Long branchId = DefaultResourceManager.get().branchRegister(BranchType.AT, getDataSourceProxy().getResourceId(), null, context.getXid(), null, context.buildLockKeys());\\n    // Save the returned branchId in the context\\n    context.setBranchId(branchId);\\n}\\n```\\n\\nVerify the above process through logs.\\n\\n```java\\n2019-04-09 21:57:48.341 DEBUG 38933 --- [nio-8081-exec-1] o.s.c.a.f.web.FescarHandlerInterceptor : xid in Root\\n\\nContext null xid in RpcContext 192.168.0.2:8091:2008546211\\n2019-04-09 21:57:48.341 DEBUG 38933 --- [nio-8081-exec-1] c.a.fescar.core.context.RootContext : bind 192.168.0.2:8091:2008546211\\n2019-04-09 21:57:48.341 DEBUG 38933 --- [nio-8081-exec-1] o.s.c.a.f.web.FescarHandlerInterceptor : bind 192.168.0.2:8091:2008546211 to RootContext\\n2019-04-09 21:57:48.386 INFO 38933 --- [nio-8081-exec-1] o.h.h.i.QueryTranslatorFactoryInitiator : HHH000397: Using ASTQueryTranslatorFactory\\nHibernate: select storage0_.id as id1_0_, storage0_.commodity_code as commodit2_0_, storage0_.count as count3_0_ from storage_tbl storage0_ where storage0_.commodity_code=?\\nHibernate: update storage_tbl set count=? where id=?\\n2019-04-09 21:57:48.673 INFO 38933 --- [nio-8081-exec-1] c.a.fescar.core.rpc.netty.RmRpcClient : will connect to 192.168.0.2:8091\\n2019-04-09 21:57:48.673 INFO 38933 --- [nio-8081-exec-1] c.a.fescar.core.rpc.netty.RmRpcClient : RM will register :jdbc:mysql://127.0.0.1:3306/db_storage?useSSL=false\\n2019-04-09 21:57:48.673 INFO 38933 --- [nio-8081-exec-1] c.a.f.c.rpc.netty.NettyPoolableFactory : NettyPool create channel to {\\"address\\":\\"192.168.0.2:8091\\", \\"message\\":{\\"applicationId\\":\\"storage-service\\", \\"byteBuffer\\":{\\"char\\":\\"\\\\u0000\\",\\"direct\\":false,\\"double\\":0.0,\\"float\\":0.0,\\"int\\":0,\\"long\\":0,\\"readOnly\\":false,\\"short\\":0},\\"resourceIds\\":\\"jdbc:mysql://127.0.0.1:3306/db_storage?useSSL=false\\",\\"transactionServiceGroup\\":\\"hello-service-fescar-service-group\\",\\"typeCode\\":103,\\"version\\":\\"0.4.0\\"},\\"transactionRole\\":\\"RMROLE\\"}\\n2019-04-09 21:57:48.677 DEBUG 38933 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Send:RegisterRMRequest{resourceIds=\'jdbc:mysql://127.0.0.1:3306/db_storage?useSSL=false\', applicationId=\'storage-service\', transactionServiceGroup=\'hello-service-fescar-service-group\'}\\n2019-04-09 21:57:48.680 DEBUG 38933 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Receive:version=0.4.1,extraData=null,identified=true,resultCode=null,msg=null,messageId:9\\n2019-04-09 21:57:48.680 DEBUG 38933 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.AbstractRpcRemoting : com.alibaba.fescar.core.rpc.netty.RmRpcClient@7d61f5d4 msgId:9 future :com.alibaba.fescar.core.protocol.MessageFuture@186cd3e0 body:version=0.4.1,extraData=null,identified=true,resultCode=null,msg=null\\n2019-04-09 21:57:48.680 INFO 38933 --- [nio-8081-exec-1] c.a.fescar.core.rpc.netty.RmRpcClient : register RM success. server version:0.4.1 channel:[id: 0xd40718e3 L:/192.168.0.2:62607 - R:/192.168.0.2:8091]\\n2019-04-09 21:57:48.680 INFO 38933 --- [nio-8081-exec-1] c.a.f.c.rpc.netty.NettyPoolableFactory : register success cost 3 ms version:0.4.1 role:RMROLE channel:[id: 0xd40718e3 L:/192.168.0.2:62607 - R:/192.168.0.2:8091]\\n2019-04-09 21:57:48.680 DEBUG 38933 --- [nio-8081-exec-1] c.a.f.c.rpc.netty.AbstractRpcRemoting : offer message: transactionId=2008546211, branchType=AT, resourceId=jdbc:mysql://127.0.0.1:3306/db_storage?useSSL=false, lockKey=storage_tbl:1\\n2019-04-09 21:57:48.681 DEBUG 38933 --- [geSend_RMROLE_1] c.a.f.c.rpc.netty.AbstractRpcRemoting : write message:FescarMergeMessage, transactionId=2008546211, branchType=AT, resourceId=jdbc:mysql://127.0.0.1:3306/db_storage?useSSL=false, lockKey=storage_tbl:1 channel:[id: 0xd40718e3 L:/192.168.0.2:62607 - R:/192.168.0.2:8091] active?true, writable?true, isopen?true\\n2019-04-09 21:57:48.681 DEBUG 38933 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Send:FescarMergeMessage, transactionId=2008546211, branchType=AT, resourceId=jdbc:mysql://127.0.0.1:3306/db_storage?useSSL=false, lockKey=storage_tbl:1\\n2019-04-09 21:57:48.687 DEBUG 38933 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Receive:MergeResultMessage, BranchRegisterResponse: transactionId=2008546211, branchId=2008546212, result code=Success, getMsg=null, messageId:11\\n2019-04-09 21:57:48.702 DEBUG 38933 --- [nio-8081-exec-1] c.a.f.rm.datasource.undo.UndoLogManager : Flushing UNDO LOG: {\\"branchId\\":2008546212, \\"sqlUndoLogs\\":[{\\"afterImage\\":{\\"rows\\":[{\\"fields\\":[{\\"keyType\\":\\"PrimaryKey\\",\\"name\\":\\"id\\",\\"type\\":4,\\"value\\":1},{\\"keyType\\":\\"NULL\\",\\"name\\":\\"count\\",\\"type\\":4,\\"value\\":993}]}],\\"tableName\\":\\"storage_tbl\\"},\\"beforeImage\\":{\\"rows\\":[{\\"fields\\":[{\\"keyType\\":\\"PrimaryKey\\",\\"name\\":\\"id\\",\\"type\\":4,\\"value\\":1},{\\"keyType\\":\\"NULL\\",\\"name\\":\\"count\\",\\"type\\":4,\\"value\\":994}]}],\\"tableName\\":\\"storage_tbl\\"},\\"sqlType\\":\\"UPDATE\\",\\"tableName\\":\\"storage_tbl\\"}],\\"xid\\":\\"192.168.0.2:8091:2008546211\\"}\\n2019-04-09 21:57:48.755 DEBUG 38933 --- [nio-8081-exec-1] c.a.f.c.rpc.netty.AbstractRpcRemoting : offer message: transactionId=2008546211, branchId=2008546212, resourceId=null, status=PhaseOne_Done, applicationData=null\\n2019-04-09 21:57:48.755 DEBUG 38933 --- [geSend_RMROLE_1] c.a.f.c.rpc.netty.AbstractRpcRemoting : write message:FescarMergeMessage, transactionId=2008546211, branchId=2008546212, resourceId=null, status=PhaseOne_Done, applicationData=null channel:[id: 0xd40718e3 L:/192.168.0.2:62607 - R:/192.168.0.2:8091] active?true, writable?true, isopen?true\\n2019-04-09 21:57:48.756 DEBUG 38933 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Send:FescarMergeMessage, transactionId=2008546211, branchId=2008546212, resourceId=null, status=PhaseOne_Done, applicationData=null\\n2019-04-09 21:57:48.758 DEBUG 38933 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Receive:MergeResultMessage, com.alibaba.fescar.core.protocol.transaction.BranchReportResponse@582a08cf, messageId:13\\n2019-04-09 21:57:48.799 DEBUG 38933 --- [nio-8081-exec-1] c.a.fescar.core.context.RootContext : unbind 192.168.0.2:8091:2008546211\\n2019-04-09 21:57:48.799 DEBUG 38933 --- [nio-8081-exec-1] o.s.c.a.f.web.FescarHandlerInterceptor : unbind 192.168.0.2:8091:2008546211 from RootContext\\n```\\n\\n1. Get the XID passed from business-service\\n2. Bind X\\n\\nID to the current context\\n3. Execute business logic SQL\\n4. Create a Netty connection to TC\\n5. Send branch transaction information to TC\\n6. Get the branchId returned by TC\\n7. Record Undo Log data\\n8. Send the processing result of PhaseOne stage to TC\\n9. Unbind XID from the current context\\n\\nSteps 1 and 9 are completed in the [FescarHandlerInterceptor](https://github.com/dongsheep/spring-cloud-alibaba/blob/master/spring-cloud-alibaba-fescar/src/main/java/org/springframework/cloud/alibaba/fescar/web/FescarHandlerInterceptor.java), which is not part of fescar, but implemented in spring-cloud-alibaba-fescar. It realizes the bind and unbind of xid to the current request context during feign and rest communication.\\n\\nHere, RM completes the work of the PhaseOne stage, then look at the processing logic of the PhaseTwo stage.\\n\\n## Transaction Commit\\n\\nAfter each branch transaction is completed, TC summarizes the reported results of each RM and sends commit or rollback instructions to each RM.\\n\\n```java\\n2019-04-09 21:57:49.813 DEBUG 38933 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Receive:xid=192.168.0.2:8091:2008546211, branchId=2008546212, branchType=AT, resourceId=jdbc:mysql://127.0.0.1:3306/db_storage?useSSL=false, applicationData=null, messageId:1\\n2019-04-09 21:57:49.813 DEBUG 38933 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.AbstractRpcRemoting : com.alibaba.fescar.core.rpc.netty.RmRpcClient@7d61f5d4 msgId:1 body:xid=192.168.0.2:8091:2008546211, branchId=2008546212, branchType=AT, resourceId=jdbc:mysql://127.0.0.1:3306/db_storage?useSSL=false, applicationData=null\\n2019-04-09 21:57:49.814 INFO 38933 --- [atch_RMROLE_1_8] c.a.f.core.rpc.netty.RmMessageListener : onMessage:xid=192.168.0.2:8091:2008546211, branchId=2008546212, branchType=AT, resourceId=jdbc:mysql://127.0.0.1:3306/db_storage?useSSL=false, applicationData=null\\n2019-04-09 21:57:49.816 INFO 38933 --- [atch_RMROLE_1_8] com.alibaba.fescar.rm.AbstractRMHandler : Branch committing: 192.168.0.2:8091:2008546211, 2008546212, jdbc:mysql://127.0.0.1:3306/db_storage?useSSL=false, null\\n2019-04-09 21:57:49.816 INFO 38933 --- [atch_RMROLE_1_8] com.alibaba.fescar.rm.AbstractRMHandler : Branch commit result: PhaseTwo_Committed\\n2019-04-09 21:57:49.817 INFO 38933 --- [atch_RMROLE_1_8] c.a.fescar.core.rpc.netty.RmRpcClient : RmRpcClient sendResponse branchStatus=PhaseTwo_Committed, result code=Success, getMsg=null\\n2019-04-09 21:57:49.817 DEBUG 38933 --- [atch_RMROLE_1_8] c.a.f.c.rpc.netty.AbstractRpcRemoting : send response:branchStatus=PhaseTwo_Committed, result code=Success, getMsg=null channel:[id: 0xd40718e3 L:/192.168.0.2:62607 - R:/192.168.0.2:8091]\\n2019-04-09 21:57:49.817 DEBUG 38933 --- [lector_RMROLE_1] c.a.f.c.rpc.netty.MessageCodecHandler : Send:branchStatus=PhaseTwo_Committed, result code=Success, getMsg=null\\n```\\n\\nThe log shows\\n\\n1. RM receives the commit notice of XID=192.168.0.2:8091:2008546211, branchId=2008546212\\n2. Execute the commit action\\n3. Send the commit result to TC, branchStatus is PhaseTwo_Committed\\n\\nSpecifically, see the execution process of the second stage commit in the doBranchCommit method of the [AbstractRMHandler](https://github.com/apache/incubator-seata/blob/develop/rm/src/main/java/com/alibaba/fescar/rm/AbstractRMHandler.java) class.\\n\\n```java\\n/**\\n * Get the key parameters such as xid and branchId notified\\n * Then call the RM\'s branchCommit\\n */\\nprotected void doBranchCommit(BranchCommitRequest request, BranchCommitResponse response) throws TransactionException {\\n    String xid = request.getXid();\\n    long branchId = request.getBranchId();\\n    String resourceId = request.getResourceId();\\n    String applicationData = request.getApplicationData();\\n    LOGGER.info(\\"Branch committing: \\" + xid + \\" \\" + branchId + \\" \\" + resourceId + \\" \\" + applicationData);\\n    BranchStatus status = getResourceManager().branchCommit(request.getBranchType(), xid, branchId, resourceId, applicationData);\\n    response.setBranchStatus(status);\\n    LOGGER.info(\\"Branch commit result: \\" + status);\\n}\\n```\\n\\nEventually, the branchCommit request will be called to the branchCommit method of [AsyncWorker](https://github.com/apache/incubator-seata/blob/develop/rm-datasource/src/main/java/com/alibaba/fescar/rm/datasource/AsyncWorker.java). AsyncWorker\'s processing method is a key part of fescar\'s architecture. Since most transactions will be committed normally, they end in the PhaseOne stage. This way, locks can be released as quickly as possible. After receiving the commit instruction in the PhaseTwo stage, the asynchronous processing can be done. This excludes the time consumption of the PhaseTwo stage from a distributed transaction.\\n\\n```java\\nprivate static final List<Phase2Context> ASYNC_COMMIT_BUFFER = Collections.synchronizedList(new ArrayList<Phase2Context>());\\n\\n/**\\n * Add the XID to be committed to the list\\n */\\n@Override\\npublic BranchStatus branchCommit(BranchType branchType, String xid, long branchId, String resourceId, String applicationData) throws TransactionException {\\n    if (ASYNC_COMMIT_BUFFER.size() < ASYNC_COMMIT_BUFFER_LIMIT) {\\n        ASYNC_COMMIT_BUFFER.add(new Phase2Context(branchType, xid, branchId, resourceId, applicationData));\\n    } else {\\n        LOGGER.warn(\\"Async commit buffer is FULL. Rejected branch [\\" + branchId + \\"/\\" + xid + \\"] will be handled by housekeeping later.\\");\\n    }\\n    return BranchStatus.PhaseTwo_Committed;\\n}\\n\\n/**\\n * Consume the XIDs in ASYNC_COMMIT_BUFFER through scheduled tasks\\n */\\npublic synchronized void init() {\\n    LOGGER.info(\\"Async Commit Buffer Limit: \\" + ASYNC_COMMIT_BUFFER_LIMIT);\\n    timerExecutor = new ScheduledThreadPoolExecutor(1, new NamedThreadFactory(\\"AsyncWorker\\", 1, true));\\n    timerExecutor.scheduleAtFixedRate(new Runnable() {\\n        @Override\\n        public void run() {\\n            try {\\n                doBranchCommits();\\n            } catch (Throwable e) {\\n                LOGGER.info(\\"Failed at async committing ... \\" + e.getMessage());\\n            }\\n        }\\n    }, 10, 1000 * 1, TimeUnit.MILLISECONDS);\\n}\\n\\nprivate void doBranchCommits() {\\n    if (ASYNC_COMMIT_BUFFER.size() == 0) {\\n        return;\\n    }\\n    Map<String, List<Phase2Context>> mappedContexts = new HashMap<>();\\n    Iterator<Phase2Context> iterator = ASYNC_COMMIT_BUFFER.iterator();\\n    // In a timed loop, take out all pending data in ASYNC_COMMIT_BUFFER\\n    // Group the data to be committed with resourceId as the key. The resourceId is the database connection URL\\n    // This can be seen in the previous log, the purpose is to cover the multiple data sources created by the application\\n    while (iterator.hasNext()) {\\n        Phase2Context commitContext = iterator.next();\\n        List<Phase2Context> contextsGroupedByResourceId = mappedContexts.get(commitContext.resourceId);\\n        if (contextsGroupedByResourceId == null) {\\n            contextsGroupedByResourceId = new ArrayList<>();\\n            mappedContexts.put(commitContext.resourceId, contextsGroupedByResourceId);\\n        }\\n        contextsGroupedByResourceId.add(commitContext);\\n        iterator.remove();\\n    }\\n    for (Map.Entry<String, List<Phase2Context>> entry : mappedContexts.entrySet()) {\\n        Connection conn = null;\\n        try {\\n            try {\\n                // Get the data source and connection based on resourceId\\n                DataSourceProxy dataSourceProxy = DataSourceManager.get().get(entry.getKey());\\n                conn = dataSourceProxy.getPlainConnection();\\n            } catch (SQLException sqle) {\\n                LOGGER.warn(\\"Failed to get connection for async committing on \\" + entry.getKey(), sqle);\\n                continue;\\n            }\\n            List<Phase2Context> contextsGroupedByResourceId = entry.getValue();\\n            for (Phase2Context commitContext : contextsGroupedByResourceId) {\\n                try {\\n                    // Perform undolog processing, that is, delete the records corresponding to xid and branchId\\n                    UndoLogManager.deleteUndoLog(commitContext.xid, commitContext.branchId, conn);\\n                } catch\\n\\n (Exception ex) {\\n                    LOGGER.warn(\\"Failed to delete undo log [\\" + commitContext.branchId + \\"/\\" + commitContext.xid + \\"]\\", ex);\\n                }\\n            }\\n        } finally {\\n            if (conn != null) {\\n                try {\\n                    conn.close();\\n                } catch (SQLException closeEx) {\\n                    LOGGER.warn(\\"Failed to close JDBC resource while deleting undo_log \\", closeEx);\\n                }\\n            }\\n        }\\n    }\\n}\\n```\\n\\nSo for the commit action, RM only needs to delete the undo_log corresponding to xid and branchId.\\n\\n## Transaction Rollback\\n\\nThere are two scenarios for triggering rollback:\\n\\n1. Branch transaction processing exception, that is the case of `report(false)` in the [ConnectionProxy](https://github.com/apache/incubator-seata/blob/develop/rm-datasource/src/main/java/com/alibaba/fescar/rm/datasource/ConnectionProxy.java)\\n2. TM catches the exceptions thrown from the downstream system, that is the exception caught by the method marked with the `@GlobalTransactional` annotation in the initiated global transaction. In the previous template method execute of the [TransactionalTemplate](https://github.com/apache/incubator-seata/blob/develop/tm/src/main/java/com/alibaba/fescar/tm/api/TransactionalTemplate.java), the call to business.execute() was caught. After the catch, it will call rollback, and TM will notify TC that the corresponding XID needs to roll back the transaction.\\n\\n```java\\npublic void rollback() throws TransactionException {\\n    // Only the Launcher can initiate this rollback\\n    if (role == GlobalTransactionRole.Participant) {\\n        // Participant has no responsibility of committing\\n        if (LOGGER.isDebugEnabled()) {\\n            LOGGER.debug(\\"Ignore Rollback(): just involved in global transaction [\\" + xid + \\"]\\");\\n        }\\n        return;\\n    }\\n    if (xid == null) {\\n        throw new IllegalStateException();\\n    }\\n    status = transactionManager.rollback(xid);\\n    if (RootContext.getXID() != null) {\\n        if (xid.equals(RootContext.getXID())) {\\n            RootContext.unbind();\\n        }\\n    }\\n}\\n```\\n\\nTC summarizes and sends rollback instructions to the participants. RM receives the rollback notice in the doBranchRollback method of the [AbstractRMHandler](https://github.com/apache/incubator-seata/blob/develop/rm/src/main/java/com/alibaba/fescar/rm/AbstractRMHandler.java) class.\\n\\n```java\\nprotected void doBranchRollback(BranchRollbackRequest request, BranchRollbackResponse response) throws TransactionException {\\n    String xid = request.getXid();\\n    long branchId = request.getBranchId();\\n    String resourceId = request.getResourceId();\\n    String applicationData = request.getApplicationData();\\n    LOGGER.info(\\"Branch rolling back: \\" + xid + \\" \\" + branchId + \\" \\" + resourceId);\\n    BranchStatus status = getResourceManager().branchRollback(request.getBranchType(), xid, branchId, resourceId, applicationData);\\n    response.setBranchStatus(status);\\n    LOGGER.info(\\"Branch rollback result: \\" + status);\\n}\\n```\\n\\nThen the rollback request is passed to the branchRollback method of the [DataSourceManager](https://github.com/apache/incubator-seata/blob/develop/rm-datasource/src/main/java/com/alibaba/fescar/rm/datasource/DataSourceManager.java) class.\\n\\n```java\\npublic BranchStatus branchRollback(BranchType branchType, String xid, long branchId, String resourceId, String applicationData) throws TransactionException {\\n    // Get the corresponding data source based on resourceId\\n    DataSourceProxy dataSourceProxy = get(resourceId);\\n    if (dataSourceProxy == null) {\\n        throw new ShouldNeverHappenException();\\n    }\\n    try {\\n        UndoLogManager.undo(dataSourceProxy, xid, branchId);\\n    } catch (TransactionException te) {\\n        if (te.getCode() == TransactionExceptionCode.BranchRollbackFailed_Unretriable) {\\n            return BranchStatus.PhaseTwo_RollbackFailed_Unretryable;\\n        } else {\\n            return BranchStatus.PhaseTwo_RollbackFailed_Retryable;\\n        }\\n    }\\n    return BranchStatus.PhaseTwo_Rollbacked;\\n}\\n```\\n\\nUltimately, the undo method of the [UndoLogManager](https://github.com/apache/incubator-seata/blob/develop/rm-datasource/src/main/java/com/alibaba/fescar/rm/datasource/undo/UndoLogManager.java) is executed. Since it is pure JDBC operation, the code is relatively long and will not be posted here. You can view the source code on GitHub via the link. Briefly describe the undo process:\\n\\n1. Find the undo_log submitted in the PhaseOne stage based on xid and branchId.\\n2. If found, generate the playback SQL based on the data recorded in the undo_log and execute it, that is, restore the data modified in the PhaseOne stage.\\n3. After step 2 is completed, delete the corresponding undo_log data.\\n4. If no corresponding undo_log is found in step 1, insert a new undo_log with the status `GlobalFinished`. The reason it is not found may be that the local transaction in the PhaseOne stage encountered an exception, resulting in no normal write-in. Because xid and branchId are unique indexes, the insertion in step 4 can prevent successful write-in after recovery in the PhaseOne stage. In this way, the business data will not be successfully submitted, achieving the effect of final rollback.\\n\\n## Summary\\n\\nCombining distributed business scenarios with local, this article analyzes the main processing flow of the fescar client side and analyzes the main source code for the TM and RM roles, hoping to help you understand the working principles of fescar. With the rapid iteration of fescar and the planning of future Roadmaps, it is believed that fescar can become a benchmark solution for open-source distributed transactions in time."},{"id":"/seata-analysis-java-server","metadata":{"permalink":"/blog/seata-analysis-java-server","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-java-server.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-java-server.md","title":"In-Depth Analysis of One-Stop Distributed Transaction Solution Seata-Server","description":"Not long ago, I wrote an analysis of the distributed transaction middleware Fescar. Shortly after, the Fescar team rebranded it as Seata (Simple Extensible Autonomous Transaction Architecture), whereas the previous Fescar\'s English full name was Fast & Easy Commit And Rollback. It can be seen that Fescar was more limited to Commit and Rollback based on its name, while the new brand name Seata aims to create a one-stop distributed transaction solution. With the name change, I am more confident about its future development.","date":"2019-04-08T00:00:00.000Z","formattedDate":"April 8, 2019","tags":[],"readingTime":20.13,"hasTruncateMarker":false,"authors":[{"name":"zhao.li,min.ji"}],"frontMatter":{"title":"In-Depth Analysis of One-Stop Distributed Transaction Solution Seata-Server","author":"zhao.li,min.ji","date":"2019/04/08","keywords":["fescar","seata","distributed transaction"]},"unlisted":false,"prevItem":{"title":"Detailed Explanation of Seata-Client Principles and Processes in Distributed Transactions","permalink":"/blog/seata-analysis-java-client"},"nextItem":{"title":"Analysis of Applicable Models and Scenarios for TCC","permalink":"/blog/tcc-mode-applicable-scenario-analysis"}},"content":"Not long ago, I wrote an analysis of the distributed transaction middleware Fescar. Shortly after, the Fescar team rebranded it as Seata (Simple Extensible Autonomous Transaction Architecture), whereas the previous Fescar\'s English full name was Fast & Easy Commit And Rollback. It can be seen that Fescar was more limited to Commit and Rollback based on its name, while the new brand name Seata aims to create a one-stop distributed transaction solution. With the name change, I am more confident about its future development.\\n\\nHere, let\'s briefly recall the overall process model of Seata:\\n\\n![Seata Process Model](/img/blog/20190327000119.png)\\n\\n- TM: Transaction initiator. Used to inform TC about the start, commit, and rollback of global transactions.\\n- RM: Specific transaction resource. Each RM is registered as a branch transaction in TC.\\n- TC: Transaction coordinator. Also known as Fescar-server, used to receive registration, commit, and rollback of transactions.\\n\\nIn previous articles, I provided a general introduction to the roles, and in this article, I will focus on the core role TC, which is the transaction coordinator.\\n\\n# 2. Transaction Coordinator\\n\\nWhy has the emphasis been placed on TC as the core role? Because TC, like God, manages the RM and TM of countless beings in the cloud. If TC fails to function properly, even minor issues with RM and TM will lead to chaos. Therefore, to understand Seata, one must understand its TC.\\n\\nSo, what capabilities should an excellent transaction coordinator possess? I think it should have the following:\\n\\n- Correct coordination: It should be able to properly coordinate what RM and TM should do next, what to do if something goes wrong, and what to do if everything goes right.\\n- High availability: The transaction coordinator is crucial in distributed transactions. If it cannot ensure high availability, it serves no purpose.\\n- High performance: The performance of the transaction coordinator must be high. If there are performance bottlenecks, it will frequently encounter timeouts, leading to frequent rollbacks.\\n- High scalability: This characteristic belongs to the code level. If it is an excellent framework, it needs to provide many customizable extensions for users, such as service registration/discovery, reading configuration, etc.\\n\\nNext, I will explain how Seata achieves the above four points.\\n\\n## 2.1 Seata-Server Design\\n\\n![Seata-Server Design](/img/seata-server/design.png)\\n\\nThe overall module diagram of Seata-Server is shown above:\\n\\n\\n- Coordinator Core: At the bottom is the core code of the transaction coordinator, mainly used to handle transaction coordination logic, such as whether to commit, rollback, etc.\\n- Store: Storage module used to persist data to prevent data loss during restarts or crashes.\\n- Discovery: Service registration/discovery module used to expose server addresses to clients.\\n- Config: Used to store and retrieve server configurations.\\n- Lock: Lock module used to provide global locking functionality to Seata.\\n- RPC: Used for communication with other endpoints.\\n- HA-Cluster: High availability cluster, currently not open-source, provides reliable high availability services to Seata, expected to be open-sourced in version 0.6.\\n\\n## 2.2 Discovery\\n\\nFirst, let\'s talk about the basic Discovery module, also known as the service registration/discovery module. After starting Seata-Sever, we need to expose our address to other users, which is the responsibility of this module.\\n\\n![Discovery Module](/img/seata-server/discover.png)\\n\\nThis module has a core interface `RegistryService`, as shown in the image above:\\n\\n- register: Used by the server to register the service.\\n- unregister: Used by the server, typically called in JVM shutdown hooks.\\n- subscribe: Used by clients to register event listeners to listen for address changes.\\n- unsubscribe: Used by clients to cancel event listeners.\\n- lookup: Used by clients to retrieve service address lists based on keys.\\n- close: Used to close the Registry resource.\\n\\nIf you need to add your own service registration/discovery, just implement this interface. So far, with the continuous development and promotion in the community, there are already five service registration/discovery implementations, including redis, zk, nacos, eruka, and consul. Below is a brief introduction to the Nacos implementation:\\n\\n### 2.2.1 register Interface:\\n\\n![Register Interface](/img/seata-server/register.png)\\n\\nStep 1: Validate the address.\\nStep 2: Get the Naming instance of Nacos and register the address with the service name `serverAddr` (fixed service name) on the corresponding cluster group (configured in registry.conf).\\n\\nThe unregister interface is similar, and I won\'t go into detail here.\\n\\n### 2.2.2 lookup Interface:\\n\\n![Lookup Interface](/img/seata-server/lookup.png)\\n\\nStep 1: Get the current cluster name.\\nStep 2: Check if the service corresponding to the current cluster name has been subscribed. If yes, directly retrieve the subscribed data from the map.\\n\\nStep 3: If not subscribed, actively query the service instance list once, then add subscription and store the data returned by subscription in the map. After that, retrieve the latest data directly from the map.\\n\\n### 2.2.3 subscribe Interface:\\n\\n![Subscribe Interface](/img/seata-server/subscribe.png)\\n\\nThis interface is relatively simple, divided into two steps:\\nStep 1: Add the `cluster -> listener` to be subscribed to the map. Since Nacos does not provide a single machine already subscribed list, it needs to be implemented by itself.\\nStep 2: Subscribe using the Nacos API.\\n\\n## 2.3 Config\\n\\nThe configuration module is also a relatively basic and simple module. We need to configure some common parameters such as the number of select and work threads for Netty, the maximum allowed session, etc. Of course, these parameters in Seata have their own default settings.\\n\\nSimilarly, Seata also provides an interface `Configuration` for customizing where we need to obtain configurations:\\n\\n![Config Interface](/img/seata-server/config.png)\\n\\n- getInt/Long/Boolean/getConfig(): Retrieves the corresponding value based on the dataId. If the configuration cannot be read, an exception occurs, or a timeout occurs, it returns the default value specified in the parameters.\\n- putConfig: Used to add configuration.\\n- removeConfig: Deletes a configuration.\\n- add/remove/get ConfigListener: Add/remove/get configuration listeners, usually used to listen for configuration changes.\\n\\nCurrently, there are four ways to obtain Config: File (file-based), Nacos, Apollo, and ZK (not recommended). In Seata, you first need to configure `registry.conf` to specify the `config.type`. Implementing Config is relatively simple, and I won\'t delve into it here.\\n\\n## 2.4 Store\\n\\nThe implementation of the storage layer is crucial for Seata\'s performance and reliability.\\nIf the storage layer is not implemented well, data being processed by TC in distributed transactions may be lost in the event of a crash. Since distributed transactions cannot tolerate data loss, if the storage layer is implemented well but has significant performance issues, RM may experience frequent rollbacks, making it unable to cope with high-concurrency scenarios.\\n\\nIn Seata, file storage is provided as the default method for storing data. Here, we define the data to be stored as sessions. Sessions created by the TM are referred to as GlobalSessions, while those created by RMs are called BranchSessions. A GlobalSession can have multiple BranchSessions. Our objective is to store all these sessions.\\n\\nthe code of FileTransactionStoreManager:\\n\\n![](/img/seata-server/store.png)\\n\\nThe code snippet above can be broken down into the following steps:\\n\\n- **Step 1**: Generate a TransactionWriteFuture.\\n- **Step 2**: Put this futureRequest into a LinkedBlockingQueue. Why do we need to put all the data into a queue? Well, in fact, we could also use locks for this purpose. In another Alibaba open-source project, RocketMQ, locks are used. Whether it\'s a queue or a lock, their purpose is to ensure single-threaded writing. But why is that necessary? Some might explain that it\'s to ensure sequential writing, which would improve speed. However, this understanding is incorrect. The write method of our FileChannel is thread-safe and already ensures sequential writing. Ensuring single-threaded writing is actually to make our write logic single-threaded. This is because there may be logic such as when a file is full or when records are written to specific positions. Of course, this logic could be actively locked, but to achieve simplicity and convenience, it\'s most appropriate to queue the entire write logic for processing.\\n- **Step 3**: Call future.get to wait for the completion notification of our write logic.\\n\\nOnce we submit the data to the queue, the next step is to consume it. The code is as follows:\\n\\n![Write Data File](/img/seata-server/storewrite.png)\\n\\nHere, a WriteDataFileRunnable() is submitted to our thread pool, and the run() method of this Runnable is as follows:\\n\\n![Store Run](/img/seata-server/storerun.png)\\n\\nIt can be broken down into the following steps:\\n\\n- **Step 1**: Check if stopping is true. If so, return null.\\n- **Step 2**: Get data from our queue.\\n- **Step 3**: Check if the future has timed out. If so, set the result to false. At this point, our producer\'s get() method will unblock.\\n- **Step 4**: Write our data to the file. At this point, the data is still in the pageCache layer and has not been flushed to the disk yet. If the write is successful, flush it based on certain conditions.\\n- **Step 5**: When the number of writes reaches a certain threshold, or when the writing time exceeds a certain limit, the current file needs to be saved as a historical file, the old historical files need to be deleted, and a new file needs to be created. This step is to prevent unlimited growth of our files, which would waste disk resources.\\n\\nIn our writeDataFile method, we have the following code:\\n\\n![Write Data File](/img/seata-server/writedatafile.png)\\n\\n- **Step 1**: First, get our ByteBuffer. If it exceeds the maximum loop BufferSize, create a new one directly; otherwise, use our cached Buffer. This step can greatly reduce garbage collection.\\n- **Step 2**: Add the data to the ByteBuffer.\\n- **Step 3**: Finally, write the ByteBuffer to our fileChannel. This will be retried three times. At this point, the data is still in the pageCache layer and is affected by two factors: the OS has its own flushing strategy, but this business program cannot control it. To prevent events such as crashes from causing a large amount of data loss, the business itself needs to control the flush. Below is the flush code:\\n\\n![Flush](/img/seata-server/flush.png)\\n\\nHere, the flush condition is based on writing a certain number of data or exceeding a certain time. This also presents a small issue: in the event of a power failure, there may still be data in the pageCache that has not been flushed to disk, resulting in a small amount of data loss. Currently, synchronous mode is not supported, which means that each piece of data needs to be flushed, ensuring that each message is written to disk. However, this would greatly affect performance. Of course, there will be continuous evolution and support for this in the future.\\n\\nOur store\'s core process mainly consists of the above methods, but there are also some other processes such as session reconstruction, which are relatively simple and readers can read them on their own.\\n\\n\\n\\n## 2.5 Lock\\n\\nAs we know, the isolation level in databases is mainly implemented through locks. Similarly, in the distributed transaction framework Seata, achieving isolation levels also requires locks. Generally, there are four isolation levels in databases: Read Uncommitted, Read Committed, Repeatable Read, and Serializable. In Seata, it can ensure that the isolation level is Read Committed but provides means to achieve Read Committed isolation.\\n\\nThe Lock module is the core module of Seata for implementing isolation levels. In the Lock module, an interface is provided for managing our locks:\\n![Lock Manager](/img/seata-server/lockManager.png)\\n\\nIt has three methods:\\n\\n- acquireLock: Used to lock our BranchSession. Although a branch transaction Session is passed here, it is actually locking the resources of the branch transaction. Returns true upon successful locking.\\n- isLockable: Queries whether the transaction ID, resource ID, and locked key are already locked.\\n- cleanAllLocks: Clears all locks.\\n\\nFor locks, we can implement them locally or use Redis or MySQL to help us implement them. The official default provides local global lock implementation:\\n![Default Lock](/img/seata-server/defaultLock.png)\\n\\nIn the local lock implementation, there are two constants to pay attention to:\\n\\n\\n- BUCKET_PER_TABLE: Defines how many buckets each table has, aiming to reduce competition when locking the same table later.\\n- LOCK_MAP: This map seems very complex from its definition, with many layers of Maps nested inside and outside. Here\'s a table to explain it specifically:\\n\\n| Layer            | Key                                                           | Value         |\\n| ---------------- | ------------------------------------------------------------- | ------------- |\\n| 1-LOCK_MAP       | resourceId (jdbcUrl)                                          | dbLockMap     |\\n| 2- dbLockMap     | tableName (table name)                                        | tableLockMap  |\\n| 3- tableLockMap  | PK.hashcode%Bucket (hashcode%bucket of the primary key value) | bucketLockMap |\\n| 4- bucketLockMap | PK                                                            | transactionId |\\n\\n\\nIt can be seen that the actual locking occurs in the bucketLockMap. The specific locking method here is relatively simple and will not be detailed. The main process is to gradually find the bucketLockMap and then insert the current transactionId. If this primary key currently has a TransactionId, then it checks if it is itself; if not, the locking fails.\\n\\n## 2.6 RPC\\n\\nOne of the key factors in ensuring Seata\'s high performance is the use of Netty as the RPC framework, with the default configuration of the thread model as shown in the diagram below:\\n\\n![Reactor](/img/seata-server/reactor.png)\\n\\nIf the default basic configuration is adopted, there will be one Acceptor thread for handling client connections and a number of NIO-Threads equal to cpu\\\\*2. In these threads, heavy business operations are not performed. They only handle relatively fast tasks such as encoding and decoding, heartbeats, and TM registration. Time-consuming business operations are delegated to the business thread pool. By default, the business thread pool is configured with a minimum of 100 threads and a maximum of 500.\\n\\nSeata currently allows for configuration of transport layer settings, as shown in the following diagram. Users can optimize Netty transport layer settings according to their needs, and the configuration takes effect when loaded for the first time.\\n\\n![Transport](/img/seata-server/transport.png)\\n\\nIt\'s worth mentioning Seata\'s heartbeat mechanism, which is implemented using Netty\'s IdleStateHandler, as shown below:\\n\\n![Idle State Handler](/img/seata-server/idleStateHandler.png)\\n\\nOn the server side, there is no maximum idle time set for writing, and for reading, the maximum idle time is set to 15 seconds (the client\'s default write idle time is 5 seconds, sending ping messages). If it exceeds 15 seconds, the connection will be disconnected, and resources will be closed.\\n\\n![User Event Triggered](/img/seata-server/userEventTriggered.png)\\n\\n- Step 1: Check if it is a read idle detection event.\\n- Step 2: If it is, disconnect the connection and close the resources.\\n\\nAdditionally, Seata has implemented features such as memory pools, batch merging of small packets by the client for sending, and Netty connection pools (reducing the service unavailable time when creating connections), one of which is batch merging of small packets.\\n\\n![](/img/seata-server/send.png)\\n\\nThe client\'s message sending process does not directly send messages. Instead, it wraps the message into an RpcMessage through AbstractRpcRemoting#sendAsyncRequest and stores it in the basket, then wakes up the merge sending thread. The merge sending thread, through a while true loop, waits for a maximum of 1ms to retrieve messages from the basket and wraps them into merge messages for actual sending. If an exception occurs in the channel during this process, it will quickly fail and return the result through fail-fast. Before sending the merge message, it is marked in the map. Upon receiving the results, batch confirmation is performed (AbstractRpcRemotingClient#channelRead), and then dispatched to messageListener and handler for processing. Additionally, timerExecutor periodically checks for timeouts in sent messages, marking them as failed if they exceed the timeout. Specific details of the message protocol design will be provided in subsequent articles, so stay tuned.\\n\\nSeata\'s Netty Client consists of TMClient and RMClient, distinguished by their transactional roles. Both inherit from AbstractRpcRemotingClient, which implements RemotingService (service start and stop), RegisterMsgListener (Netty connection pool connection creation callback), and ClientMessageSender (message sending), further inheriting from AbstractRpcRemoting (the top-level message sending and processing template for Client and Server).\\n\\nThe class diagram for RMClient is depicted below:\\n![RMClient Class Diagram](/img/seata-server/class.png)\\n\\nTMClient and RMClient interact with channel connections based on their respective poolConfig and NettyPoolableFactory, which implements KeyedPoolableObjectFactory\\\\<NettyPoolKey, Channel>. The channel connection pool locates each connection pool based on the role key+IP, and manages channels uniformly. During the sending process, TMClient and RMClient each use only one long-lived connection per IP. However, if a connection becomes unavailable, it is quickly retrieved from the connection pool, reducing service downtime.\\n\\n## 2.7 HA-Cluster\\n\\nCurrently, the official HA-Cluster design has not been publicly disclosed. However, based on some hints from other middleware and the official channels, HA-Cluster could be designed as follows:\\n![HA-Cluster Design](/img/seata-server/hacluster.png)\\n\\nThe specific process is as follows:\\n\\n**Step 1:** When clients publish information, they ensure that the same transaction with the same transaction ID is handled on the same master. This is achieved by horizontally scaling multiple masters to provide concurrent processing performance.\\n\\n**Step 2:** On the server side, each master has multiple slaves. Data in the master is nearly real-time synchronized to the slaves, ensuring that when the master crashes, other slaves can take over.\\n\\nHowever, all of the above is speculation, and the actual design and implementation will have to wait until version 0.5. Currently, there is a Go version of Seata-Server donated to Seata (still in progress), which implements replica consistency through Raft. However, other details are not clear.\\n\\n## 2.8 Metrics\\n\\nThis module has not yet disclosed a specific implementation. However, it may provide a plugin interface for integrating with other third-party metrics. Recently, Apache SkyWalking has been discussing how to integrate with the Seata team.\\n\\n# 3. Coordinator Core\\nWe have covered many foundational modules of the Seata server. I believe you now have a general understanding of Seata\'s implementation. Next, I will explain how the transaction coordinator\'s specific logic is implemented, providing you with a deeper understanding of Seata\'s internal workings.\\n\\n## 3.1 Startup Process\\nThe startup method is defined in the Server class\'s main method, outlining our startup process:\\n\\n![](/img/seata-server/main.png)\\n\\nstep1: Create an RpcServer, which encapsulates network operations using Netty to implement the server.\\n\\nstep2: Parse the port number, local file address (for recovering incomplete transactions if the server crashes), and IP address (optional, useful for obtaining an external VIP registration service when crossing networks).\\n\\nstep3: Initialize SessionHolder, wherein the crucial step is to recover data from the dataDir folder and rebuild sessions.\\n\\nstep4: Create a Coordinator, the core logic of the transaction coordinator, and initialize it. The initialization process includes creating four scheduled tasks:\\n\\n- retryRollbacking: Retry rollback task, used to retry failed rollbacks, executed every 5ms.\\n- retryCommitting: Retry commit task, used to retry failed commits, executed every 5ms.\\n- asyncCommitting: Asynchronous commit task, used to perform asynchronous commits, executed every 10ms.\\n- timeoutCheck: Timeout task check, used to detect timeout tasks and execute timeout logic, executed every 2ms.\\n\\nstep5: Initialize UUIDGenerator, a basic class used for generating various IDs (transactionId, branchId).\\n\\nstep6: Set the local IP and listening port in XID, initialize rpcServer, and wait for client connections.\\n\\nThe startup process is relatively straightforward. Next, I will describe how Seata handles common business logic in distributed transaction frameworks.\\n\\n## 3.2 Begin - Start Global Transaction\\n\\nThe starting point of a distributed transaction is always to start a global transaction. Let\'s see how Seata implements global transactions:\\n\\n![Begin Global Transaction](/img/seata-server/begin.png)\\n\\nstep1: Create a GloabSession based on the application ID, transaction group, name, and timeout. As mentioned earlier, GloabSession and BranchSession represent different aspects of the transaction.\\n\\nstep2: Add a RootSessionManager to it for listening to some events. Currently, Seata has four types of listeners (it\'s worth noting that all session managers implement SessionLifecycleListener):\\n\\n- ROOT_SESSION_MANAGER: The largest, containing all sessions.\\n- ASYNC_COMMITTING_SESSION_MANAGER: Manages sessions that need asynchronous commit.\\n- RETRY_COMMITTING_SESSION_MANAGER: Manages sessions for retrying commit.\\n- RETRY_ROLLBACKING_SESSION_MANAGER: Manages sessions for retrying rollback.\\nSince this is the beginning of a transaction, other SessionManagers are not needed, so only add RootSessionManager.\\n\\n\\nstep3: Start GloabSession, which changes the state to Begin, records the start time, and calls the onBegin method of RootSessionManager to save the session to the map and write it to the file.\\n\\nstep4: Finally, return the XID. This XID is composed of ip+port+transactionId, which is crucial. When the TM acquires it, it needs to pass this ID to RM. RM uses XID to determine which server to access.\\n\\n## 3.3 BranchRegister - Register Branch Transaction\\n\\nAfter the global transaction is initiated by TM, the branch transaction of our RM also needs to be registered on top of our global transaction. Here\'s how it\'s handled:\\n\\n![Branch Transaction Registration](/img/seata-server/branchRegister.png)\\n\\nstep1: Retrieve and validate the global transaction\'s state based on the transactionId.\\n\\nstep2: Create a new branch transaction, which is our BranchSession.\\n\\nstep3: Lock the branch transaction globally. Here, the logic uses the lock module.\\n\\nstep4: Add the branchSession, mainly by adding it to the globalSession object and writing it to our file.\\n\\nstep5: Return the branchId, which is also important. We will need it later to roll back our transaction or update the status of our branch transaction.\\n\\nAfter registering the branch transaction, it is necessary to report whether the local transaction execution of the branch transaction was successful or failed. Currently, the server simply records this information. The purpose of reporting is that even if this branch transaction fails, if the TM insists on committing the global transaction (catches exceptions without throwing), then when traversing to commit the branch transaction, this failed branch transaction does not need to be committed (users can choose to skip it).\\n\\n## 3.4 GlobalCommit - Global Commit\\n\\nWhen our branch transaction is completed, it\'s up to our TM - Transaction Manager to decide whether to commit or rollback. If it\'s a commit, then the following logic will be executed:\\n\\n![Global Commit](/img/seata-server/commit.png)\\n\\nstep1: First, find our globalSession. If it\'s null, it means it has already been committed, so perform idempotent operation and return success.\\n\\nstep2: Close our GloabSession to prevent new branches from coming in (rollback due to timeout in cross-service calls, provider continues execution).\\n\\nstep3: If the status is Begin, it means it hasn\'t been committed yet, so change its status to Committing, indicating that it\'s committing.\\n\\nstep4: Determine if it can be asynchronously committed. Currently, only AT mode can be asynchronously committed. In two-phase global commits, undolog is only deleted without strict order. Here, a timer task is used, and the client merges and deletes in batches after receiving it.\\n\\nstep5: If it\'s an asynchronous commit, directly put it into our ASYNC_COMMITTING_SESSION_MANAGER to let it asynchronously execute step6 in the background. If it\'s synchronous, then execute step6 directly.\\n\\nstep6: Traverse our BranchSessions for submission. If a branch transaction fails, determine whether to retry based on different conditions. This branchSession can be executed asynchronously, and if it fails, it can continue with the next one because it remains in the manager and won\'t be deleted until it succeeds. If it\'s a synchronous commit, it will be put into the retry queue for scheduled retries and will block and submit in sequence.\\n\\n## 3.5 GlobalRollback - Global Rollback\\n\\nIf our TM decides to globally rollback, it will follow the logic below:\\n\\n![Global Rollback](/img/seata-server/rollback.png)\\n\\nThis logic is basically the same as the commit process but in reverse. I won\'t delve into it here.\\n\\n# 4. Conclusion\\n\\nFinally, let\'s summarize how Seata solves the key points of distributed transactions:\\n\\n- Correct coordination: Through background scheduled tasks, various retries are performed correctly, and in the future, a monitoring platform may be introduced, possibly allowing manual rollback.\\n- High availability: Ensured by HA-Cluster.\\n- High performance: Sequential file writing, RPC implemented through Netty, and Seata can be horizontally scaled in the future to improve processing performance.\\n- High extensibility: Provides places where users can freely implement, such as configuration, service discovery and registration, global lock, etc.\\n\\nIn conclusion, I hope you can understand the core design principles of Seata-Server from this article. Of course, you can also imagine how you would design a distributed transaction server if you were to implement one yourself.\\n\\nSeata GitHub Repository: [https://github.com/apache/incubator-seata](https://github.com/apache/incubator-seata)\\n\\nArticle Authors:\\n\\nLi Zhao, GitHub ID @CoffeeLatte007, author of the public account \\"\u5496\u5561\u62ff\u94c1\\", Seata community Committer, Java engineer at Yuanfudao, formerly employed at Meituan. Has a strong interest in distributed middleware and distributed systems.\\nJi Min (Qingming), GitHub ID @slievrly, Seata open source project leader, core R&D member of Alibaba middleware TXC/GTS, has long been engaged in core R&D work of distributed middleware, and has rich technical accumulation in the field of distributed transactions."},{"id":"/tcc-mode-applicable-scenario-analysis","metadata":{"permalink":"/blog/tcc-mode-applicable-scenario-analysis","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/tcc-mode-applicable-scenario-analysis.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/tcc-mode-applicable-scenario-analysis.md","title":"Analysis of Applicable Models and Scenarios for TCC","description":"Fescar 0.4.0 version released the TCC model, contributed by the ant gold service team, welcome to try, the end of the article also provides the project follow-up Roadmap, welcome to pay attention.","date":"2019-03-27T00:00:00.000Z","formattedDate":"March 27, 2019","tags":[],"readingTime":12.005,"hasTruncateMarker":false,"authors":[{"name":"zhangthen"}],"frontMatter":{"title":"Analysis of Applicable Models and Scenarios for TCC","author":"zhangthen","date":"2019/03/27","keywords":["seata","distributed transaction","TCC","roadmap"]},"unlisted":false,"prevItem":{"title":"In-Depth Analysis of One-Stop Distributed Transaction Solution Seata-Server","permalink":"/blog/seata-analysis-java-server"},"nextItem":{"title":"Introduction to TCC Theory and Design Implementation Guide","permalink":"/blog/tcc-mode-design-principle"}},"content":"Fescar 0.4.0 version released the TCC model, contributed by the ant gold service team, welcome to try, the end of the article also provides the project follow-up Roadmap, welcome to pay attention.\\n<a name=\\"2143093f\\"></a>\\n\\n## Preface: Application scenarios based on TCC model\\n <br />\\n![1.png](/img/blog/TCC1.png)<br />\\n\\nThe TCC distributed transaction model acts directly on the service layer. It is not coupled with the specific service framework, has nothing to do with the underlying RPC protocol, has nothing to do with the underlying storage media, can flexibly choose the locking granularity of the business resources, reduces the resource locking holding time, has good scalability, and can be said to be designed for independently deployed SOA services.\\n\\n\\n<a name=\\"d9b462de\\"></a>\\n\\n## I. TCC model advantages\\n\\nFor TCC distributed transaction model, I think its application in business scenarios, there are two aspects of significance.\\n\\n<a name=\\"95179108\\"></a>\\n\\n### 1.1 Distributed transaction across services\\n\\nThe splitting of services can also be thought of as horizontal scaling of resources, only in a different direction.\\n\\nHorizontal extensions may go along two directions:\\n\\n1. functional scaling, where data is grouped according to function and different functional groups are distributed over multiple different databases, which is effectively servitisation under the SOA architecture.\\n1. data sharding, which adds a new dimension to horizontal scaling by splitting data across multiple databases within functional groups.\\n\\nThe following figure briefly illustrates the horizontal data scaling strategy:\\n\\n![2.png](/img/blog/TCC2.png)\\n\\nTherefore, one of the roles of TCC is to ensure the transaction property of multi-resource access when scaling resources horizontally by function.\\n\\n<a name=\\"240e4d15\\"></a>\\n\\n### 1.2 Two-stage splitting\\n\\nAnother effect of TCC is that it splits the two phases into two separate phases that are related by means of resource business locking. The advantage of resource locking is that it does not block other transactions from continuing to use the same resources in the first phase, nor does it affect the correct execution of the second phase of the transaction.\\n\\n**The traditional model of concurrent transactions:**<br />\\n![3.png](/img/blog/TCC3.png)\\n\\n**Concurrent transactions for the TCC model:**<br />\\n![4.png](/img/blog/TCC4.png)\\n\\nHow does this benefit the business? Taking the secured transaction scenario of Alipay, the simplified case involves only two services, the transaction service and the billing service. The transaction service is the main business service, and the accounting service is the slave business service, which provides the Try, Commit, and Cancel interfaces:\\n\\n1. The Try interface deducts the user\'s available funds and transfers them to pre-frozen funds. Pre-frozen funds is the business locking programme, each transaction can only use the pre-frozen funds of this transaction in the second phase, and other concurrent transactions can continue to process the user\'s available funds after the first phase of execution.\\n1. The Commit interface deducts the pre-frozen funds and increases the funds available in the intermediate account (secured transactions do not immediately credit the merchant and require an intermediate account for suspense).\\n\\nAssuming there is only one intermediary account, every time the Commit interface of the payment service is called, it locks the intermediary account, and there are hotspot performance issues with the intermediary account. However, in the secured transaction scenario, the funds need to be transferred from the intermediate account to the merchant only after seven days, and the intermediate account does not need to be displayed to the public. Therefore, after executing the first stage of the payment service, it can be considered that the payment part of this transaction has been completed and return the result of successful payment to the user and the merchant, and does not need to execute the Commit interface of the second stage of the payment service right away, and wait until the low-frontal period, and then slowly digest it and execute it asynchronously. <br />\\n![5.png](/img/blog/TCC5.png)\\n\\nThis is the two-phase asynchronisation feature of TCC distributed transaction model, the first phase of execution from the business service is successful, the master business service can be committed to complete, and then the framework asynchronously execute the second phase of each slave business service.\\n\\n<a name=\\"ad5fc026\\"></a>\\n\\n## General-purpose TCC solution\\n\\nThe generic TCC solution is the most typical implementation of the TCC distributed transaction model, where all the slave business services need to participate in the decision making of the master business service. <br />\\n![6.png](/img/blog/TCC6.png)<br />\\n<a name=\\"62b37e99\\"></a>\\n\\n### Applicable scenarios\\n\\nSince the slave business services are invoked synchronously and their results affect the decisions of the master business service, the generic TCC distributed transaction solution is suitable for businesses with deterministic and short execution times, such as the three most core services of an Internet financial enterprise: transaction, payment, and accounting:<br />\\n![7.png](/img/blog/TCC7.png)<br /> <br /> When a user initiates a transaction, the transaction service is accessed first to create the transaction order; then the transaction service calls the payment service to create the payment order for the transaction and performs the collection action, and finally, the payment service calls the billing service to record the account flow and bookkeeping.\\n\\nIn order to ensure that the three services work together to complete a transaction, either succeed or fail at the same time, you can use a general-purpose TCC solution that puts the three services in a distributed transaction, with the transaction as the master service, the payment as the slave service, and the billing as the nested slave service of the payment service, and the atomicity of the transaction is guaranteed by the TCC model. <br />\\n![8.png](/img/blog/TCC8.png)\\n\\nThe Try interface of the payment service creates the payment order, opens a nested distributed transaction, and calls the Try interface of the billing service; the billing service freezes the buyer\'s funds in the Try interface. After the first stage of the call is completed, the transaction is completed, the local transaction is submitted, and the TCC framework completes the second stage of the distributed transaction from the business service.\\n\\nThe second stage of the payment service first calls the Confirm interface of the accounting service to deduct the buyer\'s frozen funds and increase the seller\'s available funds. After the call is successful, the payment service modifies the payment order to the completed state and completes the payment.\\n\\nWhen both payment and billing service phase 2 are finished, the whole distributed transaction is finished.\\n\\n<a name=\\"827f0f82\\"></a>\\n\\n## Asynchronous guaranteed TCC solution\\n\\nThe direct slave service of the asynchronous assured TCC solution is the reliable messaging service, while the real slave service is decoupled by the messaging service and executed asynchronously as the consumer of the messaging service. <br />\\n![9.png](/img/blog/TCC9.png)<br /> <br /> The Reliable Messaging Service needs to provide three interfaces, Try, Confirm, and Cancel. The Try interface pre-sends, and is only responsible for persistently storing the message data; the Confirm interface confirms the sending, and this is when the actual delivery of the message begins The Confirm interface confirms the delivery, which is when the actual delivery of the message begins; and the Cancel interface cancels the delivery and deletes the message data.\\n\\nThe message data of the message service is stored independently and scaled independently, which reduces the coupling between the business service and the messaging system, and achieves the ultimate consistency of the distributed transaction under the premise that the message service is reliable.\\n\\nThis solution increases the maintenance cost of message service, but since message service implements TCC interface instead of slave business service, slave business service doesn\'t need any modification and the access cost is very low.\\n\\n<a name=\\"62b37e99-1\\"></a>\\n\\n### Application scenario\\n\\nSince consuming messages from a business service is an asynchronous process, the execution time is uncertain, which may lead to an increase in the inconsistency time window. Therefore, the Asynchronous Ensured TCC Distributed Transaction Solution is only applicable to some passive businesses that are less sensitive to the final consistency time (the processing result of the slave business service does not affect the decision of the master business service, and only passively receives the decision result of the master business service). For example, the member registration service and the email sending service:<br />\\n![10.png](/img/blog/TCC10.png)<br />\xa0<br />When a user registers for a membership successfully, an email needs to be sent to the user to tell the user that the registration was successful and to prompt the user to activate the membership. But pay attention to two points:\\n\\n1. If the user registration is successful, make sure to send an email to the user;\\n1. if the user\'s registration fails, an email must not be sent to the user.\\n\\nSo again, this requires the membership service and the mail service to ensure atomicity, either both are executed or neither is executed. The difference is that the mail service is only a passive business, it does not affect whether the user can register successfully or not, it only needs to send an email to the user after the user has registered successfully, and the mail service does not need to be involved in the decision making of the activities of the membership service.\\n\\nFor this kind of business scenario, you can use the asynchronous ensured TCC distributed transaction solution, as follows:<br />\\n![11.png](/img/blog/TCC11.png)<br />\xa0<br />\xa0<br /> The reliable messaging service decouples the member and mail services, and the member service and the messaging service comprise the TCC transaction model, which ensures the atomicity of transactions. Then through the reliable feature of the message service, it ensures that the message can definitely be consumed by the mail service, so that the member and the mail service are in the same distributed transaction. At the same time, the mail service will not affect the execution process of the member service, and will only passively receive the request to send mail after the member service is executed successfully.\\n\\n<a name=\\"69910d05\\"></a>\\n\\n## Compensated TCC solution\\n\\nCompensated TCC solution is similar in structure to generic TCC solution, and its slave services also need to participate in the decision making of the main business service. However, the difference is that the former slave service only needs to provide Do and Compensate two interfaces, while the latter needs to provide three interfaces. <br />\\n![12.png](/img/blog/TCC12.png)<br />\xa0<br /> The Do interface directly executes the real complete business logic, completes the business processing, and the result of the business execution is visible externally; the Compensate operation is used for the business compensation, which offsets or partially offsets the business result of the positive business operation. Compensate operation needs to satisfy idempotency. <br />Compensate operation is used to offset or partially offset the business results of positive business operations, and the Compensate operation needs to satisfy idempotency. <br />Compared with the general-purpose solution, Compensate solution does not need to transform the original business logic from the business service, and only needs to add an additional Compensate rollback logic, which is a lesser business transformation. However, it is important to note that the business executes the entire business logic in one phase and cannot achieve effective transaction isolation. When rollback is required, there may be a compensation failure, and additional exception handling mechanisms, such as manual intervention, are also required.\\n\\n<a name=\\"62b37e99-2\\"></a>\\n\\n### Applicable scenarios\\n\\nDue to the existence of rollback compensation failure, the compensated TCC distributed transaction solution is only applicable to some of the less concurrent conflict or need to interact with external business, these external business is not a passive business, its execution results will affect the decision of the main business service, such as the ticket booking service of the air ticket agency:<br />\\n![13.png](/img/blog/TCC13.png)<br /> <br /> This air ticket service provides multi-destination air ticket booking service, which can book air tickets for multiple itinerary flights at the same time, e.g., to travel from Beijing to St. Petersburg, it is necessary to take the first journey from Beijing to Moscow, as well as the second journey from Moscow to St. Petersburg.\\n\\nWhen a user books a ticket, he/she would definitely want to book tickets for both flights at the same time, and booking only one flight does not make sense for the user. Therefore, such a business service also imposes the atomicity requirement that if the booking for one of the flights fails, the other flight needs to be able to be cancelled.\\n\\nHowever, it is extremely difficult to push the airlines to change as they are external to the ticket agents and only provide booking and cancellation interfaces. Therefore, for this type of business service, a compensated TCC distributed transaction solution can be used, as follows:<br />\\n![14.png](/img/blog/TCC14.png)\\n\\nThe gateway service adds the Compensate interface on top of the original logic, which is responsible for calling the cancellation interface of the corresponding airline.\\n\\nWhen the user initiates a ticket booking request, the ticket service first calls the booking interface of each airline through the Do interface of the gateway, and if all flights are booked successfully, the whole distributed transaction is executed successfully; once the booking of tickets for a certain flight fails, the distributed transaction is rolled back, and the Compensate interface of each gateway is called by the TCC transaction framework, which then calls the corresponding airline\'s The TCC transaction framework calls the Compensate compensation interface of each gateway, which then calls the corresponding airline\'s cancellation interface. In this way, the atomicity of multi-way ticket booking service can also be guaranteed.\\n\\n<a name=\\"acfe75a0\\"></a>\\n\\n## V. Summary\\n\\nFor today\'s Internet applications, horizontal scaling of resources provides more flexibility and is a relatively easy to implement outward scaling solution, but at the same time, it also significantly increases the complexity and introduces some new challenges, such as data consistency issues between resources.\\n\\nHorizontal data scaling can be done both by data slicing and by functionality. the TCC model ensures the transactional properties of multi-resource access while scaling resources horizontally by functionality.\\n\\nTCC model in addition to the role of cross-service distributed transactions this layer , but also has a two-stage division of the function , through the business resource locking , allowing the second stage of asynchronous execution , and asynchronous idea is to solve the hot spot data concurrency performance problems of one of the tools . <br />\\n<a name=\\"Roadmap\\"></a>\\n\\n## Roadmap\\n\\nCurrently, we have released 0.4.0, and we will release 0.5 ~ 1.0 to continue to improve and enrich the functionality of AT and TCC modes, and to solve the problem of high availability of the server side. After 1.0, this open source product will reach the standard of production environment. <br /><br /><br />![image1.png](/img/blog/roadmap.png)<br />"},{"id":"/tcc-mode-design-principle","metadata":{"permalink":"/blog/tcc-mode-design-principle","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/tcc-mode-design-principle.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/tcc-mode-design-principle.md","title":"Introduction to TCC Theory and Design Implementation Guide","description":"Fescar 0.4.0 version released the TCC schema, contributed by the Anthem team, you are welcome to try it out,Sample address//github.com/fescar-group/fescar-samples/tree/master/tcc,At the end of this article, we also provide the roadmap of the project, welcome to follow.","date":"2019-03-26T00:00:00.000Z","formattedDate":"March 26, 2019","tags":[],"readingTime":6.18,"hasTruncateMarker":false,"authors":[{"name":"zhangthen"}],"frontMatter":{"title":"Introduction to TCC Theory and Design Implementation Guide","author":"zhangthen","date":"2019/03/26","keywords":["fescar","distributed transaction","TCC","roadmap"]},"unlisted":false,"prevItem":{"title":"Analysis of Applicable Models and Scenarios for TCC","permalink":"/blog/tcc-mode-applicable-scenario-analysis"},"nextItem":{"title":"How to use Seata to ensure consistency between Dubbo Microservices","permalink":"/blog/quick-start-use-seata-and-dubbo-services"}},"content":"Fescar 0.4.0 version released the TCC schema, contributed by the Anthem team, you are welcome to try it out,<br />Sample address:[https://github.com/fescar-group/fescar-samples/tree/master/tcc](https. //github.com/fescar-group/fescar-samples/tree/master/tcc),<br />At the end of this article, we also provide the roadmap of the project, welcome to follow.\\n\\n<a name=\\"f1d2fc6a\\"></a>\\n\\n### I. Introduction to TCC\\n\\nIn the Two Phase Commitment Protocol (2PC), the resource manager (RM, resource manager) needs to provide three functions: \\"prepare\\", \\"commit\\" and \\"rollback\\". \\"Rollback\\" 3 operations; while the transaction manager (TM, transaction manager) coordinates all resource managers in 2 phases, in the first phase asks all resource managers whether the \\"preparation\\" is successful, if all resources are If all resources are \\"ready\\" successfully, then perform \\"commit\\" operation of all resources in the second phase, otherwise perform \\"rollback\\" operation of all resources in the second phase to ensure that the final state of all resources is the same, either all commits or all commits, or the final state of all resources is the same. to ensure that the final state of all resources is the same, either all commit or all rollback.\\n\\nResource Manager has many implementations, among which TCC (Try-Confirm-Cancel) is a service-based implementation of Resource Manager; TCC is a relatively mature distributed transaction solution that can be used to solve the data consistency problem of cross-database and cross-service business operations; TCC\'s Try, Confirm and Cancel methods are implemented by business code. TCC\'s Try, Confirm, and Cancel methods are all implemented by business code, so TCC can be called a service-based resource manager.\\n\\nThe Try operation of TCC is the first stage, which is responsible for checking and reserving resources; Confirm operation is the second stage, which is the submit operation to execute the real business; Cancel is the second stage, which is the rollback operation, which is the cancellation of the reserved resources to return the resources to the initial state.\\n\\nAs shown in the figure below, after the user implements a TCC service, the TCC service will be one of the resources of the distributed transaction, participating in the whole distributed transaction; the transaction manager coordinates the TCC services in two stages, calling the Try method of all TCC services in the first stage, and executing the Confirm or Cancel method of all TCC services in the second stage; eventually all TCC services are either committed or cancelled; all TCC services are either committed or cancelled. services are either all committed or all rolled back.\\n\\n![image.png](/img/blog/tcc.png)\\n\\n<a name=\\"48153343\\"></a>\\n\\n### II. TCC Design\\n\\nWhen users access TCC, most of the work is focused on how to implement TCC service, after years of TCC application by Anthem, the following main TCC design and implementation of the main matters are summarised below:\\n\\n<a name=\\"4226dc7c\\"></a>\\n\\n#### 1, **Business operation is completed in two stages**\\n\\nBefore connecting to TCC, business operation can be completed in one step only, but after connecting to TCC, we need to consider how to divide it into 2 phases to complete, put the resource checking and reserving in Try operation in the first phase, and put the execution of real business operation in Confirm operation in the second phase.\\n\\nBelow is an example of how the business model can be designed in two phases. Example scenario: \\"Account A has a balance of $100, of which $30 needs to be deducted\\";\\n\\nBefore accessing TCC, the user could write SQL: \\"update account table set balance = balance - 30 where account = A\\" to complete the deduction operation in one step.\\n\\nAfter connecting to TCC, you need to consider how to split the debit operation into 2 steps:\\n\\n* Try operation: checking and reserving resources;\\n\\nIn the deduction scenario, what Try operation has to do is to check whether the balance of A account is enough, and then freeze the $30 to be deducted (reserved resources); no real deduction will happen at this stage.\\n\\n* Confirm operation: performs the submission of the real operation;\\n\\nIn the deduction scenario, the Confirm phase takes place when the real deduction occurs, deducting the $30 already frozen in A\'s account.\\n\\n* Cancel operation: whether or not the reserved resource is released;\\n\\nIn a debit scenario, the debit is cancelled and the Cancel operation performs the task of releasing the $30 that was frozen by the Try operation, returning Account A to its initial state.\\n\\n![image.png](/img/blog/tow_step.png)\\n\\n\\n<a name=\\"bce861f1\\"></a>\\n\\n#### 2, **Concurrency Control**\\n\\nUsers should consider concurrency issues when implementing TCC and minimise lock granularity to maximise concurrency in distributed transactions.\\n\\nThe following is still an example of deducting money from account A. \\"There is $100 on account A. Transaction T1 has to deduct $30 of it, and transaction T2 also has to deduct $30, and there is concurrency\\".\\n\\nIn the first phase of the Try operation, distributed transaction T1 and distributed transaction T2 are freezing that part of the funds without interfering with each other; so that in the second phase of the distributed transaction, no matter whether T1 is a commit or a rollback, there will be no impact on T2, so that T1 and T2 are executing in parallel on the same piece of business data.\\n\\n![image.png](/img/blog/conc.png) <br />\\n\\n<a name=\\"e945e352\\"></a>\\n\\n#### 3, **Allow empty rollback**\\n\\nAs shown in the following figure, when the transaction coordinator invokes the first-phase Try operation of the TCC service, there may be a network timeout due to packet loss, and at this time the transaction manager triggers a two-phase rollback to invoke the Cancel operation of the TCC service, which is invoked without a timeout.\\n\\nThe TCC service receives a Cancel request without receiving a Try request, this scenario is called a null rollback; null rollbacks often occur in production environments, and users should allow for null rollbacks when implementing TCC services, i.e., return success when receiving a null rollback.\\n\\n![image.png](/img/blog/empty_rollback.png)\\n\\n<a name=\\"e02f3ee9\\"></a>\\n\\n#### 4. Anti-suspension control\\n\\nAs shown in the figure below, when the transaction coordinator calls the TCC service\'s one-phase Try operation, there may be a timeout due to network congestion, at this time, the transaction manager will trigger a two-phase rollback and call the TCC service\'s Cancel operation, and the Cancel call is not timed out; after this, the one-phase Try packet that is congested in the network is received by the TCC service, and there is a two-phase After this, the first-phase Try packet on the congested network is received by the TCC service, and the second-phase Cancel request is executed before the first-phase Try request, and the TCC service will never receive the second-phase Confirm or Cancel after executing the late Try, resulting in the suspension of the TCC service.\\n\\nWhen you implement TCC service, you should allow empty rollback, but refuse to execute Try request after empty rollback to avoid hanging.\\n\\n![image.png](/img/blog/susp.png)\\n\\n\\n<a name=\\"5322a3d5\\"></a>\\n\\n#### 5. Idempotent control\\n\\nWhether it is network packet retransmission or compensation execution of abnormal transaction, it will lead to the Try, Confirm or Cancel operation of TCC service to be executed repeatedly; users need to consider idempotent control when implementing TCC service, i.e., the business result of Try, Confirm, Cancel executed once and executed many times is the same. <br />![image.png](/img/blog/miden.png)<br /><br />\\n\\n<a name=\\"Roadmap\\"></a>\\n\\n### Roadmap\\n\\nCurrently we have released version 0.4.0, we will release version 0.5 ~ 1.0, continue to improve and enrich the functions of AT, TCC mode, and solve the problem of high availability of the server side, after version 1.0, this open source product will reach the standard of production environment.\\n\\n\\n![image1.png](/img/blog/roadmap.png)"},{"id":"/quick-start-use-seata-and-dubbo-services","metadata":{"permalink":"/blog/quick-start-use-seata-and-dubbo-services","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/quick-start-use-seata-and-dubbo-services.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/quick-start-use-seata-and-dubbo-services.md","title":"How to use Seata to ensure consistency between Dubbo Microservices","description":"This article will introduce you how to use Seata to ensure consistency between Dubbo Microservices.","date":"2019-03-07T00:00:00.000Z","formattedDate":"March 7, 2019","tags":[],"readingTime":2.84,"hasTruncateMarker":false,"authors":[{"name":"slievrly"}],"frontMatter":{"title":"How to use Seata to ensure consistency between Dubbo Microservices","keywords":["Dubbo","Seata","Consistency"],"description":"This article will introduce you how to use Seata to ensure consistency between Dubbo Microservices.","author":"slievrly","date":"2019-03-07T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Introduction to TCC Theory and Design Implementation Guide","permalink":"/blog/tcc-mode-design-principle"},"nextItem":{"title":"Unveiling the Principles of Fescar Distributed Transaction","permalink":"/blog/seata-analysis-simple"}},"content":"## Use case\\n\\nA business logic for user purchasing commodities. The whole business logic is powered by 3 microservices:\\n\\n- Storage service: deduct storage count on given commodity.\\n- Order service: create order according to purchase request.\\n- Account service: debit the balance of user\'s account.\\n\\n### Architecture\\n\\n![Architecture](/img/blog/seata/seata-1.png) \\n\\n\\n### StorageService\\n\\n```java\\npublic interface StorageService {\\n\\n    /**\\n     * deduct storage count\\n     */\\n    void deduct(String commodityCode, int count);\\n}\\n```\\n\\n### OrderService\\n\\n```java\\npublic interface OrderService {\\n\\n    /**\\n     * create order\\n     */\\n    Order create(String userId, String commodityCode, int orderCount);\\n}\\n```\\n\\n### AccountService\\n\\n```java\\npublic interface AccountService {\\n\\n    /**\\n     * debit balance of user\'s account\\n     */\\n    void debit(String userId, int money);\\n}\\n```\\n\\n### Main business logic\\n\\n```java\\npublic class BusinessServiceImpl implements BusinessService {\\n\\n    private StorageService storageService;\\n\\n    private OrderService orderService;\\n\\n    /**\\n     * purchase\\n     */\\n    public void purchase(String userId, String commodityCode, int orderCount) {\\n\\n        storageService.deduct(commodityCode, orderCount);\\n\\n        orderService.create(userId, commodityCode, orderCount);\\n    }\\n}\\n```\\n\\n```java\\npublic class StorageServiceImpl implements StorageService {\\n\\n  private StorageDAO storageDAO;\\n  \\n    @Override\\n    public void deduct(String commodityCode, int count) {\\n        Storage storage = new Storage();\\n        storage.setCount(count);\\n        storage.setCommodityCode(commodityCode);\\n        storageDAO.update(storage);\\n    }\\n}\\n```\\n\\n```java\\npublic class OrderServiceImpl implements OrderService {\\n\\n    private OrderDAO orderDAO;\\n\\n    private AccountService accountService;\\n\\n    public Order create(String userId, String commodityCode, int orderCount) {\\n\\n        int orderMoney = calculate(commodityCode, orderCount);\\n\\n        accountService.debit(userId, orderMoney);\\n\\n        Order order = new Order();\\n        order.userId = userId;\\n        order.commodityCode = commodityCode;\\n        order.count = orderCount;\\n        order.money = orderMoney;\\n\\n        return orderDAO.insert(order);\\n    }\\n}\\n```\\n\\n## Distributed Transaction Solution with Seata\\n\\n![undefined](/img/blog/seata/seata-2.png) \\n\\nWe just need an annotation `@GlobalTransactional` on business method: \\n\\n```java\\n\\n    @GlobalTransactional\\n    public void purchase(String userId, String commodityCode, int orderCount) {\\n        ......\\n    }\\n```\\n\\n## Example powered by Dubbo + Seata\\n\\n### Step 1: Setup database\\n\\n- Requirement: MySQL with InnoDB engine.\\n\\n**Note:** In fact, there should be 3 database for the 3 services in the example use case. However, we can just create one database and configure 3 data sources for simple. \\n\\nModify Spring XML with the database URL/username/password you just created.\\n\\ndubbo-account-service.xml\\ndubbo-order-service.xml\\ndubbo-storage-service.xml\\n\\n```xml\\n    <property name=\\"url\\" value=\\"jdbc:mysql://x.x.x.x:3306/xxx\\" />\\n    <property name=\\"username\\" value=\\"xxx\\" />\\n    <property name=\\"password\\" value=\\"xxx\\" />\\n```\\n### Step 2: Create UNDO_LOG table for Seata\\n\\n`UNDO_LOG` table is required by Seata AT mode.\\n\\n```sql\\nCREATE TABLE `undo_log` (\\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\\n  `branch_id` bigint(20) NOT NULL,\\n  `xid` varchar(100) NOT NULL,\\n  `rollback_info` longblob NOT NULL,\\n  `log_status` int(11) NOT NULL,\\n  `log_created` datetime NOT NULL,\\n  `log_modified` datetime NOT NULL,\\n  `ext` varchar(100) DEFAULT NULL,\\n  PRIMARY KEY (`id`),\\n  KEY `idx_unionkey` (`xid`,`branch_id`)\\n) ENGINE=InnoDB AUTO_INCREMENT=159 DEFAULT CHARSET=utf8\\n```\\n\\n### Step 3: Create tables for example business\\n\\n```sql\\n\\nDROP TABLE IF EXISTS `storage_tbl`;\\nCREATE TABLE `storage_tbl` (\\n  `id` int(11) NOT NULL AUTO_INCREMENT,\\n  `commodity_code` varchar(255) DEFAULT NULL,\\n  `count` int(11) DEFAULT 0,\\n  PRIMARY KEY (`id`),\\n  UNIQUE KEY (`commodity_code`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\\n\\n\\nDROP TABLE IF EXISTS `order_tbl`;\\nCREATE TABLE `order_tbl` (\\n  `id` int(11) NOT NULL AUTO_INCREMENT,\\n  `user_id` varchar(255) DEFAULT NULL,\\n  `commodity_code` varchar(255) DEFAULT NULL,\\n  `count` int(11) DEFAULT 0,\\n  `money` int(11) DEFAULT 0,\\n  PRIMARY KEY (`id`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\\n\\n\\nDROP TABLE IF EXISTS `account_tbl`;\\nCREATE TABLE `account_tbl` (\\n  `id` int(11) NOT NULL AUTO_INCREMENT,\\n  `user_id` varchar(255) DEFAULT NULL,\\n  `money` int(11) DEFAULT 0,\\n  PRIMARY KEY (`id`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\\n```\\n### Step 4: Start Seata-Server\\n\\n- Download server [package](https://github.com/apache/incubator-seata/releases), unzip it.\\n- Start Seata-Server\\n\\n```shell\\nsh seata-server.sh $LISTEN_PORT $PATH_FOR_PERSISTENT_DATA\\n\\ne.g.\\n\\nsh seata-server.sh 8091 /home/admin/seata/data/\\n```\\n\\n### Step 5: Run example\\n\\n- Start AccountService ([DubboAccountServiceStarter](https://github.com/apache/incubator-seata-samples/blob/master/dubbo/src/main/java/com/seata/seata/samples/dubbo/starter/DubboAccountServiceStarter.java)).\\n- Start StorageService ([DubboStorageServiceStarter](https://github.com/apache/incubator-seata-samples/blob/master/dubbo/src/main/java/com/seata/seata/samples/dubbo/starter/DubboStorageServiceStarter.java)).\\n- Start OrderService ([DubboOrderServiceStarter](https://github.com/apache/incubator-seata-samples/blob/master/dubbo/src/main/java/com/seata/seata/samples/dubbo/starter/DubboOrderServiceStarter.java)).\\n- Run BusinessService for test ([DubboBusinessTester](https://github.com/apache/incubator-seata-samples/blob/master/dubbo/src/main/java/com/seata/seata/samples/dubbo/starter/DubboBusinessTester.java)).\\n\\n### Related projects\\n* seata:          https://github.com/apache/incubator-seata/\\n* seata-samples : https://github.com/apache/incubator-seata-samples"},{"id":"/seata-analysis-simple","metadata":{"permalink":"/blog/seata-analysis-simple","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-analysis-simple.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/seata-analysis-simple.md","title":"Unveiling the Principles of Fescar Distributed Transaction","description":"Fescar has been released for a while, and distributed transactions have always been a highly focused area in the industry. Fescar received nearly 5000 stars within a month of its release, indicating its popularity. Of course, before Fescar, there were already relatively mature open-source distributed transaction solutions, such as the 2PC non-intrusive transaction of LCN, which has now evolved to version 5.0 and supports TCX transactions similar to Fescar\'s transaction model. Other implementations of TCC transactions include hmily and tcc-transaction. In the current era of microservice architecture, and given Alibaba\'s extensive background in open source, the release of Fescar has undoubtedly sparked a new wave of research into distributed transactions. Fescar originated from Alibaba Cloud\'s commercial distributed transaction service GTS, a model that has undergone rigorous testing in online environments. The TXC distributed transaction model of Fescar is similar to the traditional XA transaction model, with the main difference being the positioning of the resource manager\u2014one at the application layer and the other at the database layer. The author believes that Fescar\'s TXC model implementation is of significant research value, so today we will thoroughly explore the Fescar project\'s code. This article is lengthy and will take about 30-60 minutes to read and understand.","date":"2019-02-18T00:00:00.000Z","formattedDate":"February 18, 2019","tags":[],"readingTime":16.97,"hasTruncateMarker":false,"authors":[{"name":"kailin.chen"}],"frontMatter":{"title":"Unveiling the Principles of Fescar Distributed Transaction","author":"kailin.chen","keywords":["Fescar","distributed transaction"],"date":"2019/02/18"},"unlisted":false,"prevItem":{"title":"How to use Seata to ensure consistency between Dubbo Microservices","permalink":"/blog/quick-start-use-seata-and-dubbo-services"},"nextItem":{"title":"MT mode","permalink":"/blog/manual-transaction-mode"}},"content":"Fescar has been released for a while, and distributed transactions have always been a highly focused area in the industry. Fescar received nearly 5000 stars within a month of its release, indicating its popularity. Of course, before Fescar, there were already relatively mature open-source distributed transaction solutions, such as the 2PC non-intrusive transaction of [LCN](https://github.com/codingapi/tx-lcn), which has now evolved to version 5.0 and supports TCX transactions similar to Fescar\'s transaction model. Other implementations of TCC transactions include [hmily](https://github.com/yu199195/hmily) and [tcc-transaction](https://github.com/changmingxie/tcc-transaction). In the current era of microservice architecture, and given Alibaba\'s extensive background in open source, the release of Fescar has undoubtedly sparked a new wave of research into distributed transactions. Fescar originated from Alibaba Cloud\'s commercial distributed transaction service GTS, a model that has undergone rigorous testing in online environments. The TXC distributed transaction model of Fescar is similar to the traditional XA transaction model, with the main difference being the positioning of the resource manager\u2014one at the application layer and the other at the database layer. The author believes that Fescar\'s TXC model implementation is of significant research value, so today we will thoroughly explore the Fescar project\'s code. This article is lengthy and will take about 30-60 minutes to read and understand.\\n\\n# Project Address\\n\\nFescar: https://github.com/alibaba/fescar\\n\\nThe code discussed in this blog post is from the 0.1.2-SNAPSHOT version of Fescar. As Fescar evolves, the project structure and module implementations might change significantly.\\n\\n# Fescar\'s TXC Model\\n\\n![](/img/blog/c45496461bca15ecd522e497d98ba954f95.jpg)\\n\\nThe above image is an official schematic of the TXC model created by Fescar. The quality of visuals produced by large companies is indeed impressive. From the schematic, we can see the overall implementation of TXC. TXC is implemented through three components, as depicted in the three dark yellow sections in the image, with the following roles:\\n\\n1.  TM: Global Transaction Manager, which starts the Fescar distributed transaction on the server side and sends the global transaction to the TC (Transaction Coordinator) for management.\\n2.  TC: Transaction Coordinator, which controls the global transaction\'s commit or rollback. This component requires independent deployment and maintenance, currently only supporting a single-machine version. Future iterations plan to include a clustered version.\\n3.  RM: Resource Manager, mainly responsible for reporting branch transactions and managing local transactions.\\n\\nA brief description of its implementation process: The initiating service starts a global transaction and registers it with the TC. When calling a cooperating service, the branch transaction of the cooperating service completes the first phase of transaction commit or rollback and generates an undo_log for transaction rollback, then registers the current cooperating service with the TC and reports its transaction status, merging it into the global transaction of the same business. If no issues arise, it proceeds to the next cooperating service call. If any branch transaction of the cooperating service rolls back, it will notify the TC, which then notifies all branch transactions of the global transaction that have completed the first phase to roll back. If all branch transactions proceed normally, it will notify the TC when returning to the global transaction initiator, and the TC will notify all branches of the global transaction to delete the rollback logs. To solve write isolation and degree isolation issues during this process, global locks managed by the TC will be involved.\\n\\nThe goal of this blog post is to delve into the code details and explore how its basic ideas are implemented. We will first outline the role of each module from the project\'s structure, then investigate the entire distributed transaction implementation process using the official examples.\\n\\n# Project Structure Analysis\\n\\nAfter pulling the project and opening it with an IDE, the directory structure is as follows. Let\'s take a look at the implementation of each module:\\n\\n![](/img/blog/a88cf147f489f913886ef1785d94183bf09.jpg)\\n\\n-   common: Common components, providing commonly used utility classes, static variables, extension mechanism class loaders, and defining global exceptions, etc.\\n-   config: Configuration loading and parsing module, providing basic interfaces for configuration. Currently, only file configuration is implemented, with plans for implementations of configuration centers like Nacos.\\n-   core: The core module mainly encapsulating RPC-related content for communication between TM, RM, and TC.\\n-   dubbo: The Dubbo module mainly adapts the Dubbo communication framework, using Dubbo\'s filter mechanism to pass global transaction information to branches.\\n-   examples: A simple example module that we will explore to understand the implementation.\\n-   rm-datasource: The resource management module, a core module that proxies some JDBC classes to parse SQL, generate rollback logs, and coordinate local transactions. Personally, I think naming this module \\"core\\" would be more appropriate.\\n-   server: The TC component, mainly coordinating and managing global transactions, responsible for committing or rolling back global transactions, and maintaining global locks.\\n-   spring: The module integrated with Spring, mainly consisting of AOP logic, serving as the entry point for the entire distributed transaction, and the breakthrough point for studying Fescar.\\n-   tm: The global transaction management module, managing the boundaries of global transactions, and controlling the initiation and rollback points of global transactions.\\n\\n# Viewing the Effects through the [examples] Module\\n\\nFirst, start the TC (Server) module, and start the main method directly. The default server port is 8091.\\n\\nSecond, go to the examples module and configure the configuration files for the order, business, account, and storage services, mainly the MySQL data source and Zookeeper connection address. Note that the default Dubbo Zookeeper registry dependency is missing, and starting it will throw a class not found exception. Add the following dependency:\\n\\n```xml\\n<dependency>\\n    <groupId>com.101tec</groupId>\\n    <artifactId>zkclient</artifactId>\\n    <version>0.10</version>\\n    <exclusions>\\n        <exclusion>\\n            <artifactId>slf4j-log4j12</artifactId>\\n            <groupId>org.slf4j</groupId>\\n        </exclusion>\\n    </exclusions>\\n</dependency>\\n```\\n\\nThird, place a breakpoint at the simulated exception in BusinessServiceImpl. Start OrderServiceImpl, StorageServiceImpl, AccountServiceImpl, and BusinessServiceImpl services one by one. After hitting the breakpoint, check the account_tbl table in the database; the amount has been reduced by 400 yuan, to 599 yuan. Then, release the breakpoint to trigger the simulated exception in the BusinessServiceImpl module. The global transaction rolls back, and the account_tbl table amount returns to 999 yuan.\\n\\nAs shown above, we have experienced the control capability of Fescar transactions. Next, let\'s look at how it controls transactions in detail.\\n\\n# Analysis of Fescar Transaction Process\\n\\n## First, Analyze the Configuration File\\n\\nThis is a golden rule: to integrate any technology or framework, the configuration file is definitely a breakthrough point. From the above example, we learned that the configuration file in the example module configured an instance of a global transaction scanner, as follows:\\n\\n```xml\\n<bean class=\\"com.alibaba.fescar.spring.annotation.GlobalTransactionScanner\\">\\n    <constructor-arg value=\\"dubbo-demo-app\\"/>\\n    <constructor-arg value=\\"my_test_tx_group\\"/>\\n</bean>\\n```\\n\\nThis instance scans all instances when the project starts. The specific implementation can be found in the [spring] module, and methods marked with the @GlobalTransactional annotation are woven into the logic of GlobalTransactionalInterceptor\'s invoke method. When the application starts, instances of TM (TmRpcClient) and RM (RmRpcClient) are initialized, connecting the service with the TC (Transaction Coordinator). Going further involves the TransactionalTemplate class in the TM module.\\n\\n## [TM] Module Starts Global Transactions\\n\\nThe opening, committing, and rolling back of global transactions are encapsulated in the TransactionalTemplate. The code is as follows:\\n```java\\n\\npublic Object execute(TransactionalExecutor business) throws TransactionalExecutor.ExecutionException {\\n    // 1. get or create a transaction\\n    GlobalTransaction tx = GlobalTransactionContext.getCurrentOrCreate();\\n    // 2. begin transaction\\n    try {\\n        tx.begin(business.timeout(), business.name());\\n    } catch (TransactionException txe) {\\n        throw new TransactionalExecutor.ExecutionException(tx, txe,\\n            TransactionalExecutor.Code.BeginFailure);\\n    }\\n    Object rs = null;\\n    try {\\n        // Do Your Business\\n        rs = business.execute();\\n    } catch (Throwable ex) {\\n        // 3. any business exception, rollback.\\n        try {\\n            tx.rollback();\\n            // 3.1 Successfully rolled back\\n            throw new TransactionalExecutor.ExecutionException(tx, TransactionalExecutor.Code.RollbackDone, ex);\\n        } catch (TransactionException txe) {\\n            // 3.2 Failed to rollback\\n            throw new TransactionalExecutor.ExecutionException(tx, txe,\\n                TransactionalExecutor.Code.RollbackFailure, ex);\\n        }\\n    }\\n    // 4. everything is fine, commit.\\n    try {\\n        tx.commit();\\n    } catch (TransactionException txe) {\\n        // 4.1 Failed to commit\\n        throw new TransactionalExecutor.ExecutionException(tx, txe,\\n            TransactionalExecutor.Code.CommitFailure);\\n    }\\n    return rs;\\n}\\n\\n```\\n\\nThe more detailed implementation in the [TM] module is divided into two classes, as follows:\\n\\nDefaultGlobalTransaction: Responsible for the specific actions of starting, committing, and rolling back global transactions.\\n\\nDefaultTransactionManager: Responsible for using TmRpcClient to send commands to the TC control center, such as starting a global transaction (GlobalBeginRequest), committing (GlobalCommitRequest), rolling back (GlobalRollbackRequest), and querying status (GlobalStatusRequest).\\n\\nThe above are the core contents of the TM module. After the TM module completes the global transaction start, we then look at how the global transaction ID, xid, is passed and how the RM component intervenes.\\n\\n## Passing Global Transaction xid with [dubbo]\\n\\nFirst is the transmission of xid. Currently, the transmission in a microservice architecture implemented with the Dubbo framework has been realized. It is also easy to implement for others like Spring Cloud and Motan. By using the filter mechanism that general RPC communication frameworks have, xid is passed from the initiating node of the global transaction to the service\'s subordinate nodes. After being received by the subordinate nodes, it is bound to the current thread context environment to determine whether to join the global transaction when the branch transaction executes SQL. Fescar\'s implementation can be seen in the [dubbo] module as follows:\\n\\n```java\\n@Activate(group = { Constants.PROVIDER, Constants.CONSUMER }, order = 100)\\npublic class TransactionPropagationFilter implements Filter {\\n\\n    private static final Logger LOGGER = LoggerFactory.getLogger(TransactionPropagationFilter.class);\\n\\n    @Override\\n    public Result invoke(Invoker<?> invoker, Invocation invocation) throws RpcException {\\n        String xid = RootContext.getXID();\\n        String rpcXid = RpcContext.getContext().getAttachment(RootContext.KEY_XID);\\n        if (LOGGER.isDebugEnabled()) {\\n            LOGGER.debug(\\"xid in RootContext\\\\[\\" + xid + \\"\\\\] xid in RpcContext\\\\[\\" + rpcXid + \\"\\\\]\\");\\n        }\\n        boolean bind = false;\\n        if (xid != null) {\\n            RpcContext.getContext().setAttachment(RootContext.KEY_XID, xid);\\n        } else {\\n            if (rpcXid != null) {\\n                RootContext.bind(rpcXid);\\n                bind = true;\\n                if (LOGGER.isDebugEnabled()) {\\n                    LOGGER.debug(\\"bind\\\\[\\" + rpcXid + \\"\\\\] to RootContext\\");\\n                }\\n            }\\n        }\\n        try {\\n            return invoker.invoke(invocation);\\n\\n        } finally {\\n            if (bind) {\\n                String unbindXid = RootContext.unbind();\\n                if (LOGGER.isDebugEnabled()) {\\n                    LOGGER.debug(\\"unbind\\\\[\\" + unbindXid + \\"\\\\] from RootContext\\");\\n                }\\n                if (!rpcXid.equalsIgnoreCase(unbindXid)) {\\n                    LOGGER.warn(\\"xid in change during RPC from \\" + rpcXid + \\" to \\" + unbindXid);\\n                    if (unbindXid != null) {\\n                        RootContext.bind(unbindXid);\\n                        LOGGER.warn(\\"bind \\\\[\\" + unbindXid + \\"\\\\] back to RootContext\\");\\n                    }\\n                }\\n            }\\n        }\\n    }\\n}\\n```\\n\\n\\nWhen rpcXid is not null, it is added to the ContextCore of RootContext. Let\'s delve into this a bit. ContextCore is an extensible interface, and the default implementation is ThreadLocalContextCore, which maintains the current xid based on ThreadLocal. Fescar provides an extensible mechanism implemented in the [common] module. Through a custom class loader, EnhancedServiceLoader, it loads the service classes that need to be extended. By adding the @LoadLevel annotation with a high order attribute to the extension class, the purpose of extension implementation can be achieved.\\n\\n## Intervention of Local Resource Management in the [RM] Module\\n\\nFescar implements proxy classes for local transaction-related interfaces through a proxy mechanism, such as DataSourceProxy, ConnectionProxy, and StatementProxy. This can be seen in the configuration file, indicating that to use Fescar distributed transactions, the proxy data source provided by Fescar must be configured. For example:\\n\\n![](/img/blog/af317255c71a5c1bf46f7140387acf365f8.jpg)\\n\\nAfter configuring the proxy data source, starting from DataSourceProxy, we can freely control all local operations on the database. From the xid transmission above, we know that the xid is saved in RootContext. Now, look at the following code to see it clearly:\\n\\nFirst, look at a piece of code from StatementProxy:\\n\\n![](/img/blog/17896deea47b9aee518812b039c39101d8f.jpg)\\n\\nThen, look at the code in ExecuteTemplate:\\n\\n![](/img/blog/04488a2745a5d564498462ed64c506174d2.jpg)\\n\\nSimilar to the transaction management template class TransactionalTemplate in the [TM] module, the crucial logic proxy here is encapsulated in the ExecuteTemplate template class. By overriding Statement with StatementProxy implementation, the execute logic of ExecuteTemplate is called when the original JDBC executeUpdate method is executed. Before the SQL is actually executed, it checks whether the current context in RootContext contains xid, i.e., whether it is a global distributed transaction. If not, the local transaction is used directly. If it is, RM adds some distributed transaction-related logic. Fescar has encapsulated five different executors to handle different types of SQL, namely UpdateExecutor, DeleteExecutor, InsertExecutor, SelectForUpdateExecutor, and PlainExecutor. The structure is as follows:\\n\\n![](/img/blog/bb9a2f07054f19bc21adc332671de4f7b75.jpg)\\n\\n### PlainExecutor:\\n\\nThe native JDBC interface implementation, without any processing, is used for ordinary select queries in global transactions.\\n\\n### UpdateExecutor, DeleteExecutor, InsertExecutor:\\n\\nThe three DML (Data Manipulation Language) executors for updating, deleting, and inserting, mainly analyze the SQL statements before and after execution and implement the following two abstract interface methods:\\n\\n```java\\nprotected abstract TableRecords beforeImage() throws SQLException;\\n\\nprotected abstract TableRecords afterImage(TableRecords beforeImage) throws SQLException;\\n```\\n\\nDuring this process, the undo_log for rollback operation is generated by analyzing the SQL, and the log is currently saved in MySQL, sharing the same transaction with the business SQL operation. The table structure is as follows:\\n\\n![](/img/blog/fd5c423815d3b84bdbc70d7efeb9cd16757.jpg)\\n\\nThe rollback_info column contains the detailed information of the undo_log, which is of type longblob. The structure is as follows:\\n```json\\n{\\n\xa0\xa0\xa0\xa0\\"branchId\\":3958194,\\n\xa0\xa0\xa0\xa0\\"sqlUndoLogs\\":[\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"afterImage\\":{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"rows\\":[\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"fields\\":[\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"keyType\\":\\"PrimaryKey\\",\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"name\\":\\"ID\\",\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"type\\":4,\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"value\\":10\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0},\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"keyType\\":\\"NULL\\",\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"name\\":\\"COUNT\\",\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"type\\":4,\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"value\\":98\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0}\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0]\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0}\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0],\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"tableName\\":\\"storage_tbl\\"\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0},\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"beforeImage\\":{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"rows\\":[\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"fields\\":[\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"keyType\\":\\"PrimaryKey\\",\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"name\\":\\"ID\\",\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"type\\":4,\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"value\\":10\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0},\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0{\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"keyType\\":\\"NULL\\",\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"name\\":\\"COUNT\\",\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"type\\":4,\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"value\\":100\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0}\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0]\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0}\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0],\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"tableName\\":\\"storage_tbl\\"\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0},\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"sqlType\\":\\"UPDATE\\",\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\\"tableName\\":\\"storage_tbl\\"\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0}\\n\xa0\xa0\xa0\xa0],\\n\xa0\xa0\xa0\xa0\\"xid\\":\\"192.168.7.77:8091:3958193\\"\\n}\\n\\n\\n```\\n\\nHere is an example of an update operation. The undo_log records very detailed information. It associates the branch ID with the global transaction xid, records the table name, the operation field names, and the records before and after the SQL execution. For instance, this record shows table name = storage_tbl, before SQL execution ID = 10, count = 100, after SQL execution ID = 10, count = 98. If the entire global transaction fails and needs to be rolled back, it can generate the following rollback SQL statement:\\n\\n```sql\\nupdate storage_tbl set count = 100 where id = 10;\\n```\\n\\n### SelectForUpdateExecutor:\\n\\nIn Fescar\'s AT mode, the default isolation level above the local transaction is read uncommitted. However, through the SelectForUpdateExecutor, it can support the read committed isolation level. The code is as follows:\\n```java\\n@Override\\npublic Object doExecute(Object... args) throws Throwable {\\n    SQLSelectRecognizer recognizer = (SQLSelectRecognizer) sqlRecognizer;\\n\\n    Connection conn = statementProxy.getConnection();\\n    ResultSet rs = null;\\n    Savepoint sp = null;\\n    LockRetryController lockRetryController = new LockRetryController();\\n    boolean originalAutoCommit = conn.getAutoCommit();\\n\\n    StringBuffer selectSQLAppender = new StringBuffer(\\"SELECT \\");\\n    selectSQLAppender.append(getTableMeta().getPkName());\\n    selectSQLAppender.append(\\" FROM \\" + getTableMeta().getTableName());\\n    String whereCondition = null;\\n    ArrayList<Object> paramAppender = new ArrayList<>();\\n    if (statementProxy instanceof ParametersHolder) {\\n        whereCondition = recognizer.getWhereCondition((ParametersHolder) statementProxy, paramAppender);\\n    } else {\\n        whereCondition = recognizer.getWhereCondition();\\n    }\\n    if (!StringUtils.isEmpty(whereCondition)) {\\n        selectSQLAppender.append(\\" WHERE \\" + whereCondition);\\n    }\\n    selectSQLAppender.append(\\" FOR UPDATE\\");\\n    String selectPKSQL = selectSQLAppender.toString();\\n\\n    try {\\n        if (originalAutoCommit) {\\n            conn.setAutoCommit(false);\\n        }\\n        sp = conn.setSavepoint();\\n        rs = statementCallback.execute(statementProxy.getTargetStatement(), args);\\n\\n        while (true) {\\n            // Try to get global lock of those rows selected\\n            Statement stPK = null;\\n            PreparedStatement pstPK = null;\\n            ResultSet rsPK = null;\\n            try {\\n                if (paramAppender.isEmpty()) {\\n                    stPK = statementProxy.getConnection().createStatement();\\n                    rsPK = stPK.executeQuery(selectPKSQL);\\n                } else {\\n                    pstPK = statementProxy.getConnection().prepareStatement(selectPKSQL);\\n                    for (int i = 0; i < paramAppender.size(); i++) {\\n                        pstPK.setObject(i + 1, paramAppender.get(i));\\n                    }\\n                    rsPK = pstPK.executeQuery();\\n                }\\n\\n                TableRecords selectPKRows = TableRecords.buildRecords(getTableMeta(), rsPK);\\n                statementProxy.getConnectionProxy().checkLock(selectPKRows);\\n                break;\\n\\n            } catch (LockConflictException lce) {\\n                conn.rollback(sp);\\n                lockRetryController.sleep(lce);\\n\\n            } finally {\\n                if (rsPK != null) {\\n                    rsPK.close();\\n                }\\n                if (stPK != null) {\\n                    stPK.close();\\n                }\\n                if (pstPK != null) {\\n                    pstPK.close();\\n                }\\n            }\\n        }\\n\\n    } finally {\\n        if (sp != null) {\\n            conn.releaseSavepoint(sp);\\n        }\\n        if (originalAutoCommit) {\\n            conn.setAutoCommit(true);\\n        }\\n    }\\n    return rs;\\n}\\n```\\n\\nThe key code is as follows:\\n\\n```java\\nTableRecords selectPKRows = TableRecords.buildRecords(getTableMeta(), rsPK);\\nstatementProxy.getConnectionProxy().checkLock(selectPKRows);\\n```\\n\\nBy operating on the selectPKRows table records, lockKeys are obtained, and then it checks with the TC controller to see if they are globally locked. If they are locked, it retries until the lock is released and then returns the query result.\\n\\n## Registration and Reporting of Branch Transactions\\n\\nBefore the local transaction is committed, Fescar registers and reports information related to the branch transaction. This can be seen in the commit part of the ConnectionProxy class code:\\n```java\\n@Override\\npublic void commit() throws SQLException {\\n    if (context.inGlobalTransaction()) {\\n        try {\\n            register();\\n        } catch (TransactionException e) {\\n            recognizeLockKeyConflictException(e);\\n        }\\n\\n        try {\\n            if (context.hasUndoLog()) {\\n                UndoLogManager.flushUndoLogs(this);\\n            }\\n            targetConnection.commit();\\n        } catch (Throwable ex) {\\n            report(false);\\n            if (ex instanceof SQLException) {\\n                throw (SQLException) ex;\\n            } else {\\n                throw new SQLException(ex);\\n            }\\n        }\\n        report(true);\\n        context.reset();\\n\\n    } else {\\n        targetConnection.commit();\\n    }\\n}\\n```\\n\\nFrom this code, we can see that it first checks whether it is a global transaction. If it is not, it commits directly. If it is, it first registers the branch transaction with the TC controller. For write isolation, it involves obtaining global locks on the TC side. Then, it saves the undo_log used for rollback operations and finally commits the local transaction. Lastly, it reports the transaction status to the TC controller. At this point, the first phase of the local transaction is complete.\\n\\n## Coordinating the Global Transaction with the [server] Module\\n\\nFor the server module, we can focus on the DefaultCoordinator class. This is the default implementation of the AbstractTCInboundHandler controller handler. It mainly implements interfaces for starting, committing, rolling back, querying the status of global transactions, registering branch transactions, reporting, and checking locks, such as:\\n\\n![](/img/blog/3da6fd82debb9470eb4a5feb1eecac6d6a2.jpg)\\n\\nReturning to the TransactionalTemplate at the beginning, if the entire distributed transaction fails and needs to be rolled back, TM first initiates a rollback instruction to TC. After TC receives it and parses the request, it is routed to the doGlobalRollback method of the default controller class. The code executed on the TC controller side is as follows:\\n\\n```java\\n@Override\\npublic void doGlobalRollback(GlobalSession globalSession, boolean retrying) throws TransactionException {\\n    for (BranchSession branchSession : globalSession.getReverseSortedBranches()) {\\n        BranchStatus currentBranchStatus = branchSession.getStatus();\\n        if (currentBranchStatus == BranchStatus.PhaseOne_Failed) {\\n            continue;\\n        }\\n        try {\\n            BranchStatus branchStatus = resourceManagerInbound.branchRollback(XID.generateXID(branchSession.getTransactionId()), branchSession.getBranchId(),\\n                    branchSession.getResourceId(), branchSession.getApplicationData());\\n\\n            switch (branchStatus) {\\n                case PhaseTwo_Rollbacked:\\n                    globalSession.removeBranch(branchSession);\\n                    LOGGER.error(\\"Successfully rolled back branch \\" + branchSession);\\n                    continue;\\n                case PhaseTwo\\\\_RollbackFailed\\\\_Unretryable:\\n                    GlobalStatus currentStatus = globalSession.getStatus();\\n                    if (currentStatus.name().startsWith(\\"Timeout\\")) {\\n                        globalSession.changeStatus(GlobalStatus.TimeoutRollbackFailed);\\n                    } else {\\n                        globalSession.changeStatus(GlobalStatus.RollbackFailed);\\n                    }\\n                    globalSession.end();\\n                    LOGGER.error(\\"Failed to rollback global\\\\[\\" + globalSession.getTransactionId() + \\"\\\\] since branch\\\\[\\" + branchSession.getBranchId() + \\"\\\\] rollback failed\\");\\n                    return;\\n                default:\\n                    LOGGER.info(\\"Failed to rollback branch \\" + branchSession);\\n                    if (!retrying) {\\n                        queueToRetryRollback(globalSession);\\n                    }\\n                    return;\\n\\n            }\\n        } catch (Exception ex) {\\n            LOGGER.info(\\"Exception rollbacking branch \\" + branchSession, ex);\\n            if (!retrying) {\\n                queueToRetryRollback(globalSession);\\n                if (ex instanceof TransactionException) {\\n                    throw (TransactionException) ex;\\n                } else {\\n                    throw new TransactionException(ex);\\n                }\\n            }\\n\\n        }\\n\\n    }\\n    GlobalStatus currentStatus = globalSession.getStatus();\\n    if (currentStatus.name().startsWith(\\"Timeout\\")) {\\n        globalSession.changeStatus(GlobalStatus.TimeoutRollbacked);\\n    } else {\\n        globalSession.changeStatus(GlobalStatus.Rollbacked);\\n    }\\n    globalSession.end();\\n}\\n```\\n\\nAs seen from the above code, during rollback, each branch transaction is iterated from the global transaction session, and then each branch transaction is notified to rollback. When the branch service receives the request, it is first routed to the doBranchRollback method in RMHandlerAT, and then the branchRollback method in RM is called. The code is as follows:\\n\\n```java\\n@Override\\npublic BranchStatus branchRollback(String xid, long branchId, String resourceId, String applicationData) throws TransactionException {\\n    DataSourceProxy dataSourceProxy = get(resourceId);\\n    if (dataSourceProxy == null) {\\n        throw new ShouldNeverHappenException();\\n    }\\n    try {\\n        UndoLogManager.undo(dataSourceProxy, xid, branchId);\\n    } catch (TransactionException te) {\\n        if (te.getCode() == TransactionExceptionCode.BranchRollbackFailed_Unretriable) {\\n            return BranchStatus.PhaseTwo_RollbackFailed_Unretryable;\\n        } else {\\n            return BranchStatus.PhaseTwo_RollbackFailed_Retryable;\\n        }\\n    }\\n    return BranchStatus.PhaseTwo_Rollbacked;\\n}\\n```\\n\\nOn the RM branch transaction side, the UndoLogManager\'s undo method is ultimately executed. It retrieves the rollback log from the database using xid and branchId to complete the data rollback operation. The entire process is completed synchronously. If the global transaction is successful, TC will have a similar coordination process as mentioned above, but it will be asynchronous, clearing the undo_log related to the global transaction. At this point, the two-phase commit or rollback is completed, thus achieving complete control of the global transaction.\\n\\n# Conclusion\\n\\nIf you\'ve made it this far, thank you very much for patiently taking the time to learn amidst your busy schedule. I believe the time spent was worthwhile. By thoroughly reading and understanding this article, you likely have a comprehensive understanding of the main flow of Fescar\'s implementation. This article took approximately one person-day from conception to completion. During this process, I also gained a deeper understanding of Fescar\'s implementation. Due to space constraints, I did not delve into every detail of the implementation, such as how SQL parsing is done, but instead focused on the key points of the implementation process of Fescar\'s TXC model. This article has been proofread, but due to personal knowledge limitations and finite energy, errors or misunderstandings may inevitably occur. Corrections are welcome.\\n\\n### About the Author:\\n\\nChen Kailing joined Kaijing Technology in May 2016. Formerly a senior developer and project manager, currently the head of the Architecture & Operations Department of Kaijing Technology\'s R&D Center. PMP project management certification, Alibaba Cloud MVP. Enthusiastic about open source, having open-sourced several popular projects. Passionate about sharing technical insights, and author of the independent blog KL Blog ([http://www.kailing.pub](http://www.kailing.pub/))."},{"id":"/manual-transaction-mode","metadata":{"permalink":"/blog/manual-transaction-mode","editUrl":"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/manual-transaction-mode.md","source":"@site/i18n/en/docusaurus-plugin-content-blog/manual-transaction-mode.md","title":"MT mode","description":"introduce MT mode","date":"2019-02-13T00:00:00.000Z","formattedDate":"February 13, 2019","tags":[],"readingTime":1.12,"hasTruncateMarker":false,"authors":[{"name":"kmmshmily"}],"frontMatter":{"title":"MT mode","keywords":["MT mode"],"description":"introduce MT mode","author":"kmmshmily","date":"2019-02-13T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Unveiling the Principles of Fescar Distributed Transaction","permalink":"/blog/seata-analysis-simple"}},"content":"Review the description in the overview: a distributed global transaction, the whole is a model of **the two-phase commit**. A global transaction consists of several branch transactions that meet the model requirements of **the two-phase commit**, which requires each branch transaction to have its own:\\n\\n- One phase prepare behavior\\n- Two phase commit or rollback behavior\\n\\n![Overview of a global transaction](https://upload-images.jianshu.io/upload_images/4420767-e48f0284a037d1df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\\n\\nAccording to the two phase behavior pattern\uff0cWe divide the branch transaction into **Automatic (Branch) Transaction Mode** and **Manual (Branch) Transaction Mode**.\\n\\nThe AT mode is based on the **Relational Database** that **supports local ACID transactions**\uff1a\\n\\n- One phase prepare behavior: In the local transaction, the business data update and the corresponding rollback log record are submitted together.\\n- Two phase commit behavior: Immediately ended successfully, **Auto** asynchronous batch cleanup of the rollback log.\\n- Two phase rollback behavior: By rolling back the log, **automatic** generates a compensation operation to complete the data rollback.\\n\\nAccordingly, the MT mode does not rely on transaction support for the underlying data resources:\\n\\n- One phase prepare behavior: Call the prepare logic of **custom** .\\n- Two phase commit behavior:Call the commit logic of **custom** .\\n- Two phase rollback behavior:Call the rollback logic of **custom** .\\n\\nThe so-called MT mode refers to the support of the branch transaction of **custom** into the management of global transactions."}]}')}}]);